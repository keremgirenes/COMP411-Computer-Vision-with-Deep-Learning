{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6b7ee23c",
      "metadata": {
        "id": "6b7ee23c",
        "tags": [
          "pdf-title"
        ]
      },
      "source": [
        "# Attentional Networks in Computer Vision\n",
        "\n",
        "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
        "\n",
        "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
        "\n",
        "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01a910e",
      "metadata": {
        "id": "a01a910e"
      },
      "source": [
        "# Part I. Preparation\n",
        "\n",
        "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
        "\n",
        "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a3cdc4fd",
      "metadata": {
        "id": "a3cdc4fd",
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6152858f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "d342e26f331b45cd9c390a7bdd1c1e05",
            "238cf8e173a548c0ab6cf47f75735fe3",
            "d86e82a2c0e349faa81b2d4db15b77b7",
            "4ea1a7c745c14811af373576f212cd8a",
            "885bd009bd5f49f18cd6c3917eafe8d7",
            "ece3db6e8fb843e6b3d8055fcb50b4d1",
            "afa737f1c7a54d36bcf2feed2b6f1b95",
            "bd8104dc914c400989fb1b9b7897b479",
            "e3caba48189a48cca56663445598b014",
            "618c338ee0804002a4e5c2c0dd0158b7",
            "62807e8b2fae424aa17bb71c90d971fb"
          ]
        },
        "id": "6152858f",
        "outputId": "164e759e-ebb2-4585-ce01-05d8b75b5eeb",
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_TRAIN = 49000\n",
        "\n",
        "# The torchvision.transforms package provides tools for preprocessing data\n",
        "# and for performing data augmentation; here we set up a transform to\n",
        "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
        "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
        "transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "\n",
        "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
        "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
        "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
        "# training set into train and val sets by passing a Sampler object to the\n",
        "# DataLoader telling how it should sample from the underlying Dataset.\n",
        "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
        "                             transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
        "                           transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
        "                            transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840763c9",
      "metadata": {
        "id": "840763c9",
        "tags": [
          "pdf-ignore"
        ]
      },
      "source": [
        "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
        "\n",
        "The global variables `dtype` and `device` will control the data types throughout this assignment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d502cffe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d502cffe",
        "outputId": "93665fe9-96f5-4c6f-b919-4f9f1e0f76db",
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n"
          ]
        }
      ],
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "787434ed",
      "metadata": {
        "id": "787434ed"
      },
      "source": [
        "# Part II. Barebones Transformers: Self-Attentional Layer\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
        "\n",
        "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
        "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
        "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
        "\n",
        "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
        "\n",
        "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
        "\n",
        "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
        "\n",
        "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
        "\n",
        "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
        "\n",
        "5. Reassemble heads into one flat vector and return the output.\n",
        "\n",
        "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "217f44ac",
      "metadata": {
        "id": "217f44ac"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class SelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        \n",
        "        ## initialize module's instance variables\n",
        "        self.input_dims = input_dims\n",
        "        self.head_dims = head_dims\n",
        "        self.num_heads = num_heads\n",
        "        self.proj_dims = head_dims * num_heads\n",
        "        \n",
        "        ## Declare module's parameters\n",
        "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Input of shape, [B, N, D] where:\n",
        "        ## - B denotes the batch size\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
        "        ## - D corresponds to model dimensionality\n",
        "        b,n,d = x.shape\n",
        "        \n",
        "        ## Construct queries,keys,values\n",
        "        q_ = self.W_Q(x)\n",
        "        k_ = self.W_K(x)\n",
        "        v_ = self.W_V(x)\n",
        "        \n",
        "        ## Seperate q,k,v into their corresponding heads,\n",
        "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
        "        ## - B denotes the batch size\n",
        "        ## - H denotes number of heads\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
        "        ## - D//H corresponds to per head dimensionality\n",
        "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
        "       \n",
        "        #########################################################################################\n",
        "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
        "        #########################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        ## Compute attention logits. Note that this operation is conducted as a\n",
        "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
        "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
        "        ## Output Attention logits should have the size: [B,H,N,N]\n",
        "       \n",
        "        alignment_scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.head_dims)\n",
        "\n",
        "        ## Compute attention Weights. Note that this operation is conducted as a\n",
        "        ## Softmax Normalization across the keys dimension. \n",
        "        ## Hint: You can apply the Softmax operation across the final dimension\n",
        "\n",
        "        attention_scores = F.softmax(alignment_scores, dim=2)\n",
        "       \n",
        "        ## Compute output values. Note that this operation is conducted as a \n",
        "        ## batched matrix multiplication between the Attention Weights matrix and \n",
        "        ## the values tensor. After computing output values, the output should be reshaped\n",
        "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
        "        ## Output should be of size [B, N, D]\n",
        "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
        "        \n",
        "        attn_out = torch.matmul(attention_scores, v)\n",
        "        attn_out = attn_out.permute(0,2,1,3)\n",
        "        attn_out = torch.reshape(attn_out,(b,n,self.proj_dims))\n",
        "        \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             \n",
        "        ################################################################################\n",
        "    \n",
        "        return attn_out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d039f68f",
      "metadata": {
        "id": "d039f68f"
      },
      "source": [
        "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape [64,16,256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "edf1a8a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edf1a8a8",
        "outputId": "93635e52-de97-4412-e1dd-946311bd413b",
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 16, 256])\n"
          ]
        }
      ],
      "source": [
        "def test_self_attn_layer():\n",
        "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
        "    layer = SelfAttention(32,64,4)\n",
        "    out = layer(x)\n",
        "    print(out.size())  # you should see [64,16,256]\n",
        "test_self_attn_layer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "980c2a5a",
      "metadata": {
        "id": "980c2a5a"
      },
      "source": [
        "# Part III. Barebones Transformers: Transformer Encoder Block\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b9ef2b6",
      "metadata": {
        "id": "0b9ef2b6"
      },
      "outputs": [],
      "source": [
        "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
        "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        o = F.elu(self.fc_1(x))\n",
        "        o = self.fc_2(o)\n",
        "        return o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8a7409ce",
      "metadata": {
        "id": "8a7409ce"
      },
      "outputs": [],
      "source": [
        "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
        "## module follows a simple computational pipeline:\n",
        "## input --> layernorm --> SelfAttention --> skip connection \n",
        "##       --> layernorm --> MLP ---> skip connection ---> output\n",
        "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
        "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
        "## to the SelfAttention block.\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "    ###############################################################\n",
        "    # TODO: Complete the consturctor of  TransformerBlock module  #\n",
        "    ###############################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dims = self.hidden_dims // self.num_heads\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(self.hidden_dims) \n",
        "        self.attention = SelfAttention(self.hidden_dims,head_dims=self.head_dims,num_heads=self.num_heads)\n",
        "        self.norm2 = nn.LayerNorm(self.hidden_dims)\n",
        "        self.mlp = MLP(hidden_dims, hidden_dims, hidden_dims)\n",
        "        \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###################################################################\n",
        "    #                                 END OF YOUR CODE                #             \n",
        "    ###################################################################\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "    ##############################################################\n",
        "    # TODO: Complete the forward of TransformerBlock module      #\n",
        "    ##############################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "\n",
        "        norm1_out = self.norm1(x)\n",
        "        attention_out = self.attention(norm1_out) + x\n",
        "        norm2_out = self.norm2(attention_out)\n",
        "        mlp_out = self.mlp(norm2_out)\n",
        "        out = mlp_out + attention_out\n",
        "\n",
        "        return out\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###################################################################\n",
        "    #                                 END OF YOUR CODE                #             \n",
        "    ###################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74208d7",
      "metadata": {
        "id": "c74208d7"
      },
      "source": [
        "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "648ab1d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "648ab1d4",
        "outputId": "1f9ed0bd-9c09-494e-ff01-0d20b0e9d8bd",
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 16, 128])\n"
          ]
        }
      ],
      "source": [
        "def test_transfomerblock_layer():\n",
        "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
        "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
        "    out = layer(x)\n",
        "    print(out.size())  # you should see [64,16,128]\n",
        "test_transfomerblock_layer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544c1132",
      "metadata": {
        "id": "544c1132"
      },
      "source": [
        "# Part IV The Vision Transformer (ViT)\n",
        "\n",
        "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "aca38dd0",
      "metadata": {
        "id": "aca38dd0"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
        "        super(ViT, self).__init__()\n",
        "                \n",
        "        ## initialize module's instance variables\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.input_dims = input_dims\n",
        "        self.output_dims = output_dims\n",
        "        self.num_trans_layers = num_trans_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.image_k = image_k\n",
        "        self.patch_k = patch_k\n",
        "        \n",
        "        self.image_height = self.image_width = image_k\n",
        "        self.patch_height = self.patch_width = patch_k\n",
        "        \n",
        "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
        "                'Image size must be divisible by the patch size.'\n",
        "\n",
        "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
        "        self.patch_flat_len = self.patch_height * self.patch_width\n",
        "        \n",
        "        ## Declare module's parameters\n",
        "        \n",
        "        ## ViT's flattened patch embedding projection:\n",
        "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
        "        \n",
        "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
        "        \n",
        "        ## Learnable classt token and its index among attention sequence elements.\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
        "        self.cls_index = torch.LongTensor([0])\n",
        "        \n",
        "        ## Declare cascaded Transformer blocks:\n",
        "        transformer_encoder_list = []\n",
        "        for _ in range(self.num_trans_layers):\n",
        "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
        "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
        "        \n",
        "        ## Declare the output mlp:\n",
        "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
        "         \n",
        "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
        "        ## Create sliding window pathes using nn.Functional.unfold\n",
        "        ## Input dimensions: [B,D,H,W] where\n",
        "        ## --B : input batch size\n",
        "        ## --D : input channels\n",
        "        ## --H, W: input height and width\n",
        "        ## Output dimensions: [B,N,H*W,D]\n",
        "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
        "        ##      sliding window stride and padding.\n",
        "        b,d,h,w = x.shape\n",
        "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
        "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
        "        n = x_unf.size(1)\n",
        "        return x_unf,n\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b = x.size(0)\n",
        "        ## create sliding window patches from the input image\n",
        "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
        "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
        "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
        "        ## linearly embed each flattened patch\n",
        "        x_embed = self.linear_embed(x_patch_flat)\n",
        "        \n",
        "        ## retrieve class token \n",
        "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
        "        ## concatanate class token to input patches\n",
        "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
        "        \n",
        "        ## add positional embedding to input patches + class token \n",
        "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
        "        \n",
        "        ## pass through the transformer encoder\n",
        "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
        "        \n",
        "        ## select the class token \n",
        "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
        "        \n",
        "        ## create output\n",
        "        out = self.out_mlp(out_cls_token)\n",
        "        \n",
        "        return out.squeeze(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba4cdab",
      "metadata": {
        "id": "cba4cdab"
      },
      "source": [
        "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2ec9176e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ec9176e",
        "outputId": "7edc3adc-7247-4900-8acb-3c9d43c88be9",
        "pycharm": {
          "is_executing": false
        },
        "tags": [
          "pdf-ignore-input"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "def test_vit():\n",
        "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
        "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
        "    out = model(x)\n",
        "    print(out.size())  # you should see [64,10]\n",
        "test_vit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833736a1",
      "metadata": {
        "id": "833736a1"
      },
      "source": [
        "# Part V. Train the ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b33f7f1",
      "metadata": {
        "id": "6b33f7f1"
      },
      "source": [
        "### Check Accuracy\n",
        "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
        "\n",
        "The check_batch_accuracy function is provided for you below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c50f0b22",
      "metadata": {
        "id": "c50f0b22",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def check_batch_accuracy(out, target,eps=1e-7):\n",
        "    b, c = out.shape\n",
        "    with torch.no_grad():\n",
        "        _, pred = out.max(-1) \n",
        "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
        "    return correct, np.float(correct) / (b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad6403f",
      "metadata": {
        "id": "dad6403f"
      },
      "source": [
        "### Training Loop\n",
        "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c758b6b0",
      "metadata": {
        "id": "c758b6b0",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer, trainloader):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "    \n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
        "    \n",
        "    Returns: overall training accuracy for the epoch\n",
        "    \"\"\"\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    network.train()  # put model to training mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
        "            \n",
        "        outputs = network(inputs)\n",
        "        loss =  F.cross_entropy(outputs, targets)\n",
        "            \n",
        "        # Zero out all of the gradients for the variables which the optimizer\n",
        "        # will update.\n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        # This is the backwards pass: compute the gradient of the loss with\n",
        "        # respect to each  parameter of the model.\n",
        "        loss.backward()\n",
        "            \n",
        "        # Actually update the parameters of the model using the gradients\n",
        "        # computed by the backwards pass.\n",
        "        optimizer.step()\n",
        "            \n",
        "        loss = loss.detach()\n",
        "        train_loss += loss.item()\n",
        "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
        "        correct += correct_p\n",
        "        total += targets.size(0)\n",
        "\n",
        "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        \n",
        "    return 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe7414b",
      "metadata": {
        "id": "dfe7414b"
      },
      "source": [
        "### Evaluation Loop\n",
        "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "23c264e5",
      "metadata": {
        "id": "23c264e5"
      },
      "outputs": [],
      "source": [
        "def evaluate(network, evalloader):\n",
        "    \"\"\"\n",
        "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "    \n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
        "    \n",
        "    Returns: overall evaluation accuracy for the epoch\n",
        "    \"\"\"\n",
        "    network.eval() # put model to evaluation mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n---- Evaluation in process ----')\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
        "            outputs = network(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
        "            correct += correct_p\n",
        "            total += targets.size(0)\n",
        "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ea5a24",
      "metadata": {
        "id": "00ea5a24"
      },
      "source": [
        "### Overfit a ViT\n",
        "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
        "\n",
        "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
        "\n",
        "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
        "\n",
        "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "eb34b985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb34b985",
        "outputId": "4cfeb969-4974-49cb-a0c8-b106938f81ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
            "==> Data ready, batchsize = 25\n"
          ]
        }
      ],
      "source": [
        "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
        "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
        "\n",
        "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
        "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
        "\n",
        "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
        "\n",
        "batch_size_sub = 25\n",
        "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
        "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
        "\n",
        "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2c1e75fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c1e75fc",
        "outputId": "bb3e3937-5a0c-499d-8111-53d60a05f5b1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Loss: 4.011 | Acc: 8.000% (2/25)\n",
            "Loss: 6.532 | Acc: 4.000% (2/50)\n",
            "Loss: 6.766 | Acc: 8.000% (6/75)\n",
            "Loss: 6.544 | Acc: 9.000% (9/100)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 9.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 5.032 | Acc: 8.000% (2/25)\n",
            "Loss: 5.127 | Acc: 6.000% (3/50)\n",
            "Loss: 4.739 | Acc: 6.667% (5/75)\n",
            "Loss: 4.756 | Acc: 7.000% (7/100)\n",
            "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 7.0\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 4.199 | Acc: 16.000% (4/25)\n",
            "Loss: 4.150 | Acc: 24.000% (12/50)\n",
            "Loss: 3.954 | Acc: 20.000% (15/75)\n",
            "Loss: 3.753 | Acc: 18.000% (18/100)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 18.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 3.721 | Acc: 24.000% (6/25)\n",
            "Loss: 3.183 | Acc: 24.000% (12/50)\n",
            "Loss: 3.337 | Acc: 20.000% (15/75)\n",
            "Loss: 3.435 | Acc: 19.000% (19/100)\n",
            "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 19.0\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 3.847 | Acc: 16.000% (4/25)\n",
            "Loss: 3.079 | Acc: 14.000% (7/50)\n",
            "Loss: 2.931 | Acc: 16.000% (12/75)\n",
            "Loss: 2.744 | Acc: 18.000% (18/100)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 18.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.745 | Acc: 12.000% (3/25)\n",
            "Loss: 2.541 | Acc: 18.000% (9/50)\n",
            "Loss: 2.502 | Acc: 14.667% (11/75)\n",
            "Loss: 2.438 | Acc: 18.000% (18/100)\n",
            "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 18.0\n",
            "\n",
            "Epoch: 3\n",
            "Loss: 1.955 | Acc: 32.000% (8/25)\n",
            "Loss: 2.074 | Acc: 30.000% (15/50)\n",
            "Loss: 2.187 | Acc: 26.667% (20/75)\n",
            "Loss: 2.192 | Acc: 26.000% (26/100)\n",
            "Epoch 3 of training is completed, Training accuracy for this epoch is 26.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.750 | Acc: 12.000% (3/25)\n",
            "Loss: 2.595 | Acc: 18.000% (9/50)\n",
            "Loss: 2.701 | Acc: 16.000% (12/75)\n",
            "Loss: 2.727 | Acc: 16.000% (16/100)\n",
            "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 16.0\n",
            "\n",
            "Epoch: 4\n",
            "Loss: 1.960 | Acc: 32.000% (8/25)\n",
            "Loss: 2.285 | Acc: 20.000% (10/50)\n",
            "Loss: 2.220 | Acc: 21.333% (16/75)\n",
            "Loss: 2.262 | Acc: 23.000% (23/100)\n",
            "Epoch 4 of training is completed, Training accuracy for this epoch is 23.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.746 | Acc: 16.000% (4/25)\n",
            "Loss: 2.663 | Acc: 18.000% (9/50)\n",
            "Loss: 2.697 | Acc: 12.000% (9/75)\n",
            "Loss: 2.775 | Acc: 15.000% (15/100)\n",
            "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 15.0\n",
            "\n",
            "Epoch: 5\n",
            "Loss: 2.057 | Acc: 36.000% (9/25)\n",
            "Loss: 1.960 | Acc: 28.000% (14/50)\n",
            "Loss: 1.960 | Acc: 25.333% (19/75)\n",
            "Loss: 1.949 | Acc: 26.000% (26/100)\n",
            "Epoch 5 of training is completed, Training accuracy for this epoch is 26.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.592 | Acc: 16.000% (4/25)\n",
            "Loss: 2.392 | Acc: 18.000% (9/50)\n",
            "Loss: 2.400 | Acc: 21.333% (16/75)\n",
            "Loss: 2.472 | Acc: 18.000% (18/100)\n",
            "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 18.0\n",
            "\n",
            "Epoch: 6\n",
            "Loss: 1.670 | Acc: 40.000% (10/25)\n",
            "Loss: 1.728 | Acc: 38.000% (19/50)\n",
            "Loss: 1.699 | Acc: 34.667% (26/75)\n",
            "Loss: 1.762 | Acc: 32.000% (32/100)\n",
            "Epoch 6 of training is completed, Training accuracy for this epoch is 32.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.909 | Acc: 12.000% (3/25)\n",
            "Loss: 2.684 | Acc: 18.000% (9/50)\n",
            "Loss: 2.820 | Acc: 14.667% (11/75)\n",
            "Loss: 2.852 | Acc: 18.000% (18/100)\n",
            "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 18.0\n",
            "\n",
            "Epoch: 7\n",
            "Loss: 1.435 | Acc: 56.000% (14/25)\n",
            "Loss: 1.482 | Acc: 52.000% (26/50)\n",
            "Loss: 1.524 | Acc: 45.333% (34/75)\n",
            "Loss: 1.544 | Acc: 42.000% (42/100)\n",
            "Epoch 7 of training is completed, Training accuracy for this epoch is 42.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.523 | Acc: 36.000% (9/25)\n",
            "Loss: 2.516 | Acc: 38.000% (19/50)\n",
            "Loss: 2.587 | Acc: 32.000% (24/75)\n",
            "Loss: 2.715 | Acc: 30.000% (30/100)\n",
            "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 30.0\n",
            "\n",
            "Epoch: 8\n",
            "Loss: 1.220 | Acc: 52.000% (13/25)\n",
            "Loss: 1.274 | Acc: 58.000% (29/50)\n",
            "Loss: 1.313 | Acc: 54.667% (41/75)\n",
            "Loss: 1.209 | Acc: 57.000% (57/100)\n",
            "Epoch 8 of training is completed, Training accuracy for this epoch is 57.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.771 | Acc: 28.000% (7/25)\n",
            "Loss: 2.564 | Acc: 30.000% (15/50)\n",
            "Loss: 2.635 | Acc: 29.333% (22/75)\n",
            "Loss: 2.707 | Acc: 30.000% (30/100)\n",
            "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 30.0\n",
            "\n",
            "Epoch: 9\n",
            "Loss: 0.859 | Acc: 68.000% (17/25)\n",
            "Loss: 0.863 | Acc: 68.000% (34/50)\n",
            "Loss: 0.951 | Acc: 64.000% (48/75)\n",
            "Loss: 1.032 | Acc: 62.000% (62/100)\n",
            "Epoch 9 of training is completed, Training accuracy for this epoch is 62.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.667 | Acc: 20.000% (5/25)\n",
            "Loss: 2.900 | Acc: 26.000% (13/50)\n",
            "Loss: 2.927 | Acc: 21.333% (16/75)\n",
            "Loss: 2.892 | Acc: 19.000% (19/100)\n",
            "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 19.0\n",
            "\n",
            "Final train set accuracy is 62.0\n",
            "Final val set accuracy is 19.0\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "num_trans_layers = 4\n",
        "num_heads = 4\n",
        "image_k = 32\n",
        "patch_k = 4\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "model = ViT(hidden_dims,input_dims,output_dims,num_trans_layers,num_heads,image_k,patch_k)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9,0.999))\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             \n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "eval_accs=[]\n",
        "for epoch in range(10):\n",
        "    tr_acc = train(model, optimizer, trainloader_sub)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))  \n",
        "    \n",
        "    eval_acc = evaluate(model, valloader_sub)\n",
        "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
        "              .format(epoch, eval_acc))  \n",
        "    tr_accs.append(tr_acc)\n",
        "    eval_accs.append(eval_acc)\n",
        "    \n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588330f1",
      "metadata": {
        "id": "588330f1"
      },
      "source": [
        "## Train the net\n",
        "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2ee3dabf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ee3dabf",
        "outputId": "a1cbef5c-6ccf-456d-e131-f585669575e1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Loss: 5.124 | Acc: 12.500% (8/64)\n",
            "Loss: 6.017 | Acc: 8.594% (11/128)\n",
            "Loss: 6.308 | Acc: 9.375% (18/192)\n",
            "Loss: 5.801 | Acc: 10.547% (27/256)\n",
            "Loss: 5.613 | Acc: 11.875% (38/320)\n",
            "Loss: 5.287 | Acc: 12.240% (47/384)\n",
            "Loss: 5.055 | Acc: 12.054% (54/448)\n",
            "Loss: 4.823 | Acc: 12.695% (65/512)\n",
            "Loss: 4.684 | Acc: 11.979% (69/576)\n",
            "Loss: 4.479 | Acc: 12.500% (80/640)\n",
            "Loss: 4.350 | Acc: 12.642% (89/704)\n",
            "Loss: 4.210 | Acc: 13.542% (104/768)\n",
            "Loss: 4.095 | Acc: 13.702% (114/832)\n",
            "Loss: 3.981 | Acc: 13.951% (125/896)\n",
            "Loss: 3.878 | Acc: 14.062% (135/960)\n",
            "Loss: 3.784 | Acc: 14.258% (146/1024)\n",
            "Loss: 3.710 | Acc: 14.430% (157/1088)\n",
            "Loss: 3.626 | Acc: 14.931% (172/1152)\n",
            "Loss: 3.555 | Acc: 15.214% (185/1216)\n",
            "Loss: 3.485 | Acc: 15.547% (199/1280)\n",
            "Loss: 3.415 | Acc: 16.220% (218/1344)\n",
            "Loss: 3.361 | Acc: 16.122% (227/1408)\n",
            "Loss: 3.312 | Acc: 16.168% (238/1472)\n",
            "Loss: 3.273 | Acc: 16.406% (252/1536)\n",
            "Loss: 3.227 | Acc: 16.562% (265/1600)\n",
            "Loss: 3.192 | Acc: 16.647% (277/1664)\n",
            "Loss: 3.153 | Acc: 16.956% (293/1728)\n",
            "Loss: 3.117 | Acc: 17.132% (307/1792)\n",
            "Loss: 3.086 | Acc: 17.188% (319/1856)\n",
            "Loss: 3.053 | Acc: 17.240% (331/1920)\n",
            "Loss: 3.022 | Acc: 17.288% (343/1984)\n",
            "Loss: 2.997 | Acc: 17.285% (354/2048)\n",
            "Loss: 2.968 | Acc: 17.235% (364/2112)\n",
            "Loss: 2.943 | Acc: 17.417% (379/2176)\n",
            "Loss: 2.921 | Acc: 17.411% (390/2240)\n",
            "Loss: 2.899 | Acc: 17.578% (405/2304)\n",
            "Loss: 2.876 | Acc: 17.736% (420/2368)\n",
            "Loss: 2.859 | Acc: 17.928% (436/2432)\n",
            "Loss: 2.839 | Acc: 18.029% (450/2496)\n",
            "Loss: 2.822 | Acc: 18.047% (462/2560)\n",
            "Loss: 2.803 | Acc: 18.102% (475/2624)\n",
            "Loss: 2.786 | Acc: 18.341% (493/2688)\n",
            "Loss: 2.765 | Acc: 18.750% (516/2752)\n",
            "Loss: 2.750 | Acc: 18.821% (530/2816)\n",
            "Loss: 2.738 | Acc: 18.785% (541/2880)\n",
            "Loss: 2.723 | Acc: 18.818% (554/2944)\n",
            "Loss: 2.704 | Acc: 19.016% (572/3008)\n",
            "Loss: 2.695 | Acc: 19.010% (584/3072)\n",
            "Loss: 2.680 | Acc: 19.133% (600/3136)\n",
            "Loss: 2.667 | Acc: 19.219% (615/3200)\n",
            "Loss: 2.656 | Acc: 19.240% (628/3264)\n",
            "Loss: 2.646 | Acc: 19.321% (643/3328)\n",
            "Loss: 2.632 | Acc: 19.546% (663/3392)\n",
            "Loss: 2.624 | Acc: 19.502% (674/3456)\n",
            "Loss: 2.615 | Acc: 19.688% (693/3520)\n",
            "Loss: 2.604 | Acc: 19.894% (713/3584)\n",
            "Loss: 2.595 | Acc: 19.901% (726/3648)\n",
            "Loss: 2.587 | Acc: 19.935% (740/3712)\n",
            "Loss: 2.574 | Acc: 20.101% (759/3776)\n",
            "Loss: 2.566 | Acc: 20.312% (780/3840)\n",
            "Loss: 2.558 | Acc: 20.312% (793/3904)\n",
            "Loss: 2.551 | Acc: 20.338% (807/3968)\n",
            "Loss: 2.549 | Acc: 20.238% (816/4032)\n",
            "Loss: 2.542 | Acc: 20.312% (832/4096)\n",
            "Loss: 2.536 | Acc: 20.337% (846/4160)\n",
            "Loss: 2.530 | Acc: 20.312% (858/4224)\n",
            "Loss: 2.525 | Acc: 20.336% (872/4288)\n",
            "Loss: 2.520 | Acc: 20.244% (881/4352)\n",
            "Loss: 2.519 | Acc: 20.109% (888/4416)\n",
            "Loss: 2.511 | Acc: 20.223% (906/4480)\n",
            "Loss: 2.504 | Acc: 20.312% (923/4544)\n",
            "Loss: 2.498 | Acc: 20.399% (940/4608)\n",
            "Loss: 2.490 | Acc: 20.484% (957/4672)\n",
            "Loss: 2.484 | Acc: 20.608% (976/4736)\n",
            "Loss: 2.480 | Acc: 20.542% (986/4800)\n",
            "Loss: 2.473 | Acc: 20.580% (1001/4864)\n",
            "Loss: 2.467 | Acc: 20.698% (1020/4928)\n",
            "Loss: 2.461 | Acc: 20.773% (1037/4992)\n",
            "Loss: 2.455 | Acc: 20.866% (1055/5056)\n",
            "Loss: 2.451 | Acc: 20.879% (1069/5120)\n",
            "Loss: 2.445 | Acc: 20.930% (1085/5184)\n",
            "Loss: 2.439 | Acc: 20.922% (1098/5248)\n",
            "Loss: 2.434 | Acc: 21.009% (1116/5312)\n",
            "Loss: 2.429 | Acc: 21.075% (1133/5376)\n",
            "Loss: 2.424 | Acc: 20.974% (1141/5440)\n",
            "Loss: 2.419 | Acc: 21.166% (1165/5504)\n",
            "Loss: 2.418 | Acc: 21.103% (1175/5568)\n",
            "Loss: 2.412 | Acc: 21.200% (1194/5632)\n",
            "Loss: 2.408 | Acc: 21.296% (1213/5696)\n",
            "Loss: 2.403 | Acc: 21.285% (1226/5760)\n",
            "Loss: 2.398 | Acc: 21.377% (1245/5824)\n",
            "Loss: 2.392 | Acc: 21.501% (1266/5888)\n",
            "Loss: 2.390 | Acc: 21.505% (1280/5952)\n",
            "Loss: 2.384 | Acc: 21.592% (1299/6016)\n",
            "Loss: 2.380 | Acc: 21.760% (1323/6080)\n",
            "Loss: 2.377 | Acc: 21.826% (1341/6144)\n",
            "Loss: 2.373 | Acc: 21.859% (1357/6208)\n",
            "Loss: 2.368 | Acc: 22.034% (1382/6272)\n",
            "Loss: 2.365 | Acc: 22.017% (1395/6336)\n",
            "Loss: 2.360 | Acc: 22.156% (1418/6400)\n",
            "Loss: 2.356 | Acc: 22.246% (1438/6464)\n",
            "Loss: 2.349 | Acc: 22.365% (1460/6528)\n",
            "Loss: 2.345 | Acc: 22.436% (1479/6592)\n",
            "Loss: 2.340 | Acc: 22.536% (1500/6656)\n",
            "Loss: 2.335 | Acc: 22.679% (1524/6720)\n",
            "Loss: 2.331 | Acc: 22.730% (1542/6784)\n",
            "Loss: 2.328 | Acc: 22.766% (1559/6848)\n",
            "Loss: 2.326 | Acc: 22.815% (1577/6912)\n",
            "Loss: 2.323 | Acc: 22.821% (1592/6976)\n",
            "Loss: 2.320 | Acc: 22.841% (1608/7040)\n",
            "Loss: 2.316 | Acc: 22.846% (1623/7104)\n",
            "Loss: 2.313 | Acc: 22.879% (1640/7168)\n",
            "Loss: 2.310 | Acc: 22.981% (1662/7232)\n",
            "Loss: 2.307 | Acc: 22.958% (1675/7296)\n",
            "Loss: 2.304 | Acc: 22.976% (1691/7360)\n",
            "Loss: 2.299 | Acc: 23.074% (1713/7424)\n",
            "Loss: 2.295 | Acc: 23.117% (1731/7488)\n",
            "Loss: 2.293 | Acc: 23.133% (1747/7552)\n",
            "Loss: 2.290 | Acc: 23.214% (1768/7616)\n",
            "Loss: 2.287 | Acc: 23.268% (1787/7680)\n",
            "Loss: 2.283 | Acc: 23.399% (1812/7744)\n",
            "Loss: 2.280 | Acc: 23.476% (1833/7808)\n",
            "Loss: 2.277 | Acc: 23.514% (1851/7872)\n",
            "Loss: 2.275 | Acc: 23.538% (1868/7936)\n",
            "Loss: 2.274 | Acc: 23.562% (1885/8000)\n",
            "Loss: 2.271 | Acc: 23.636% (1906/8064)\n",
            "Loss: 2.268 | Acc: 23.708% (1927/8128)\n",
            "Loss: 2.265 | Acc: 23.755% (1946/8192)\n",
            "Loss: 2.261 | Acc: 23.861% (1970/8256)\n",
            "Loss: 2.259 | Acc: 23.894% (1988/8320)\n",
            "Loss: 2.254 | Acc: 23.998% (2012/8384)\n",
            "Loss: 2.249 | Acc: 24.041% (2031/8448)\n",
            "Loss: 2.248 | Acc: 24.048% (2047/8512)\n",
            "Loss: 2.244 | Acc: 24.160% (2072/8576)\n",
            "Loss: 2.241 | Acc: 24.225% (2093/8640)\n",
            "Loss: 2.239 | Acc: 24.253% (2111/8704)\n",
            "Loss: 2.237 | Acc: 24.270% (2128/8768)\n",
            "Loss: 2.236 | Acc: 24.264% (2143/8832)\n",
            "Loss: 2.233 | Acc: 24.348% (2166/8896)\n",
            "Loss: 2.230 | Acc: 24.364% (2183/8960)\n",
            "Loss: 2.227 | Acc: 24.457% (2207/9024)\n",
            "Loss: 2.224 | Acc: 24.582% (2234/9088)\n",
            "Loss: 2.220 | Acc: 24.628% (2254/9152)\n",
            "Loss: 2.219 | Acc: 24.653% (2272/9216)\n",
            "Loss: 2.216 | Acc: 24.774% (2299/9280)\n",
            "Loss: 2.216 | Acc: 24.754% (2313/9344)\n",
            "Loss: 2.214 | Acc: 24.787% (2332/9408)\n",
            "Loss: 2.211 | Acc: 24.842% (2353/9472)\n",
            "Loss: 2.209 | Acc: 24.853% (2370/9536)\n",
            "Loss: 2.206 | Acc: 24.906% (2391/9600)\n",
            "Loss: 2.204 | Acc: 24.969% (2413/9664)\n",
            "Loss: 2.202 | Acc: 25.000% (2432/9728)\n",
            "Loss: 2.199 | Acc: 25.051% (2453/9792)\n",
            "Loss: 2.197 | Acc: 25.122% (2476/9856)\n",
            "Loss: 2.195 | Acc: 25.161% (2496/9920)\n",
            "Loss: 2.193 | Acc: 25.190% (2515/9984)\n",
            "Loss: 2.191 | Acc: 25.199% (2532/10048)\n",
            "Loss: 2.189 | Acc: 25.247% (2553/10112)\n",
            "Loss: 2.187 | Acc: 25.324% (2577/10176)\n",
            "Loss: 2.185 | Acc: 25.342% (2595/10240)\n",
            "Loss: 2.182 | Acc: 25.417% (2619/10304)\n",
            "Loss: 2.180 | Acc: 25.463% (2640/10368)\n",
            "Loss: 2.177 | Acc: 25.546% (2665/10432)\n",
            "Loss: 2.176 | Acc: 25.562% (2683/10496)\n",
            "Loss: 2.174 | Acc: 25.587% (2702/10560)\n",
            "Loss: 2.173 | Acc: 25.574% (2717/10624)\n",
            "Loss: 2.171 | Acc: 25.552% (2731/10688)\n",
            "Loss: 2.169 | Acc: 25.614% (2754/10752)\n",
            "Loss: 2.167 | Acc: 25.712% (2781/10816)\n",
            "Loss: 2.165 | Acc: 25.717% (2798/10880)\n",
            "Loss: 2.163 | Acc: 25.722% (2815/10944)\n",
            "Loss: 2.162 | Acc: 25.727% (2832/11008)\n",
            "Loss: 2.160 | Acc: 25.786% (2855/11072)\n",
            "Loss: 2.157 | Acc: 25.880% (2882/11136)\n",
            "Loss: 2.155 | Acc: 25.938% (2905/11200)\n",
            "Loss: 2.154 | Acc: 25.977% (2926/11264)\n",
            "Loss: 2.151 | Acc: 26.024% (2948/11328)\n",
            "Loss: 2.150 | Acc: 26.053% (2968/11392)\n",
            "Loss: 2.148 | Acc: 26.135% (2994/11456)\n",
            "Loss: 2.147 | Acc: 26.128% (3010/11520)\n",
            "Loss: 2.145 | Acc: 26.148% (3029/11584)\n",
            "Loss: 2.143 | Acc: 26.202% (3052/11648)\n",
            "Loss: 2.141 | Acc: 26.281% (3078/11712)\n",
            "Loss: 2.139 | Acc: 26.291% (3096/11776)\n",
            "Loss: 2.138 | Acc: 26.301% (3114/11840)\n",
            "Loss: 2.137 | Acc: 26.277% (3128/11904)\n",
            "Loss: 2.135 | Acc: 26.270% (3144/11968)\n",
            "Loss: 2.133 | Acc: 26.346% (3170/12032)\n",
            "Loss: 2.131 | Acc: 26.422% (3196/12096)\n",
            "Loss: 2.130 | Acc: 26.447% (3216/12160)\n",
            "Loss: 2.126 | Acc: 26.497% (3239/12224)\n",
            "Loss: 2.125 | Acc: 26.506% (3257/12288)\n",
            "Loss: 2.123 | Acc: 26.538% (3278/12352)\n",
            "Loss: 2.122 | Acc: 26.595% (3302/12416)\n",
            "Loss: 2.119 | Acc: 26.667% (3328/12480)\n",
            "Loss: 2.118 | Acc: 26.714% (3351/12544)\n",
            "Loss: 2.117 | Acc: 26.769% (3375/12608)\n",
            "Loss: 2.115 | Acc: 26.862% (3404/12672)\n",
            "Loss: 2.114 | Acc: 26.892% (3425/12736)\n",
            "Loss: 2.113 | Acc: 26.891% (3442/12800)\n",
            "Loss: 2.113 | Acc: 26.905% (3461/12864)\n",
            "Loss: 2.112 | Acc: 26.911% (3479/12928)\n",
            "Loss: 2.111 | Acc: 26.924% (3498/12992)\n",
            "Loss: 2.109 | Acc: 26.991% (3524/13056)\n",
            "Loss: 2.107 | Acc: 27.035% (3547/13120)\n",
            "Loss: 2.106 | Acc: 27.040% (3565/13184)\n",
            "Loss: 2.108 | Acc: 26.993% (3576/13248)\n",
            "Loss: 2.106 | Acc: 26.976% (3591/13312)\n",
            "Loss: 2.105 | Acc: 26.996% (3611/13376)\n",
            "Loss: 2.104 | Acc: 27.031% (3633/13440)\n",
            "Loss: 2.103 | Acc: 27.081% (3657/13504)\n",
            "Loss: 2.101 | Acc: 27.167% (3686/13568)\n",
            "Loss: 2.100 | Acc: 27.164% (3703/13632)\n",
            "Loss: 2.100 | Acc: 27.169% (3721/13696)\n",
            "Loss: 2.099 | Acc: 27.180% (3740/13760)\n",
            "Loss: 2.098 | Acc: 27.235% (3765/13824)\n",
            "Loss: 2.097 | Acc: 27.254% (3785/13888)\n",
            "Loss: 2.096 | Acc: 27.301% (3809/13952)\n",
            "Loss: 2.095 | Acc: 27.262% (3821/14016)\n",
            "Loss: 2.094 | Acc: 27.259% (3838/14080)\n",
            "Loss: 2.094 | Acc: 27.291% (3860/14144)\n",
            "Loss: 2.092 | Acc: 27.309% (3880/14208)\n",
            "Loss: 2.091 | Acc: 27.354% (3904/14272)\n",
            "Loss: 2.090 | Acc: 27.344% (3920/14336)\n",
            "Loss: 2.089 | Acc: 27.347% (3938/14400)\n",
            "Loss: 2.088 | Acc: 27.392% (3962/14464)\n",
            "Loss: 2.087 | Acc: 27.402% (3981/14528)\n",
            "Loss: 2.085 | Acc: 27.474% (4009/14592)\n",
            "Loss: 2.083 | Acc: 27.518% (4033/14656)\n",
            "Loss: 2.083 | Acc: 27.493% (4047/14720)\n",
            "Loss: 2.083 | Acc: 27.523% (4069/14784)\n",
            "Loss: 2.083 | Acc: 27.539% (4089/14848)\n",
            "Loss: 2.082 | Acc: 27.562% (4110/14912)\n",
            "Loss: 2.080 | Acc: 27.618% (4136/14976)\n",
            "Loss: 2.080 | Acc: 27.593% (4150/15040)\n",
            "Loss: 2.079 | Acc: 27.648% (4176/15104)\n",
            "Loss: 2.078 | Acc: 27.637% (4192/15168)\n",
            "Loss: 2.078 | Acc: 27.626% (4208/15232)\n",
            "Loss: 2.076 | Acc: 27.726% (4241/15296)\n",
            "Loss: 2.075 | Acc: 27.734% (4260/15360)\n",
            "Loss: 2.074 | Acc: 27.755% (4281/15424)\n",
            "Loss: 2.074 | Acc: 27.731% (4295/15488)\n",
            "Loss: 2.074 | Acc: 27.739% (4314/15552)\n",
            "Loss: 2.072 | Acc: 27.792% (4340/15616)\n",
            "Loss: 2.071 | Acc: 27.819% (4362/15680)\n",
            "Loss: 2.070 | Acc: 27.852% (4385/15744)\n",
            "Loss: 2.068 | Acc: 27.916% (4413/15808)\n",
            "Loss: 2.067 | Acc: 27.949% (4436/15872)\n",
            "Loss: 2.066 | Acc: 27.924% (4450/15936)\n",
            "Loss: 2.065 | Acc: 27.944% (4471/16000)\n",
            "Loss: 2.065 | Acc: 27.969% (4493/16064)\n",
            "Loss: 2.063 | Acc: 28.007% (4517/16128)\n",
            "Loss: 2.061 | Acc: 28.051% (4542/16192)\n",
            "Loss: 2.060 | Acc: 28.100% (4568/16256)\n",
            "Loss: 2.059 | Acc: 28.150% (4594/16320)\n",
            "Loss: 2.058 | Acc: 28.198% (4620/16384)\n",
            "Loss: 2.058 | Acc: 28.198% (4638/16448)\n",
            "Loss: 2.056 | Acc: 28.240% (4663/16512)\n",
            "Loss: 2.055 | Acc: 28.258% (4684/16576)\n",
            "Loss: 2.054 | Acc: 28.299% (4709/16640)\n",
            "Loss: 2.053 | Acc: 28.335% (4733/16704)\n",
            "Loss: 2.053 | Acc: 28.304% (4746/16768)\n",
            "Loss: 2.051 | Acc: 28.339% (4770/16832)\n",
            "Loss: 2.051 | Acc: 28.338% (4788/16896)\n",
            "Loss: 2.050 | Acc: 28.373% (4812/16960)\n",
            "Loss: 2.049 | Acc: 28.360% (4828/17024)\n",
            "Loss: 2.047 | Acc: 28.406% (4854/17088)\n",
            "Loss: 2.046 | Acc: 28.446% (4879/17152)\n",
            "Loss: 2.045 | Acc: 28.462% (4900/17216)\n",
            "Loss: 2.043 | Acc: 28.507% (4926/17280)\n",
            "Loss: 2.043 | Acc: 28.534% (4949/17344)\n",
            "Loss: 2.041 | Acc: 28.562% (4972/17408)\n",
            "Loss: 2.040 | Acc: 28.589% (4995/17472)\n",
            "Loss: 2.039 | Acc: 28.667% (5027/17536)\n",
            "Loss: 2.038 | Acc: 28.670% (5046/17600)\n",
            "Loss: 2.036 | Acc: 28.714% (5072/17664)\n",
            "Loss: 2.035 | Acc: 28.751% (5097/17728)\n",
            "Loss: 2.035 | Acc: 28.766% (5118/17792)\n",
            "Loss: 2.034 | Acc: 28.808% (5144/17856)\n",
            "Loss: 2.034 | Acc: 28.828% (5166/17920)\n",
            "Loss: 2.035 | Acc: 28.820% (5183/17984)\n",
            "Loss: 2.034 | Acc: 28.851% (5207/18048)\n",
            "Loss: 2.033 | Acc: 28.881% (5231/18112)\n",
            "Loss: 2.032 | Acc: 28.912% (5255/18176)\n",
            "Loss: 2.031 | Acc: 28.920% (5275/18240)\n",
            "Loss: 2.030 | Acc: 28.944% (5298/18304)\n",
            "Loss: 2.030 | Acc: 28.963% (5320/18368)\n",
            "Loss: 2.029 | Acc: 28.977% (5341/18432)\n",
            "Loss: 2.029 | Acc: 28.958% (5356/18496)\n",
            "Loss: 2.027 | Acc: 28.987% (5380/18560)\n",
            "Loss: 2.026 | Acc: 29.011% (5403/18624)\n",
            "Loss: 2.026 | Acc: 28.976% (5415/18688)\n",
            "Loss: 2.025 | Acc: 29.005% (5439/18752)\n",
            "Loss: 2.024 | Acc: 29.013% (5459/18816)\n",
            "Loss: 2.024 | Acc: 29.031% (5481/18880)\n",
            "Loss: 2.023 | Acc: 29.054% (5504/18944)\n",
            "Loss: 2.022 | Acc: 29.067% (5525/19008)\n",
            "Loss: 2.021 | Acc: 29.085% (5547/19072)\n",
            "Loss: 2.021 | Acc: 29.139% (5576/19136)\n",
            "Loss: 2.020 | Acc: 29.141% (5595/19200)\n",
            "Loss: 2.020 | Acc: 29.158% (5617/19264)\n",
            "Loss: 2.019 | Acc: 29.196% (5643/19328)\n",
            "Loss: 2.019 | Acc: 29.229% (5668/19392)\n",
            "Loss: 2.019 | Acc: 29.189% (5679/19456)\n",
            "Loss: 2.018 | Acc: 29.206% (5701/19520)\n",
            "Loss: 2.018 | Acc: 29.223% (5723/19584)\n",
            "Loss: 2.017 | Acc: 29.270% (5751/19648)\n",
            "Loss: 2.016 | Acc: 29.312% (5778/19712)\n",
            "Loss: 2.016 | Acc: 29.298% (5794/19776)\n",
            "Loss: 2.015 | Acc: 29.299% (5813/19840)\n",
            "Loss: 2.014 | Acc: 29.311% (5834/19904)\n",
            "Loss: 2.014 | Acc: 29.307% (5852/19968)\n",
            "Loss: 2.013 | Acc: 29.338% (5877/20032)\n",
            "Loss: 2.012 | Acc: 29.354% (5899/20096)\n",
            "Loss: 2.011 | Acc: 29.355% (5918/20160)\n",
            "Loss: 2.010 | Acc: 29.381% (5942/20224)\n",
            "Loss: 2.009 | Acc: 29.382% (5961/20288)\n",
            "Loss: 2.009 | Acc: 29.388% (5981/20352)\n",
            "Loss: 2.007 | Acc: 29.423% (6007/20416)\n",
            "Loss: 2.006 | Acc: 29.478% (6037/20480)\n",
            "Loss: 2.005 | Acc: 29.517% (6064/20544)\n",
            "Loss: 2.004 | Acc: 29.542% (6088/20608)\n",
            "Loss: 2.003 | Acc: 29.567% (6112/20672)\n",
            "Loss: 2.002 | Acc: 29.606% (6139/20736)\n",
            "Loss: 2.002 | Acc: 29.601% (6157/20800)\n",
            "Loss: 2.001 | Acc: 29.620% (6180/20864)\n",
            "Loss: 2.000 | Acc: 29.659% (6207/20928)\n",
            "Loss: 2.000 | Acc: 29.649% (6224/20992)\n",
            "Loss: 1.999 | Acc: 29.659% (6245/21056)\n",
            "Loss: 1.998 | Acc: 29.669% (6266/21120)\n",
            "Loss: 1.997 | Acc: 29.688% (6289/21184)\n",
            "Loss: 1.996 | Acc: 29.692% (6309/21248)\n",
            "Loss: 1.996 | Acc: 29.716% (6333/21312)\n",
            "Loss: 1.996 | Acc: 29.711% (6351/21376)\n",
            "Loss: 1.995 | Acc: 29.706% (6369/21440)\n",
            "Loss: 1.995 | Acc: 29.734% (6394/21504)\n",
            "Loss: 1.994 | Acc: 29.752% (6417/21568)\n",
            "Loss: 1.993 | Acc: 29.771% (6440/21632)\n",
            "Loss: 1.993 | Acc: 29.789% (6463/21696)\n",
            "Loss: 1.992 | Acc: 29.784% (6481/21760)\n",
            "Loss: 1.991 | Acc: 29.830% (6510/21824)\n",
            "Loss: 1.991 | Acc: 29.815% (6526/21888)\n",
            "Loss: 1.991 | Acc: 29.829% (6548/21952)\n",
            "Loss: 1.990 | Acc: 29.856% (6573/22016)\n",
            "Loss: 1.989 | Acc: 29.891% (6600/22080)\n",
            "Loss: 1.989 | Acc: 29.891% (6619/22144)\n",
            "Loss: 1.988 | Acc: 29.922% (6645/22208)\n",
            "Loss: 1.987 | Acc: 29.930% (6666/22272)\n",
            "Loss: 1.987 | Acc: 29.934% (6686/22336)\n",
            "Loss: 1.986 | Acc: 29.969% (6713/22400)\n",
            "Loss: 1.985 | Acc: 29.986% (6736/22464)\n",
            "Loss: 1.985 | Acc: 29.980% (6754/22528)\n",
            "Loss: 1.984 | Acc: 30.002% (6778/22592)\n",
            "Loss: 1.983 | Acc: 30.041% (6806/22656)\n",
            "Loss: 1.983 | Acc: 30.031% (6823/22720)\n",
            "Loss: 1.982 | Acc: 30.056% (6848/22784)\n",
            "Loss: 1.982 | Acc: 30.064% (6869/22848)\n",
            "Loss: 1.981 | Acc: 30.072% (6890/22912)\n",
            "Loss: 1.980 | Acc: 30.084% (6912/22976)\n",
            "Loss: 1.979 | Acc: 30.113% (6938/23040)\n",
            "Loss: 1.978 | Acc: 30.125% (6960/23104)\n",
            "Loss: 1.977 | Acc: 30.141% (6983/23168)\n",
            "Loss: 1.976 | Acc: 30.165% (7008/23232)\n",
            "Loss: 1.976 | Acc: 30.164% (7027/23296)\n",
            "Loss: 1.975 | Acc: 30.171% (7048/23360)\n",
            "Loss: 1.975 | Acc: 30.191% (7072/23424)\n",
            "Loss: 1.974 | Acc: 30.228% (7100/23488)\n",
            "Loss: 1.973 | Acc: 30.231% (7120/23552)\n",
            "Loss: 1.972 | Acc: 30.246% (7143/23616)\n",
            "Loss: 1.971 | Acc: 30.266% (7167/23680)\n",
            "Loss: 1.971 | Acc: 30.277% (7189/23744)\n",
            "Loss: 1.971 | Acc: 30.246% (7201/23808)\n",
            "Loss: 1.970 | Acc: 30.274% (7227/23872)\n",
            "Loss: 1.969 | Acc: 30.306% (7254/23936)\n",
            "Loss: 1.969 | Acc: 30.296% (7271/24000)\n",
            "Loss: 1.968 | Acc: 30.311% (7294/24064)\n",
            "Loss: 1.967 | Acc: 30.338% (7320/24128)\n",
            "Loss: 1.966 | Acc: 30.349% (7342/24192)\n",
            "Loss: 1.966 | Acc: 30.359% (7364/24256)\n",
            "Loss: 1.965 | Acc: 30.378% (7388/24320)\n",
            "Loss: 1.965 | Acc: 30.381% (7408/24384)\n",
            "Loss: 1.964 | Acc: 30.399% (7432/24448)\n",
            "Loss: 1.964 | Acc: 30.381% (7447/24512)\n",
            "Loss: 1.963 | Acc: 30.404% (7472/24576)\n",
            "Loss: 1.963 | Acc: 30.410% (7493/24640)\n",
            "Loss: 1.962 | Acc: 30.428% (7517/24704)\n",
            "Loss: 1.961 | Acc: 30.426% (7536/24768)\n",
            "Loss: 1.961 | Acc: 30.445% (7560/24832)\n",
            "Loss: 1.960 | Acc: 30.467% (7585/24896)\n",
            "Loss: 1.960 | Acc: 30.489% (7610/24960)\n",
            "Loss: 1.960 | Acc: 30.471% (7625/25024)\n",
            "Loss: 1.960 | Acc: 30.485% (7648/25088)\n",
            "Loss: 1.959 | Acc: 30.518% (7676/25152)\n",
            "Loss: 1.959 | Acc: 30.536% (7700/25216)\n",
            "Loss: 1.960 | Acc: 30.546% (7722/25280)\n",
            "Loss: 1.959 | Acc: 30.548% (7742/25344)\n",
            "Loss: 1.958 | Acc: 30.561% (7765/25408)\n",
            "Loss: 1.958 | Acc: 30.602% (7795/25472)\n",
            "Loss: 1.957 | Acc: 30.623% (7820/25536)\n",
            "Loss: 1.956 | Acc: 30.633% (7842/25600)\n",
            "Loss: 1.956 | Acc: 30.646% (7865/25664)\n",
            "Loss: 1.955 | Acc: 30.694% (7897/25728)\n",
            "Loss: 1.954 | Acc: 30.719% (7923/25792)\n",
            "Loss: 1.954 | Acc: 30.736% (7947/25856)\n",
            "Loss: 1.953 | Acc: 30.756% (7972/25920)\n",
            "Loss: 1.953 | Acc: 30.769% (7995/25984)\n",
            "Loss: 1.952 | Acc: 30.805% (8024/26048)\n",
            "Loss: 1.952 | Acc: 30.813% (8046/26112)\n",
            "Loss: 1.951 | Acc: 30.830% (8070/26176)\n",
            "Loss: 1.950 | Acc: 30.846% (8094/26240)\n",
            "Loss: 1.950 | Acc: 30.866% (8119/26304)\n",
            "Loss: 1.949 | Acc: 30.871% (8140/26368)\n",
            "Loss: 1.948 | Acc: 30.898% (8167/26432)\n",
            "Loss: 1.948 | Acc: 30.914% (8191/26496)\n",
            "Loss: 1.947 | Acc: 30.907% (8209/26560)\n",
            "Loss: 1.947 | Acc: 30.934% (8236/26624)\n",
            "Loss: 1.946 | Acc: 30.939% (8257/26688)\n",
            "Loss: 1.946 | Acc: 30.929% (8274/26752)\n",
            "Loss: 1.946 | Acc: 30.955% (8301/26816)\n",
            "Loss: 1.945 | Acc: 30.982% (8328/26880)\n",
            "Loss: 1.944 | Acc: 30.990% (8350/26944)\n",
            "Loss: 1.944 | Acc: 30.998% (8372/27008)\n",
            "Loss: 1.943 | Acc: 31.028% (8400/27072)\n",
            "Loss: 1.943 | Acc: 31.036% (8422/27136)\n",
            "Loss: 1.943 | Acc: 31.037% (8442/27200)\n",
            "Loss: 1.942 | Acc: 31.048% (8465/27264)\n",
            "Loss: 1.942 | Acc: 31.071% (8491/27328)\n",
            "Loss: 1.942 | Acc: 31.071% (8511/27392)\n",
            "Loss: 1.941 | Acc: 31.097% (8538/27456)\n",
            "Loss: 1.941 | Acc: 31.101% (8559/27520)\n",
            "Loss: 1.940 | Acc: 31.127% (8586/27584)\n",
            "Loss: 1.940 | Acc: 31.138% (8609/27648)\n",
            "Loss: 1.939 | Acc: 31.149% (8632/27712)\n",
            "Loss: 1.939 | Acc: 31.138% (8649/27776)\n",
            "Loss: 1.939 | Acc: 31.157% (8674/27840)\n",
            "Loss: 1.938 | Acc: 31.175% (8699/27904)\n",
            "Loss: 1.938 | Acc: 31.178% (8720/27968)\n",
            "Loss: 1.937 | Acc: 31.200% (8746/28032)\n",
            "Loss: 1.937 | Acc: 31.214% (8770/28096)\n",
            "Loss: 1.936 | Acc: 31.236% (8796/28160)\n",
            "Loss: 1.936 | Acc: 31.250% (8820/28224)\n",
            "Loss: 1.935 | Acc: 31.264% (8844/28288)\n",
            "Loss: 1.935 | Acc: 31.289% (8871/28352)\n",
            "Loss: 1.934 | Acc: 31.317% (8899/28416)\n",
            "Loss: 1.933 | Acc: 31.355% (8930/28480)\n",
            "Loss: 1.933 | Acc: 31.359% (8951/28544)\n",
            "Loss: 1.933 | Acc: 31.330% (8963/28608)\n",
            "Loss: 1.933 | Acc: 31.355% (8990/28672)\n",
            "Loss: 1.932 | Acc: 31.361% (9012/28736)\n",
            "Loss: 1.931 | Acc: 31.375% (9036/28800)\n",
            "Loss: 1.931 | Acc: 31.382% (9058/28864)\n",
            "Loss: 1.930 | Acc: 31.416% (9088/28928)\n",
            "Loss: 1.930 | Acc: 31.440% (9115/28992)\n",
            "Loss: 1.929 | Acc: 31.477% (9146/29056)\n",
            "Loss: 1.928 | Acc: 31.484% (9168/29120)\n",
            "Loss: 1.928 | Acc: 31.497% (9192/29184)\n",
            "Loss: 1.927 | Acc: 31.496% (9212/29248)\n",
            "Loss: 1.926 | Acc: 31.496% (9232/29312)\n",
            "Loss: 1.926 | Acc: 31.519% (9259/29376)\n",
            "Loss: 1.925 | Acc: 31.518% (9279/29440)\n",
            "Loss: 1.925 | Acc: 31.555% (9310/29504)\n",
            "Loss: 1.925 | Acc: 31.568% (9334/29568)\n",
            "Loss: 1.924 | Acc: 31.591% (9361/29632)\n",
            "Loss: 1.924 | Acc: 31.593% (9382/29696)\n",
            "Loss: 1.923 | Acc: 31.606% (9406/29760)\n",
            "Loss: 1.923 | Acc: 31.605% (9426/29824)\n",
            "Loss: 1.923 | Acc: 31.615% (9449/29888)\n",
            "Loss: 1.922 | Acc: 31.614% (9469/29952)\n",
            "Loss: 1.922 | Acc: 31.620% (9491/30016)\n",
            "Loss: 1.922 | Acc: 31.619% (9511/30080)\n",
            "Loss: 1.922 | Acc: 31.618% (9531/30144)\n",
            "Loss: 1.921 | Acc: 31.631% (9555/30208)\n",
            "Loss: 1.921 | Acc: 31.640% (9578/30272)\n",
            "Loss: 1.921 | Acc: 31.642% (9599/30336)\n",
            "Loss: 1.920 | Acc: 31.671% (9628/30400)\n",
            "Loss: 1.920 | Acc: 31.690% (9654/30464)\n",
            "Loss: 1.919 | Acc: 31.699% (9677/30528)\n",
            "Loss: 1.919 | Acc: 31.711% (9701/30592)\n",
            "Loss: 1.919 | Acc: 31.710% (9721/30656)\n",
            "Loss: 1.918 | Acc: 31.735% (9749/30720)\n",
            "Loss: 1.917 | Acc: 31.757% (9776/30784)\n",
            "Loss: 1.916 | Acc: 31.765% (9799/30848)\n",
            "Loss: 1.916 | Acc: 31.787% (9826/30912)\n",
            "Loss: 1.916 | Acc: 31.805% (9852/30976)\n",
            "Loss: 1.915 | Acc: 31.840% (9883/31040)\n",
            "Loss: 1.914 | Acc: 31.854% (9908/31104)\n",
            "Loss: 1.915 | Acc: 31.856% (9929/31168)\n",
            "Loss: 1.914 | Acc: 31.874% (9955/31232)\n",
            "Loss: 1.914 | Acc: 31.873% (9975/31296)\n",
            "Loss: 1.914 | Acc: 31.888% (10000/31360)\n",
            "Loss: 1.914 | Acc: 31.883% (10019/31424)\n",
            "Loss: 1.913 | Acc: 31.898% (10044/31488)\n",
            "Loss: 1.913 | Acc: 31.916% (10070/31552)\n",
            "Loss: 1.912 | Acc: 31.943% (10099/31616)\n",
            "Loss: 1.911 | Acc: 31.976% (10130/31680)\n",
            "Loss: 1.911 | Acc: 31.981% (10152/31744)\n",
            "Loss: 1.910 | Acc: 31.995% (10177/31808)\n",
            "Loss: 1.910 | Acc: 32.025% (10207/31872)\n",
            "Loss: 1.909 | Acc: 32.027% (10228/31936)\n",
            "Loss: 1.909 | Acc: 32.022% (10247/32000)\n",
            "Loss: 1.909 | Acc: 32.020% (10267/32064)\n",
            "Loss: 1.908 | Acc: 32.022% (10288/32128)\n",
            "Loss: 1.908 | Acc: 32.048% (10317/32192)\n",
            "Loss: 1.908 | Acc: 32.062% (10342/32256)\n",
            "Loss: 1.907 | Acc: 32.061% (10362/32320)\n",
            "Loss: 1.906 | Acc: 32.068% (10385/32384)\n",
            "Loss: 1.906 | Acc: 32.073% (10407/32448)\n",
            "Loss: 1.906 | Acc: 32.087% (10432/32512)\n",
            "Loss: 1.905 | Acc: 32.113% (10461/32576)\n",
            "Loss: 1.905 | Acc: 32.120% (10484/32640)\n",
            "Loss: 1.904 | Acc: 32.143% (10512/32704)\n",
            "Loss: 1.904 | Acc: 32.129% (10528/32768)\n",
            "Loss: 1.903 | Acc: 32.124% (10547/32832)\n",
            "Loss: 1.903 | Acc: 32.141% (10573/32896)\n",
            "Loss: 1.903 | Acc: 32.154% (10598/32960)\n",
            "Loss: 1.902 | Acc: 32.158% (10620/33024)\n",
            "Loss: 1.902 | Acc: 32.175% (10646/33088)\n",
            "Loss: 1.901 | Acc: 32.170% (10665/33152)\n",
            "Loss: 1.901 | Acc: 32.201% (10696/33216)\n",
            "Loss: 1.900 | Acc: 32.209% (10719/33280)\n",
            "Loss: 1.900 | Acc: 32.219% (10743/33344)\n",
            "Loss: 1.899 | Acc: 32.238% (10770/33408)\n",
            "Loss: 1.899 | Acc: 32.242% (10792/33472)\n",
            "Loss: 1.898 | Acc: 32.261% (10819/33536)\n",
            "Loss: 1.897 | Acc: 32.277% (10845/33600)\n",
            "Loss: 1.897 | Acc: 32.281% (10867/33664)\n",
            "Loss: 1.897 | Acc: 32.297% (10893/33728)\n",
            "Loss: 1.896 | Acc: 32.304% (10916/33792)\n",
            "Loss: 1.896 | Acc: 32.331% (10946/33856)\n",
            "Loss: 1.896 | Acc: 32.350% (10973/33920)\n",
            "Loss: 1.895 | Acc: 32.348% (10993/33984)\n",
            "Loss: 1.895 | Acc: 32.384% (11026/34048)\n",
            "Loss: 1.894 | Acc: 32.408% (11055/34112)\n",
            "Loss: 1.893 | Acc: 32.441% (11087/34176)\n",
            "Loss: 1.893 | Acc: 32.445% (11109/34240)\n",
            "Loss: 1.892 | Acc: 32.466% (11137/34304)\n",
            "Loss: 1.892 | Acc: 32.481% (11163/34368)\n",
            "Loss: 1.891 | Acc: 32.493% (11188/34432)\n",
            "Loss: 1.891 | Acc: 32.514% (11216/34496)\n",
            "Loss: 1.891 | Acc: 32.514% (11237/34560)\n",
            "Loss: 1.890 | Acc: 32.532% (11264/34624)\n",
            "Loss: 1.890 | Acc: 32.550% (11291/34688)\n",
            "Loss: 1.889 | Acc: 32.565% (11317/34752)\n",
            "Loss: 1.889 | Acc: 32.568% (11339/34816)\n",
            "Loss: 1.889 | Acc: 32.563% (11358/34880)\n",
            "Loss: 1.889 | Acc: 32.581% (11385/34944)\n",
            "Loss: 1.888 | Acc: 32.604% (11414/35008)\n",
            "Loss: 1.887 | Acc: 32.613% (11438/35072)\n",
            "Loss: 1.887 | Acc: 32.642% (11469/35136)\n",
            "Loss: 1.886 | Acc: 32.656% (11495/35200)\n",
            "Loss: 1.886 | Acc: 32.674% (11522/35264)\n",
            "Loss: 1.885 | Acc: 32.679% (11545/35328)\n",
            "Loss: 1.885 | Acc: 32.674% (11564/35392)\n",
            "Loss: 1.884 | Acc: 32.705% (11596/35456)\n",
            "Loss: 1.884 | Acc: 32.734% (11627/35520)\n",
            "Loss: 1.883 | Acc: 32.765% (11659/35584)\n",
            "Loss: 1.882 | Acc: 32.776% (11684/35648)\n",
            "Loss: 1.882 | Acc: 32.787% (11709/35712)\n",
            "Loss: 1.882 | Acc: 32.793% (11732/35776)\n",
            "Loss: 1.881 | Acc: 32.821% (11763/35840)\n",
            "Loss: 1.881 | Acc: 32.832% (11788/35904)\n",
            "Loss: 1.880 | Acc: 32.843% (11813/35968)\n",
            "Loss: 1.880 | Acc: 32.854% (11838/36032)\n",
            "Loss: 1.879 | Acc: 32.871% (11865/36096)\n",
            "Loss: 1.879 | Acc: 32.890% (11893/36160)\n",
            "Loss: 1.878 | Acc: 32.909% (11921/36224)\n",
            "Loss: 1.878 | Acc: 32.928% (11949/36288)\n",
            "Loss: 1.878 | Acc: 32.920% (11967/36352)\n",
            "Loss: 1.877 | Acc: 32.942% (11996/36416)\n",
            "Loss: 1.877 | Acc: 32.950% (12020/36480)\n",
            "Loss: 1.877 | Acc: 32.944% (12039/36544)\n",
            "Loss: 1.876 | Acc: 32.965% (12068/36608)\n",
            "Loss: 1.876 | Acc: 32.984% (12096/36672)\n",
            "Loss: 1.876 | Acc: 32.981% (12116/36736)\n",
            "Loss: 1.876 | Acc: 33.003% (12145/36800)\n",
            "Loss: 1.875 | Acc: 33.021% (12173/36864)\n",
            "Loss: 1.874 | Acc: 33.040% (12201/36928)\n",
            "Loss: 1.874 | Acc: 33.058% (12229/36992)\n",
            "Loss: 1.874 | Acc: 33.066% (12253/37056)\n",
            "Loss: 1.873 | Acc: 33.074% (12277/37120)\n",
            "Loss: 1.873 | Acc: 33.092% (12305/37184)\n",
            "Loss: 1.872 | Acc: 33.102% (12330/37248)\n",
            "Loss: 1.872 | Acc: 33.094% (12348/37312)\n",
            "Loss: 1.872 | Acc: 33.104% (12373/37376)\n",
            "Loss: 1.871 | Acc: 33.106% (12395/37440)\n",
            "Loss: 1.871 | Acc: 33.103% (12415/37504)\n",
            "Loss: 1.871 | Acc: 33.119% (12442/37568)\n",
            "Loss: 1.870 | Acc: 33.137% (12470/37632)\n",
            "Loss: 1.870 | Acc: 33.139% (12492/37696)\n",
            "Loss: 1.870 | Acc: 33.149% (12517/37760)\n",
            "Loss: 1.869 | Acc: 33.154% (12540/37824)\n",
            "Loss: 1.869 | Acc: 33.153% (12561/37888)\n",
            "Loss: 1.869 | Acc: 33.166% (12587/37952)\n",
            "Loss: 1.869 | Acc: 33.178% (12613/38016)\n",
            "Loss: 1.868 | Acc: 33.191% (12639/38080)\n",
            "Loss: 1.868 | Acc: 33.201% (12664/38144)\n",
            "Loss: 1.868 | Acc: 33.200% (12685/38208)\n",
            "Loss: 1.867 | Acc: 33.231% (12718/38272)\n",
            "Loss: 1.867 | Acc: 33.235% (12741/38336)\n",
            "Loss: 1.866 | Acc: 33.253% (12769/38400)\n",
            "Loss: 1.866 | Acc: 33.265% (12795/38464)\n",
            "Loss: 1.865 | Acc: 33.282% (12823/38528)\n",
            "Loss: 1.865 | Acc: 33.289% (12847/38592)\n",
            "Loss: 1.865 | Acc: 33.312% (12877/38656)\n",
            "Loss: 1.864 | Acc: 33.319% (12901/38720)\n",
            "Loss: 1.864 | Acc: 33.336% (12929/38784)\n",
            "Loss: 1.863 | Acc: 33.345% (12954/38848)\n",
            "Loss: 1.863 | Acc: 33.380% (12989/38912)\n",
            "Loss: 1.862 | Acc: 33.382% (13011/38976)\n",
            "Loss: 1.862 | Acc: 33.394% (13037/39040)\n",
            "Loss: 1.862 | Acc: 33.406% (13063/39104)\n",
            "Loss: 1.861 | Acc: 33.423% (13091/39168)\n",
            "Loss: 1.860 | Acc: 33.432% (13116/39232)\n",
            "Loss: 1.860 | Acc: 33.449% (13144/39296)\n",
            "Loss: 1.860 | Acc: 33.471% (13174/39360)\n",
            "Loss: 1.859 | Acc: 33.487% (13202/39424)\n",
            "Loss: 1.859 | Acc: 33.506% (13231/39488)\n",
            "Loss: 1.858 | Acc: 33.520% (13258/39552)\n",
            "Loss: 1.857 | Acc: 33.524% (13281/39616)\n",
            "Loss: 1.857 | Acc: 33.538% (13308/39680)\n",
            "Loss: 1.857 | Acc: 33.550% (13334/39744)\n",
            "Loss: 1.856 | Acc: 33.554% (13357/39808)\n",
            "Loss: 1.856 | Acc: 33.555% (13379/39872)\n",
            "Loss: 1.855 | Acc: 33.576% (13409/39936)\n",
            "Loss: 1.855 | Acc: 33.587% (13435/40000)\n",
            "Loss: 1.854 | Acc: 33.609% (13465/40064)\n",
            "Loss: 1.854 | Acc: 33.617% (13490/40128)\n",
            "Loss: 1.854 | Acc: 33.621% (13513/40192)\n",
            "Loss: 1.854 | Acc: 33.640% (13542/40256)\n",
            "Loss: 1.853 | Acc: 33.656% (13570/40320)\n",
            "Loss: 1.853 | Acc: 33.667% (13596/40384)\n",
            "Loss: 1.852 | Acc: 33.675% (13621/40448)\n",
            "Loss: 1.852 | Acc: 33.679% (13644/40512)\n",
            "Loss: 1.851 | Acc: 33.692% (13671/40576)\n",
            "Loss: 1.851 | Acc: 33.701% (13696/40640)\n",
            "Loss: 1.851 | Acc: 33.694% (13715/40704)\n",
            "Loss: 1.850 | Acc: 33.705% (13741/40768)\n",
            "Loss: 1.850 | Acc: 33.714% (13766/40832)\n",
            "Loss: 1.849 | Acc: 33.734% (13796/40896)\n",
            "Loss: 1.849 | Acc: 33.733% (13817/40960)\n",
            "Loss: 1.849 | Acc: 33.739% (13841/41024)\n",
            "Loss: 1.848 | Acc: 33.762% (13872/41088)\n",
            "Loss: 1.848 | Acc: 33.777% (13900/41152)\n",
            "Loss: 1.848 | Acc: 33.766% (13917/41216)\n",
            "Loss: 1.847 | Acc: 33.781% (13945/41280)\n",
            "Loss: 1.847 | Acc: 33.790% (13970/41344)\n",
            "Loss: 1.847 | Acc: 33.788% (13991/41408)\n",
            "Loss: 1.846 | Acc: 33.799% (14017/41472)\n",
            "Loss: 1.846 | Acc: 33.812% (14044/41536)\n",
            "Loss: 1.845 | Acc: 33.820% (14069/41600)\n",
            "Loss: 1.845 | Acc: 33.842% (14100/41664)\n",
            "Loss: 1.844 | Acc: 33.848% (14124/41728)\n",
            "Loss: 1.844 | Acc: 33.882% (14160/41792)\n",
            "Loss: 1.843 | Acc: 33.890% (14185/41856)\n",
            "Loss: 1.843 | Acc: 33.905% (14213/41920)\n",
            "Loss: 1.843 | Acc: 33.920% (14241/41984)\n",
            "Loss: 1.842 | Acc: 33.918% (14262/42048)\n",
            "Loss: 1.842 | Acc: 33.914% (14282/42112)\n",
            "Loss: 1.841 | Acc: 33.943% (14316/42176)\n",
            "Loss: 1.841 | Acc: 33.961% (14345/42240)\n",
            "Loss: 1.840 | Acc: 33.973% (14372/42304)\n",
            "Loss: 1.840 | Acc: 33.978% (14396/42368)\n",
            "Loss: 1.840 | Acc: 33.981% (14419/42432)\n",
            "Loss: 1.839 | Acc: 33.991% (14445/42496)\n",
            "Loss: 1.839 | Acc: 34.004% (14472/42560)\n",
            "Loss: 1.839 | Acc: 34.002% (14493/42624)\n",
            "Loss: 1.839 | Acc: 34.007% (14517/42688)\n",
            "Loss: 1.839 | Acc: 34.017% (14543/42752)\n",
            "Loss: 1.838 | Acc: 34.027% (14569/42816)\n",
            "Loss: 1.838 | Acc: 34.021% (14588/42880)\n",
            "Loss: 1.838 | Acc: 34.033% (14615/42944)\n",
            "Loss: 1.838 | Acc: 34.033% (14637/43008)\n",
            "Loss: 1.837 | Acc: 34.045% (14664/43072)\n",
            "Loss: 1.837 | Acc: 34.053% (14689/43136)\n",
            "Loss: 1.837 | Acc: 34.053% (14711/43200)\n",
            "Loss: 1.836 | Acc: 34.068% (14739/43264)\n",
            "Loss: 1.836 | Acc: 34.075% (14764/43328)\n",
            "Loss: 1.836 | Acc: 34.064% (14781/43392)\n",
            "Loss: 1.836 | Acc: 34.076% (14808/43456)\n",
            "Loss: 1.835 | Acc: 34.092% (14837/43520)\n",
            "Loss: 1.835 | Acc: 34.104% (14864/43584)\n",
            "Loss: 1.835 | Acc: 34.100% (14884/43648)\n",
            "Loss: 1.834 | Acc: 34.114% (14912/43712)\n",
            "Loss: 1.834 | Acc: 34.115% (14934/43776)\n",
            "Loss: 1.834 | Acc: 34.110% (14954/43840)\n",
            "Loss: 1.833 | Acc: 34.134% (14986/43904)\n",
            "Loss: 1.833 | Acc: 34.143% (15012/43968)\n",
            "Loss: 1.832 | Acc: 34.150% (15037/44032)\n",
            "Loss: 1.832 | Acc: 34.153% (15060/44096)\n",
            "Loss: 1.832 | Acc: 34.162% (15086/44160)\n",
            "Loss: 1.831 | Acc: 34.183% (15117/44224)\n",
            "Loss: 1.831 | Acc: 34.206% (15149/44288)\n",
            "Loss: 1.830 | Acc: 34.226% (15180/44352)\n",
            "Loss: 1.830 | Acc: 34.220% (15199/44416)\n",
            "Loss: 1.830 | Acc: 34.233% (15227/44480)\n",
            "Loss: 1.829 | Acc: 34.249% (15256/44544)\n",
            "Loss: 1.828 | Acc: 34.265% (15285/44608)\n",
            "Loss: 1.828 | Acc: 34.268% (15308/44672)\n",
            "Loss: 1.828 | Acc: 34.274% (15333/44736)\n",
            "Loss: 1.827 | Acc: 34.292% (15363/44800)\n",
            "Loss: 1.827 | Acc: 34.299% (15388/44864)\n",
            "Loss: 1.827 | Acc: 34.295% (15408/44928)\n",
            "Loss: 1.826 | Acc: 34.302% (15433/44992)\n",
            "Loss: 1.826 | Acc: 34.322% (15464/45056)\n",
            "Loss: 1.825 | Acc: 34.315% (15483/45120)\n",
            "Loss: 1.825 | Acc: 34.322% (15508/45184)\n",
            "Loss: 1.825 | Acc: 34.318% (15528/45248)\n",
            "Loss: 1.824 | Acc: 34.335% (15558/45312)\n",
            "Loss: 1.824 | Acc: 34.331% (15578/45376)\n",
            "Loss: 1.824 | Acc: 34.333% (15601/45440)\n",
            "Loss: 1.823 | Acc: 34.342% (15627/45504)\n",
            "Loss: 1.823 | Acc: 34.362% (15658/45568)\n",
            "Loss: 1.823 | Acc: 34.360% (15679/45632)\n",
            "Loss: 1.822 | Acc: 34.371% (15706/45696)\n",
            "Loss: 1.822 | Acc: 34.386% (15735/45760)\n",
            "Loss: 1.821 | Acc: 34.408% (15767/45824)\n",
            "Loss: 1.821 | Acc: 34.419% (15794/45888)\n",
            "Loss: 1.821 | Acc: 34.438% (15825/45952)\n",
            "Loss: 1.821 | Acc: 34.436% (15846/46016)\n",
            "Loss: 1.820 | Acc: 34.460% (15879/46080)\n",
            "Loss: 1.820 | Acc: 34.466% (15904/46144)\n",
            "Loss: 1.819 | Acc: 34.464% (15925/46208)\n",
            "Loss: 1.819 | Acc: 34.485% (15957/46272)\n",
            "Loss: 1.818 | Acc: 34.507% (15989/46336)\n",
            "Loss: 1.818 | Acc: 34.509% (16012/46400)\n",
            "Loss: 1.818 | Acc: 34.515% (16037/46464)\n",
            "Loss: 1.818 | Acc: 34.515% (16059/46528)\n",
            "Loss: 1.818 | Acc: 34.523% (16085/46592)\n",
            "Loss: 1.817 | Acc: 34.536% (16113/46656)\n",
            "Loss: 1.817 | Acc: 34.548% (16141/46720)\n",
            "Loss: 1.817 | Acc: 34.559% (16168/46784)\n",
            "Loss: 1.817 | Acc: 34.561% (16191/46848)\n",
            "Loss: 1.817 | Acc: 34.560% (16213/46912)\n",
            "Loss: 1.817 | Acc: 34.564% (16237/46976)\n",
            "Loss: 1.816 | Acc: 34.581% (16267/47040)\n",
            "Loss: 1.815 | Acc: 34.606% (16301/47104)\n",
            "Loss: 1.815 | Acc: 34.623% (16331/47168)\n",
            "Loss: 1.814 | Acc: 34.638% (16360/47232)\n",
            "Loss: 1.814 | Acc: 34.669% (16397/47296)\n",
            "Loss: 1.813 | Acc: 34.679% (16424/47360)\n",
            "Loss: 1.813 | Acc: 34.691% (16452/47424)\n",
            "Loss: 1.812 | Acc: 34.704% (16480/47488)\n",
            "Loss: 1.812 | Acc: 34.714% (16507/47552)\n",
            "Loss: 1.812 | Acc: 34.726% (16535/47616)\n",
            "Loss: 1.811 | Acc: 34.736% (16562/47680)\n",
            "Loss: 1.811 | Acc: 34.748% (16590/47744)\n",
            "Loss: 1.811 | Acc: 34.758% (16617/47808)\n",
            "Loss: 1.810 | Acc: 34.780% (16650/47872)\n",
            "Loss: 1.810 | Acc: 34.786% (16675/47936)\n",
            "Loss: 1.810 | Acc: 34.798% (16703/48000)\n",
            "Loss: 1.810 | Acc: 34.799% (16726/48064)\n",
            "Loss: 1.809 | Acc: 34.828% (16762/48128)\n",
            "Loss: 1.809 | Acc: 34.842% (16791/48192)\n",
            "Loss: 1.809 | Acc: 34.852% (16818/48256)\n",
            "Loss: 1.808 | Acc: 34.863% (16846/48320)\n",
            "Loss: 1.808 | Acc: 34.869% (16871/48384)\n",
            "Loss: 1.808 | Acc: 34.883% (16900/48448)\n",
            "Loss: 1.807 | Acc: 34.892% (16927/48512)\n",
            "Loss: 1.807 | Acc: 34.906% (16956/48576)\n",
            "Loss: 1.807 | Acc: 34.916% (16983/48640)\n",
            "Loss: 1.807 | Acc: 34.921% (17008/48704)\n",
            "Loss: 1.806 | Acc: 34.933% (17036/48768)\n",
            "Loss: 1.806 | Acc: 34.948% (17066/48832)\n",
            "Loss: 1.806 | Acc: 34.958% (17093/48896)\n",
            "Loss: 1.805 | Acc: 34.963% (17118/48960)\n",
            "Loss: 1.805 | Acc: 34.967% (17134/49000)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 34.96734693877551\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.410 | Acc: 42.188% (27/64)\n",
            "Loss: 1.516 | Acc: 41.406% (53/128)\n",
            "Loss: 1.550 | Acc: 40.104% (77/192)\n",
            "Loss: 1.596 | Acc: 40.234% (103/256)\n",
            "Loss: 1.552 | Acc: 40.938% (131/320)\n",
            "Loss: 1.560 | Acc: 40.625% (156/384)\n",
            "Loss: 1.560 | Acc: 40.848% (183/448)\n",
            "Loss: 1.563 | Acc: 41.602% (213/512)\n",
            "Loss: 1.541 | Acc: 42.361% (244/576)\n",
            "Loss: 1.532 | Acc: 43.438% (278/640)\n",
            "Loss: 1.546 | Acc: 43.040% (303/704)\n",
            "Loss: 1.561 | Acc: 42.578% (327/768)\n",
            "Loss: 1.554 | Acc: 43.029% (358/832)\n",
            "Loss: 1.559 | Acc: 42.411% (380/896)\n",
            "Loss: 1.552 | Acc: 42.708% (410/960)\n",
            "Loss: 1.541 | Acc: 43.652% (447/1024)\n",
            "Loss: 1.550 | Acc: 43.290% (471/1088)\n",
            "Loss: 1.548 | Acc: 43.142% (497/1152)\n",
            "Loss: 1.543 | Acc: 43.257% (526/1216)\n",
            "Loss: 1.551 | Acc: 42.656% (546/1280)\n",
            "Loss: 1.543 | Acc: 43.006% (578/1344)\n",
            "Loss: 1.545 | Acc: 42.969% (605/1408)\n",
            "Loss: 1.535 | Acc: 43.274% (637/1472)\n",
            "Loss: 1.539 | Acc: 43.229% (664/1536)\n",
            "Loss: 1.541 | Acc: 43.500% (696/1600)\n",
            "Loss: 1.541 | Acc: 43.630% (726/1664)\n",
            "Loss: 1.541 | Acc: 43.924% (759/1728)\n",
            "Loss: 1.537 | Acc: 43.917% (787/1792)\n",
            "Loss: 1.531 | Acc: 44.073% (818/1856)\n",
            "Loss: 1.530 | Acc: 44.219% (849/1920)\n",
            "Loss: 1.531 | Acc: 44.153% (876/1984)\n",
            "Loss: 1.532 | Acc: 43.945% (900/2048)\n",
            "Loss: 1.531 | Acc: 44.034% (930/2112)\n",
            "Loss: 1.529 | Acc: 44.072% (959/2176)\n",
            "Loss: 1.531 | Acc: 44.062% (987/2240)\n",
            "Loss: 1.531 | Acc: 44.010% (1014/2304)\n",
            "Loss: 1.531 | Acc: 44.046% (1043/2368)\n",
            "Loss: 1.531 | Acc: 44.202% (1075/2432)\n",
            "Loss: 1.529 | Acc: 44.351% (1107/2496)\n",
            "Loss: 1.536 | Acc: 44.141% (1130/2560)\n",
            "Loss: 1.539 | Acc: 43.979% (1154/2624)\n",
            "Loss: 1.540 | Acc: 44.010% (1183/2688)\n",
            "Loss: 1.539 | Acc: 43.859% (1207/2752)\n",
            "Loss: 1.536 | Acc: 43.928% (1237/2816)\n",
            "Loss: 1.535 | Acc: 43.993% (1267/2880)\n",
            "Loss: 1.535 | Acc: 43.920% (1293/2944)\n",
            "Loss: 1.534 | Acc: 44.116% (1327/3008)\n",
            "Loss: 1.535 | Acc: 44.010% (1352/3072)\n",
            "Loss: 1.535 | Acc: 44.069% (1382/3136)\n",
            "Loss: 1.534 | Acc: 44.156% (1413/3200)\n",
            "Loss: 1.533 | Acc: 44.056% (1438/3264)\n",
            "Loss: 1.533 | Acc: 44.050% (1466/3328)\n",
            "Loss: 1.532 | Acc: 44.045% (1494/3392)\n",
            "Loss: 1.532 | Acc: 44.184% (1527/3456)\n",
            "Loss: 1.533 | Acc: 44.119% (1553/3520)\n",
            "Loss: 1.532 | Acc: 44.169% (1583/3584)\n",
            "Loss: 1.533 | Acc: 44.106% (1609/3648)\n",
            "Loss: 1.531 | Acc: 44.208% (1641/3712)\n",
            "Loss: 1.532 | Acc: 44.174% (1668/3776)\n",
            "Loss: 1.532 | Acc: 44.193% (1697/3840)\n",
            "Loss: 1.533 | Acc: 44.160% (1724/3904)\n",
            "Loss: 1.532 | Acc: 44.204% (1754/3968)\n",
            "Loss: 1.534 | Acc: 44.221% (1783/4032)\n",
            "Loss: 1.535 | Acc: 44.092% (1806/4096)\n",
            "Loss: 1.537 | Acc: 43.942% (1828/4160)\n",
            "Loss: 1.537 | Acc: 44.034% (1860/4224)\n",
            "Loss: 1.536 | Acc: 43.937% (1884/4288)\n",
            "Loss: 1.535 | Acc: 44.049% (1917/4352)\n",
            "Loss: 1.533 | Acc: 44.158% (1950/4416)\n",
            "Loss: 1.532 | Acc: 44.152% (1978/4480)\n",
            "Loss: 1.532 | Acc: 44.124% (2005/4544)\n",
            "Loss: 1.534 | Acc: 44.097% (2032/4608)\n",
            "Loss: 1.533 | Acc: 44.157% (2063/4672)\n",
            "Loss: 1.531 | Acc: 44.236% (2095/4736)\n",
            "Loss: 1.534 | Acc: 44.292% (2126/4800)\n",
            "Loss: 1.532 | Acc: 44.387% (2159/4864)\n",
            "Loss: 1.532 | Acc: 44.399% (2188/4928)\n",
            "Loss: 1.531 | Acc: 44.491% (2221/4992)\n",
            "Loss: 1.531 | Acc: 44.462% (2248/5056)\n",
            "Loss: 1.532 | Acc: 44.375% (2272/5120)\n",
            "Loss: 1.530 | Acc: 44.387% (2301/5184)\n",
            "Loss: 1.532 | Acc: 44.245% (2322/5248)\n",
            "Loss: 1.531 | Acc: 44.239% (2350/5312)\n",
            "Loss: 1.531 | Acc: 44.196% (2376/5376)\n",
            "Loss: 1.532 | Acc: 44.118% (2400/5440)\n",
            "Loss: 1.532 | Acc: 44.150% (2430/5504)\n",
            "Loss: 1.534 | Acc: 44.109% (2456/5568)\n",
            "Loss: 1.536 | Acc: 44.087% (2483/5632)\n",
            "Loss: 1.536 | Acc: 44.119% (2513/5696)\n",
            "Loss: 1.535 | Acc: 44.115% (2541/5760)\n",
            "Loss: 1.533 | Acc: 44.179% (2573/5824)\n",
            "Loss: 1.533 | Acc: 44.158% (2600/5888)\n",
            "Loss: 1.533 | Acc: 44.120% (2626/5952)\n",
            "Loss: 1.532 | Acc: 44.116% (2654/6016)\n",
            "Loss: 1.533 | Acc: 44.095% (2681/6080)\n",
            "Loss: 1.533 | Acc: 44.092% (2709/6144)\n",
            "Loss: 1.533 | Acc: 44.056% (2735/6208)\n",
            "Loss: 1.535 | Acc: 44.021% (2761/6272)\n",
            "Loss: 1.535 | Acc: 44.018% (2789/6336)\n",
            "Loss: 1.535 | Acc: 44.000% (2816/6400)\n",
            "Loss: 1.537 | Acc: 43.920% (2839/6464)\n",
            "Loss: 1.536 | Acc: 43.980% (2871/6528)\n",
            "Loss: 1.537 | Acc: 43.932% (2896/6592)\n",
            "Loss: 1.537 | Acc: 43.915% (2923/6656)\n",
            "Loss: 1.539 | Acc: 43.854% (2947/6720)\n",
            "Loss: 1.538 | Acc: 43.897% (2978/6784)\n",
            "Loss: 1.536 | Acc: 43.925% (3008/6848)\n",
            "Loss: 1.539 | Acc: 43.837% (3030/6912)\n",
            "Loss: 1.540 | Acc: 43.850% (3059/6976)\n",
            "Loss: 1.542 | Acc: 43.736% (3079/7040)\n",
            "Loss: 1.543 | Acc: 43.764% (3109/7104)\n",
            "Loss: 1.541 | Acc: 43.862% (3144/7168)\n",
            "Loss: 1.542 | Acc: 43.819% (3169/7232)\n",
            "Loss: 1.539 | Acc: 43.928% (3205/7296)\n",
            "Loss: 1.538 | Acc: 43.899% (3231/7360)\n",
            "Loss: 1.538 | Acc: 43.885% (3258/7424)\n",
            "Loss: 1.537 | Acc: 43.950% (3291/7488)\n",
            "Loss: 1.536 | Acc: 43.988% (3322/7552)\n",
            "Loss: 1.538 | Acc: 43.921% (3345/7616)\n",
            "Loss: 1.538 | Acc: 43.906% (3372/7680)\n",
            "Loss: 1.537 | Acc: 43.957% (3404/7744)\n",
            "Loss: 1.536 | Acc: 43.981% (3434/7808)\n",
            "Loss: 1.537 | Acc: 43.877% (3454/7872)\n",
            "Loss: 1.537 | Acc: 43.889% (3483/7936)\n",
            "Loss: 1.538 | Acc: 43.888% (3511/8000)\n",
            "Loss: 1.537 | Acc: 43.899% (3540/8064)\n",
            "Loss: 1.538 | Acc: 43.922% (3570/8128)\n",
            "Loss: 1.537 | Acc: 43.884% (3595/8192)\n",
            "Loss: 1.538 | Acc: 43.859% (3621/8256)\n",
            "Loss: 1.541 | Acc: 43.726% (3638/8320)\n",
            "Loss: 1.541 | Acc: 43.690% (3663/8384)\n",
            "Loss: 1.540 | Acc: 43.620% (3685/8448)\n",
            "Loss: 1.541 | Acc: 43.562% (3708/8512)\n",
            "Loss: 1.541 | Acc: 43.598% (3739/8576)\n",
            "Loss: 1.542 | Acc: 43.600% (3767/8640)\n",
            "Loss: 1.543 | Acc: 43.612% (3796/8704)\n",
            "Loss: 1.543 | Acc: 43.522% (3816/8768)\n",
            "Loss: 1.544 | Acc: 43.512% (3843/8832)\n",
            "Loss: 1.544 | Acc: 43.548% (3874/8896)\n",
            "Loss: 1.543 | Acc: 43.549% (3902/8960)\n",
            "Loss: 1.544 | Acc: 43.539% (3929/9024)\n",
            "Loss: 1.545 | Acc: 43.552% (3958/9088)\n",
            "Loss: 1.544 | Acc: 43.575% (3988/9152)\n",
            "Loss: 1.541 | Acc: 43.631% (4021/9216)\n",
            "Loss: 1.540 | Acc: 43.685% (4054/9280)\n",
            "Loss: 1.540 | Acc: 43.675% (4081/9344)\n",
            "Loss: 1.540 | Acc: 43.676% (4109/9408)\n",
            "Loss: 1.540 | Acc: 43.655% (4135/9472)\n",
            "Loss: 1.540 | Acc: 43.687% (4166/9536)\n",
            "Loss: 1.539 | Acc: 43.729% (4198/9600)\n",
            "Loss: 1.539 | Acc: 43.729% (4226/9664)\n",
            "Loss: 1.539 | Acc: 43.688% (4250/9728)\n",
            "Loss: 1.540 | Acc: 43.658% (4275/9792)\n",
            "Loss: 1.539 | Acc: 43.679% (4305/9856)\n",
            "Loss: 1.539 | Acc: 43.649% (4330/9920)\n",
            "Loss: 1.540 | Acc: 43.660% (4359/9984)\n",
            "Loss: 1.539 | Acc: 43.640% (4364/10000)\n",
            "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 43.64\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 1.385 | Acc: 51.562% (33/64)\n",
            "Loss: 1.405 | Acc: 50.781% (65/128)\n",
            "Loss: 1.481 | Acc: 47.396% (91/192)\n",
            "Loss: 1.488 | Acc: 44.141% (113/256)\n",
            "Loss: 1.522 | Acc: 42.812% (137/320)\n",
            "Loss: 1.507 | Acc: 42.448% (163/384)\n",
            "Loss: 1.524 | Acc: 41.964% (188/448)\n",
            "Loss: 1.563 | Acc: 41.406% (212/512)\n",
            "Loss: 1.547 | Acc: 42.188% (243/576)\n",
            "Loss: 1.551 | Acc: 42.188% (270/640)\n",
            "Loss: 1.543 | Acc: 42.330% (298/704)\n",
            "Loss: 1.547 | Acc: 42.318% (325/768)\n",
            "Loss: 1.544 | Acc: 41.947% (349/832)\n",
            "Loss: 1.552 | Acc: 41.518% (372/896)\n",
            "Loss: 1.537 | Acc: 42.292% (406/960)\n",
            "Loss: 1.542 | Acc: 42.090% (431/1024)\n",
            "Loss: 1.537 | Acc: 42.555% (463/1088)\n",
            "Loss: 1.536 | Acc: 42.448% (489/1152)\n",
            "Loss: 1.535 | Acc: 42.681% (519/1216)\n",
            "Loss: 1.537 | Acc: 42.422% (543/1280)\n",
            "Loss: 1.529 | Acc: 42.783% (575/1344)\n",
            "Loss: 1.534 | Acc: 42.685% (601/1408)\n",
            "Loss: 1.525 | Acc: 43.139% (635/1472)\n",
            "Loss: 1.524 | Acc: 43.099% (662/1536)\n",
            "Loss: 1.522 | Acc: 43.188% (691/1600)\n",
            "Loss: 1.515 | Acc: 43.510% (724/1664)\n",
            "Loss: 1.514 | Acc: 43.692% (755/1728)\n",
            "Loss: 1.517 | Acc: 43.638% (782/1792)\n",
            "Loss: 1.515 | Acc: 43.534% (808/1856)\n",
            "Loss: 1.527 | Acc: 43.125% (828/1920)\n",
            "Loss: 1.524 | Acc: 43.044% (854/1984)\n",
            "Loss: 1.521 | Acc: 43.066% (882/2048)\n",
            "Loss: 1.525 | Acc: 42.898% (906/2112)\n",
            "Loss: 1.524 | Acc: 42.877% (933/2176)\n",
            "Loss: 1.527 | Acc: 42.857% (960/2240)\n",
            "Loss: 1.521 | Acc: 42.925% (989/2304)\n",
            "Loss: 1.526 | Acc: 42.863% (1015/2368)\n",
            "Loss: 1.527 | Acc: 42.969% (1045/2432)\n",
            "Loss: 1.528 | Acc: 42.949% (1072/2496)\n",
            "Loss: 1.526 | Acc: 43.281% (1108/2560)\n",
            "Loss: 1.529 | Acc: 43.216% (1134/2624)\n",
            "Loss: 1.528 | Acc: 43.229% (1162/2688)\n",
            "Loss: 1.529 | Acc: 43.169% (1188/2752)\n",
            "Loss: 1.531 | Acc: 43.253% (1218/2816)\n",
            "Loss: 1.535 | Acc: 43.229% (1245/2880)\n",
            "Loss: 1.536 | Acc: 43.410% (1278/2944)\n",
            "Loss: 1.533 | Acc: 43.484% (1308/3008)\n",
            "Loss: 1.534 | Acc: 43.555% (1338/3072)\n",
            "Loss: 1.539 | Acc: 43.272% (1357/3136)\n",
            "Loss: 1.541 | Acc: 43.125% (1380/3200)\n",
            "Loss: 1.540 | Acc: 43.045% (1405/3264)\n",
            "Loss: 1.539 | Acc: 43.029% (1432/3328)\n",
            "Loss: 1.535 | Acc: 43.190% (1465/3392)\n",
            "Loss: 1.536 | Acc: 43.171% (1492/3456)\n",
            "Loss: 1.533 | Acc: 43.210% (1521/3520)\n",
            "Loss: 1.536 | Acc: 43.108% (1545/3584)\n",
            "Loss: 1.535 | Acc: 43.120% (1573/3648)\n",
            "Loss: 1.535 | Acc: 43.157% (1602/3712)\n",
            "Loss: 1.537 | Acc: 43.088% (1627/3776)\n",
            "Loss: 1.537 | Acc: 43.073% (1654/3840)\n",
            "Loss: 1.537 | Acc: 43.084% (1682/3904)\n",
            "Loss: 1.533 | Acc: 43.246% (1716/3968)\n",
            "Loss: 1.533 | Acc: 43.328% (1747/4032)\n",
            "Loss: 1.535 | Acc: 43.213% (1770/4096)\n",
            "Loss: 1.532 | Acc: 43.341% (1803/4160)\n",
            "Loss: 1.530 | Acc: 43.395% (1833/4224)\n",
            "Loss: 1.530 | Acc: 43.400% (1861/4288)\n",
            "Loss: 1.529 | Acc: 43.474% (1892/4352)\n",
            "Loss: 1.528 | Acc: 43.524% (1922/4416)\n",
            "Loss: 1.528 | Acc: 43.571% (1952/4480)\n",
            "Loss: 1.525 | Acc: 43.596% (1981/4544)\n",
            "Loss: 1.524 | Acc: 43.641% (2011/4608)\n",
            "Loss: 1.526 | Acc: 43.600% (2037/4672)\n",
            "Loss: 1.524 | Acc: 43.708% (2070/4736)\n",
            "Loss: 1.523 | Acc: 43.812% (2103/4800)\n",
            "Loss: 1.521 | Acc: 43.956% (2138/4864)\n",
            "Loss: 1.521 | Acc: 43.973% (2167/4928)\n",
            "Loss: 1.523 | Acc: 43.870% (2190/4992)\n",
            "Loss: 1.526 | Acc: 43.829% (2216/5056)\n",
            "Loss: 1.525 | Acc: 43.867% (2246/5120)\n",
            "Loss: 1.523 | Acc: 43.943% (2278/5184)\n",
            "Loss: 1.523 | Acc: 44.017% (2310/5248)\n",
            "Loss: 1.523 | Acc: 43.976% (2336/5312)\n",
            "Loss: 1.526 | Acc: 43.992% (2365/5376)\n",
            "Loss: 1.525 | Acc: 43.952% (2391/5440)\n",
            "Loss: 1.525 | Acc: 43.841% (2413/5504)\n",
            "Loss: 1.528 | Acc: 43.750% (2436/5568)\n",
            "Loss: 1.528 | Acc: 43.803% (2467/5632)\n",
            "Loss: 1.528 | Acc: 43.750% (2492/5696)\n",
            "Loss: 1.528 | Acc: 43.802% (2523/5760)\n",
            "Loss: 1.527 | Acc: 43.767% (2549/5824)\n",
            "Loss: 1.531 | Acc: 43.631% (2569/5888)\n",
            "Loss: 1.534 | Acc: 43.464% (2587/5952)\n",
            "Loss: 1.534 | Acc: 43.434% (2613/6016)\n",
            "Loss: 1.534 | Acc: 43.438% (2641/6080)\n",
            "Loss: 1.537 | Acc: 43.343% (2663/6144)\n",
            "Loss: 1.538 | Acc: 43.380% (2693/6208)\n",
            "Loss: 1.538 | Acc: 43.431% (2724/6272)\n",
            "Loss: 1.540 | Acc: 43.324% (2745/6336)\n",
            "Loss: 1.539 | Acc: 43.391% (2777/6400)\n",
            "Loss: 1.537 | Acc: 43.410% (2806/6464)\n",
            "Loss: 1.537 | Acc: 43.520% (2841/6528)\n",
            "Loss: 1.538 | Acc: 43.538% (2870/6592)\n",
            "Loss: 1.535 | Acc: 43.600% (2902/6656)\n",
            "Loss: 1.535 | Acc: 43.542% (2926/6720)\n",
            "Loss: 1.535 | Acc: 43.573% (2956/6784)\n",
            "Loss: 1.532 | Acc: 43.677% (2991/6848)\n",
            "Loss: 1.533 | Acc: 43.663% (3018/6912)\n",
            "Loss: 1.535 | Acc: 43.592% (3041/6976)\n",
            "Loss: 1.532 | Acc: 43.636% (3072/7040)\n",
            "Loss: 1.529 | Acc: 43.764% (3109/7104)\n",
            "Loss: 1.530 | Acc: 43.736% (3135/7168)\n",
            "Loss: 1.531 | Acc: 43.653% (3157/7232)\n",
            "Loss: 1.529 | Acc: 43.791% (3195/7296)\n",
            "Loss: 1.530 | Acc: 43.777% (3222/7360)\n",
            "Loss: 1.530 | Acc: 43.790% (3251/7424)\n",
            "Loss: 1.528 | Acc: 43.870% (3285/7488)\n",
            "Loss: 1.528 | Acc: 43.869% (3313/7552)\n",
            "Loss: 1.528 | Acc: 43.855% (3340/7616)\n",
            "Loss: 1.527 | Acc: 43.906% (3372/7680)\n",
            "Loss: 1.526 | Acc: 43.982% (3406/7744)\n",
            "Loss: 1.527 | Acc: 43.865% (3425/7808)\n",
            "Loss: 1.528 | Acc: 43.814% (3449/7872)\n",
            "Loss: 1.527 | Acc: 43.813% (3477/7936)\n",
            "Loss: 1.527 | Acc: 43.875% (3510/8000)\n",
            "Loss: 1.527 | Acc: 43.862% (3537/8064)\n",
            "Loss: 1.528 | Acc: 43.861% (3565/8128)\n",
            "Loss: 1.530 | Acc: 43.823% (3590/8192)\n",
            "Loss: 1.528 | Acc: 43.920% (3626/8256)\n",
            "Loss: 1.528 | Acc: 43.882% (3651/8320)\n",
            "Loss: 1.527 | Acc: 43.953% (3685/8384)\n",
            "Loss: 1.526 | Acc: 43.975% (3715/8448)\n",
            "Loss: 1.527 | Acc: 43.973% (3743/8512)\n",
            "Loss: 1.526 | Acc: 43.948% (3769/8576)\n",
            "Loss: 1.526 | Acc: 43.877% (3791/8640)\n",
            "Loss: 1.527 | Acc: 43.876% (3819/8704)\n",
            "Loss: 1.529 | Acc: 43.807% (3841/8768)\n",
            "Loss: 1.529 | Acc: 43.807% (3869/8832)\n",
            "Loss: 1.528 | Acc: 43.806% (3897/8896)\n",
            "Loss: 1.529 | Acc: 43.839% (3928/8960)\n",
            "Loss: 1.528 | Acc: 43.872% (3959/9024)\n",
            "Loss: 1.530 | Acc: 43.772% (3978/9088)\n",
            "Loss: 1.532 | Acc: 43.695% (3999/9152)\n",
            "Loss: 1.531 | Acc: 43.696% (4027/9216)\n",
            "Loss: 1.531 | Acc: 43.642% (4050/9280)\n",
            "Loss: 1.531 | Acc: 43.600% (4074/9344)\n",
            "Loss: 1.529 | Acc: 43.665% (4108/9408)\n",
            "Loss: 1.529 | Acc: 43.676% (4137/9472)\n",
            "Loss: 1.530 | Acc: 43.635% (4161/9536)\n",
            "Loss: 1.528 | Acc: 43.667% (4192/9600)\n",
            "Loss: 1.529 | Acc: 43.626% (4216/9664)\n",
            "Loss: 1.529 | Acc: 43.678% (4249/9728)\n",
            "Loss: 1.529 | Acc: 43.668% (4276/9792)\n",
            "Loss: 1.529 | Acc: 43.679% (4305/9856)\n",
            "Loss: 1.529 | Acc: 43.659% (4331/9920)\n",
            "Loss: 1.529 | Acc: 43.730% (4366/9984)\n",
            "Loss: 1.529 | Acc: 43.730% (4394/10048)\n",
            "Loss: 1.528 | Acc: 43.730% (4422/10112)\n",
            "Loss: 1.528 | Acc: 43.721% (4449/10176)\n",
            "Loss: 1.527 | Acc: 43.730% (4478/10240)\n",
            "Loss: 1.526 | Acc: 43.779% (4511/10304)\n",
            "Loss: 1.525 | Acc: 43.837% (4545/10368)\n",
            "Loss: 1.525 | Acc: 43.846% (4574/10432)\n",
            "Loss: 1.523 | Acc: 43.883% (4606/10496)\n",
            "Loss: 1.523 | Acc: 43.845% (4630/10560)\n",
            "Loss: 1.525 | Acc: 43.769% (4650/10624)\n",
            "Loss: 1.523 | Acc: 43.815% (4683/10688)\n",
            "Loss: 1.522 | Acc: 43.880% (4718/10752)\n",
            "Loss: 1.521 | Acc: 43.926% (4751/10816)\n",
            "Loss: 1.521 | Acc: 43.961% (4783/10880)\n",
            "Loss: 1.521 | Acc: 43.933% (4808/10944)\n",
            "Loss: 1.524 | Acc: 43.859% (4828/11008)\n",
            "Loss: 1.522 | Acc: 43.885% (4859/11072)\n",
            "Loss: 1.523 | Acc: 43.876% (4886/11136)\n",
            "Loss: 1.522 | Acc: 43.875% (4914/11200)\n",
            "Loss: 1.522 | Acc: 43.857% (4940/11264)\n",
            "Loss: 1.522 | Acc: 43.847% (4967/11328)\n",
            "Loss: 1.523 | Acc: 43.847% (4995/11392)\n",
            "Loss: 1.523 | Acc: 43.820% (5020/11456)\n",
            "Loss: 1.524 | Acc: 43.767% (5042/11520)\n",
            "Loss: 1.524 | Acc: 43.750% (5068/11584)\n",
            "Loss: 1.524 | Acc: 43.793% (5101/11648)\n",
            "Loss: 1.524 | Acc: 43.827% (5133/11712)\n",
            "Loss: 1.525 | Acc: 43.767% (5154/11776)\n",
            "Loss: 1.525 | Acc: 43.792% (5185/11840)\n",
            "Loss: 1.524 | Acc: 43.859% (5221/11904)\n",
            "Loss: 1.524 | Acc: 43.825% (5245/11968)\n",
            "Loss: 1.524 | Acc: 43.858% (5277/12032)\n",
            "Loss: 1.524 | Acc: 43.857% (5305/12096)\n",
            "Loss: 1.524 | Acc: 43.849% (5332/12160)\n",
            "Loss: 1.524 | Acc: 43.856% (5361/12224)\n",
            "Loss: 1.525 | Acc: 43.831% (5386/12288)\n",
            "Loss: 1.525 | Acc: 43.790% (5409/12352)\n",
            "Loss: 1.525 | Acc: 43.750% (5432/12416)\n",
            "Loss: 1.526 | Acc: 43.710% (5455/12480)\n",
            "Loss: 1.526 | Acc: 43.734% (5486/12544)\n",
            "Loss: 1.526 | Acc: 43.766% (5518/12608)\n",
            "Loss: 1.525 | Acc: 43.805% (5551/12672)\n",
            "Loss: 1.525 | Acc: 43.789% (5577/12736)\n",
            "Loss: 1.526 | Acc: 43.734% (5598/12800)\n",
            "Loss: 1.526 | Acc: 43.758% (5629/12864)\n",
            "Loss: 1.526 | Acc: 43.773% (5659/12928)\n",
            "Loss: 1.526 | Acc: 43.858% (5698/12992)\n",
            "Loss: 1.525 | Acc: 43.880% (5729/13056)\n",
            "Loss: 1.526 | Acc: 43.849% (5753/13120)\n",
            "Loss: 1.525 | Acc: 43.833% (5779/13184)\n",
            "Loss: 1.525 | Acc: 43.871% (5812/13248)\n",
            "Loss: 1.526 | Acc: 43.848% (5837/13312)\n",
            "Loss: 1.526 | Acc: 43.817% (5861/13376)\n",
            "Loss: 1.525 | Acc: 43.847% (5893/13440)\n",
            "Loss: 1.526 | Acc: 43.846% (5921/13504)\n",
            "Loss: 1.526 | Acc: 43.861% (5951/13568)\n",
            "Loss: 1.525 | Acc: 43.860% (5979/13632)\n",
            "Loss: 1.525 | Acc: 43.867% (6008/13696)\n",
            "Loss: 1.525 | Acc: 43.895% (6040/13760)\n",
            "Loss: 1.525 | Acc: 43.924% (6072/13824)\n",
            "Loss: 1.524 | Acc: 43.923% (6100/13888)\n",
            "Loss: 1.523 | Acc: 43.936% (6130/13952)\n",
            "Loss: 1.525 | Acc: 43.950% (6160/14016)\n",
            "Loss: 1.524 | Acc: 43.963% (6190/14080)\n",
            "Loss: 1.524 | Acc: 43.976% (6220/14144)\n",
            "Loss: 1.524 | Acc: 44.010% (6253/14208)\n",
            "Loss: 1.524 | Acc: 44.009% (6281/14272)\n",
            "Loss: 1.523 | Acc: 44.001% (6308/14336)\n",
            "Loss: 1.524 | Acc: 44.014% (6338/14400)\n",
            "Loss: 1.523 | Acc: 44.040% (6370/14464)\n",
            "Loss: 1.523 | Acc: 44.025% (6396/14528)\n",
            "Loss: 1.523 | Acc: 44.017% (6423/14592)\n",
            "Loss: 1.522 | Acc: 44.050% (6456/14656)\n",
            "Loss: 1.522 | Acc: 44.076% (6488/14720)\n",
            "Loss: 1.522 | Acc: 44.041% (6511/14784)\n",
            "Loss: 1.521 | Acc: 44.087% (6546/14848)\n",
            "Loss: 1.520 | Acc: 44.146% (6583/14912)\n",
            "Loss: 1.520 | Acc: 44.124% (6608/14976)\n",
            "Loss: 1.520 | Acc: 44.122% (6636/15040)\n",
            "Loss: 1.520 | Acc: 44.147% (6668/15104)\n",
            "Loss: 1.520 | Acc: 44.192% (6703/15168)\n",
            "Loss: 1.520 | Acc: 44.196% (6732/15232)\n",
            "Loss: 1.521 | Acc: 44.175% (6757/15296)\n",
            "Loss: 1.521 | Acc: 44.173% (6785/15360)\n",
            "Loss: 1.521 | Acc: 44.171% (6813/15424)\n",
            "Loss: 1.521 | Acc: 44.189% (6844/15488)\n",
            "Loss: 1.520 | Acc: 44.252% (6882/15552)\n",
            "Loss: 1.519 | Acc: 44.256% (6911/15616)\n",
            "Loss: 1.518 | Acc: 44.279% (6943/15680)\n",
            "Loss: 1.518 | Acc: 44.284% (6972/15744)\n",
            "Loss: 1.518 | Acc: 44.313% (7005/15808)\n",
            "Loss: 1.517 | Acc: 44.323% (7035/15872)\n",
            "Loss: 1.518 | Acc: 44.321% (7063/15936)\n",
            "Loss: 1.517 | Acc: 44.325% (7092/16000)\n",
            "Loss: 1.517 | Acc: 44.354% (7125/16064)\n",
            "Loss: 1.518 | Acc: 44.320% (7148/16128)\n",
            "Loss: 1.518 | Acc: 44.324% (7177/16192)\n",
            "Loss: 1.518 | Acc: 44.310% (7203/16256)\n",
            "Loss: 1.518 | Acc: 44.289% (7228/16320)\n",
            "Loss: 1.518 | Acc: 44.293% (7257/16384)\n",
            "Loss: 1.518 | Acc: 44.279% (7283/16448)\n",
            "Loss: 1.518 | Acc: 44.301% (7315/16512)\n",
            "Loss: 1.518 | Acc: 44.317% (7346/16576)\n",
            "Loss: 1.517 | Acc: 44.351% (7380/16640)\n",
            "Loss: 1.518 | Acc: 44.295% (7399/16704)\n",
            "Loss: 1.518 | Acc: 44.287% (7426/16768)\n",
            "Loss: 1.519 | Acc: 44.285% (7454/16832)\n",
            "Loss: 1.520 | Acc: 44.247% (7476/16896)\n",
            "Loss: 1.520 | Acc: 44.233% (7502/16960)\n",
            "Loss: 1.521 | Acc: 44.220% (7528/17024)\n",
            "Loss: 1.522 | Acc: 44.212% (7555/17088)\n",
            "Loss: 1.522 | Acc: 44.228% (7586/17152)\n",
            "Loss: 1.522 | Acc: 44.226% (7614/17216)\n",
            "Loss: 1.522 | Acc: 44.207% (7639/17280)\n",
            "Loss: 1.523 | Acc: 44.171% (7661/17344)\n",
            "Loss: 1.521 | Acc: 44.250% (7703/17408)\n",
            "Loss: 1.521 | Acc: 44.265% (7734/17472)\n",
            "Loss: 1.521 | Acc: 44.235% (7757/17536)\n",
            "Loss: 1.521 | Acc: 44.256% (7789/17600)\n",
            "Loss: 1.521 | Acc: 44.265% (7819/17664)\n",
            "Loss: 1.520 | Acc: 44.280% (7850/17728)\n",
            "Loss: 1.521 | Acc: 44.239% (7871/17792)\n",
            "Loss: 1.521 | Acc: 44.248% (7901/17856)\n",
            "Loss: 1.521 | Acc: 44.241% (7928/17920)\n",
            "Loss: 1.520 | Acc: 44.239% (7956/17984)\n",
            "Loss: 1.521 | Acc: 44.199% (7977/18048)\n",
            "Loss: 1.521 | Acc: 44.214% (8008/18112)\n",
            "Loss: 1.522 | Acc: 44.223% (8038/18176)\n",
            "Loss: 1.522 | Acc: 44.194% (8061/18240)\n",
            "Loss: 1.523 | Acc: 44.149% (8081/18304)\n",
            "Loss: 1.523 | Acc: 44.175% (8114/18368)\n",
            "Loss: 1.523 | Acc: 44.195% (8146/18432)\n",
            "Loss: 1.523 | Acc: 44.183% (8172/18496)\n",
            "Loss: 1.523 | Acc: 44.197% (8203/18560)\n",
            "Loss: 1.523 | Acc: 44.196% (8231/18624)\n",
            "Loss: 1.523 | Acc: 44.189% (8258/18688)\n",
            "Loss: 1.523 | Acc: 44.171% (8283/18752)\n",
            "Loss: 1.522 | Acc: 44.175% (8312/18816)\n",
            "Loss: 1.522 | Acc: 44.184% (8342/18880)\n",
            "Loss: 1.523 | Acc: 44.156% (8365/18944)\n",
            "Loss: 1.523 | Acc: 44.139% (8390/19008)\n",
            "Loss: 1.524 | Acc: 44.091% (8409/19072)\n",
            "Loss: 1.524 | Acc: 44.100% (8439/19136)\n",
            "Loss: 1.524 | Acc: 44.073% (8462/19200)\n",
            "Loss: 1.524 | Acc: 44.077% (8491/19264)\n",
            "Loss: 1.525 | Acc: 44.060% (8516/19328)\n",
            "Loss: 1.526 | Acc: 44.018% (8536/19392)\n",
            "Loss: 1.525 | Acc: 44.028% (8566/19456)\n",
            "Loss: 1.525 | Acc: 44.016% (8592/19520)\n",
            "Loss: 1.524 | Acc: 44.051% (8627/19584)\n",
            "Loss: 1.525 | Acc: 44.040% (8653/19648)\n",
            "Loss: 1.525 | Acc: 44.039% (8681/19712)\n",
            "Loss: 1.525 | Acc: 44.074% (8716/19776)\n",
            "Loss: 1.525 | Acc: 44.083% (8746/19840)\n",
            "Loss: 1.525 | Acc: 44.087% (8775/19904)\n",
            "Loss: 1.524 | Acc: 44.060% (8798/19968)\n",
            "Loss: 1.524 | Acc: 44.060% (8826/20032)\n",
            "Loss: 1.524 | Acc: 44.029% (8848/20096)\n",
            "Loss: 1.524 | Acc: 44.013% (8873/20160)\n",
            "Loss: 1.524 | Acc: 44.027% (8904/20224)\n",
            "Loss: 1.523 | Acc: 44.070% (8941/20288)\n",
            "Loss: 1.523 | Acc: 44.055% (8966/20352)\n",
            "Loss: 1.523 | Acc: 44.083% (9000/20416)\n",
            "Loss: 1.523 | Acc: 44.077% (9027/20480)\n",
            "Loss: 1.522 | Acc: 44.066% (9053/20544)\n",
            "Loss: 1.522 | Acc: 44.090% (9086/20608)\n",
            "Loss: 1.522 | Acc: 44.084% (9113/20672)\n",
            "Loss: 1.522 | Acc: 44.092% (9143/20736)\n",
            "Loss: 1.523 | Acc: 44.106% (9174/20800)\n",
            "Loss: 1.522 | Acc: 44.124% (9206/20864)\n",
            "Loss: 1.522 | Acc: 44.118% (9233/20928)\n",
            "Loss: 1.523 | Acc: 44.098% (9257/20992)\n",
            "Loss: 1.522 | Acc: 44.144% (9295/21056)\n",
            "Loss: 1.522 | Acc: 44.138% (9322/21120)\n",
            "Loss: 1.522 | Acc: 44.151% (9353/21184)\n",
            "Loss: 1.522 | Acc: 44.159% (9383/21248)\n",
            "Loss: 1.522 | Acc: 44.154% (9410/21312)\n",
            "Loss: 1.521 | Acc: 44.157% (9439/21376)\n",
            "Loss: 1.522 | Acc: 44.137% (9463/21440)\n",
            "Loss: 1.522 | Acc: 44.136% (9491/21504)\n",
            "Loss: 1.521 | Acc: 44.153% (9523/21568)\n",
            "Loss: 1.522 | Acc: 44.120% (9544/21632)\n",
            "Loss: 1.522 | Acc: 44.128% (9574/21696)\n",
            "Loss: 1.522 | Acc: 44.136% (9604/21760)\n",
            "Loss: 1.522 | Acc: 44.098% (9624/21824)\n",
            "Loss: 1.522 | Acc: 44.097% (9652/21888)\n",
            "Loss: 1.522 | Acc: 44.083% (9677/21952)\n",
            "Loss: 1.522 | Acc: 44.059% (9700/22016)\n",
            "Loss: 1.522 | Acc: 44.094% (9736/22080)\n",
            "Loss: 1.523 | Acc: 44.053% (9755/22144)\n",
            "Loss: 1.523 | Acc: 44.047% (9782/22208)\n",
            "Loss: 1.524 | Acc: 44.015% (9803/22272)\n",
            "Loss: 1.524 | Acc: 43.987% (9825/22336)\n",
            "Loss: 1.525 | Acc: 43.946% (9844/22400)\n",
            "Loss: 1.526 | Acc: 43.941% (9871/22464)\n",
            "Loss: 1.526 | Acc: 43.941% (9899/22528)\n",
            "Loss: 1.526 | Acc: 43.945% (9928/22592)\n",
            "Loss: 1.525 | Acc: 43.953% (9958/22656)\n",
            "Loss: 1.525 | Acc: 43.966% (9989/22720)\n",
            "Loss: 1.525 | Acc: 43.965% (10017/22784)\n",
            "Loss: 1.525 | Acc: 43.973% (10047/22848)\n",
            "Loss: 1.525 | Acc: 43.994% (10080/22912)\n",
            "Loss: 1.525 | Acc: 44.011% (10112/22976)\n",
            "Loss: 1.525 | Acc: 44.041% (10147/23040)\n",
            "Loss: 1.525 | Acc: 44.040% (10175/23104)\n",
            "Loss: 1.525 | Acc: 44.044% (10204/23168)\n",
            "Loss: 1.526 | Acc: 44.004% (10223/23232)\n",
            "Loss: 1.526 | Acc: 44.016% (10254/23296)\n",
            "Loss: 1.525 | Acc: 44.054% (10291/23360)\n",
            "Loss: 1.525 | Acc: 44.040% (10316/23424)\n",
            "Loss: 1.524 | Acc: 44.078% (10353/23488)\n",
            "Loss: 1.524 | Acc: 44.085% (10383/23552)\n",
            "Loss: 1.524 | Acc: 44.072% (10408/23616)\n",
            "Loss: 1.524 | Acc: 44.079% (10438/23680)\n",
            "Loss: 1.524 | Acc: 44.083% (10467/23744)\n",
            "Loss: 1.524 | Acc: 44.069% (10492/23808)\n",
            "Loss: 1.523 | Acc: 44.085% (10524/23872)\n",
            "Loss: 1.524 | Acc: 44.080% (10551/23936)\n",
            "Loss: 1.524 | Acc: 44.092% (10582/24000)\n",
            "Loss: 1.524 | Acc: 44.082% (10608/24064)\n",
            "Loss: 1.523 | Acc: 44.106% (10642/24128)\n",
            "Loss: 1.523 | Acc: 44.122% (10674/24192)\n",
            "Loss: 1.522 | Acc: 44.158% (10711/24256)\n",
            "Loss: 1.522 | Acc: 44.149% (10737/24320)\n",
            "Loss: 1.522 | Acc: 44.152% (10766/24384)\n",
            "Loss: 1.522 | Acc: 44.163% (10797/24448)\n",
            "Loss: 1.522 | Acc: 44.178% (10829/24512)\n",
            "Loss: 1.521 | Acc: 44.206% (10864/24576)\n",
            "Loss: 1.521 | Acc: 44.184% (10887/24640)\n",
            "Loss: 1.520 | Acc: 44.232% (10927/24704)\n",
            "Loss: 1.520 | Acc: 44.218% (10952/24768)\n",
            "Loss: 1.521 | Acc: 44.201% (10976/24832)\n",
            "Loss: 1.521 | Acc: 44.192% (11002/24896)\n",
            "Loss: 1.521 | Acc: 44.175% (11026/24960)\n",
            "Loss: 1.520 | Acc: 44.178% (11055/25024)\n",
            "Loss: 1.520 | Acc: 44.176% (11083/25088)\n",
            "Loss: 1.519 | Acc: 44.195% (11116/25152)\n",
            "Loss: 1.520 | Acc: 44.186% (11142/25216)\n",
            "Loss: 1.520 | Acc: 44.193% (11172/25280)\n",
            "Loss: 1.520 | Acc: 44.200% (11202/25344)\n",
            "Loss: 1.520 | Acc: 44.195% (11229/25408)\n",
            "Loss: 1.519 | Acc: 44.198% (11258/25472)\n",
            "Loss: 1.520 | Acc: 44.193% (11285/25536)\n",
            "Loss: 1.519 | Acc: 44.199% (11315/25600)\n",
            "Loss: 1.520 | Acc: 44.194% (11342/25664)\n",
            "Loss: 1.519 | Acc: 44.220% (11377/25728)\n",
            "Loss: 1.519 | Acc: 44.235% (11409/25792)\n",
            "Loss: 1.518 | Acc: 44.253% (11442/25856)\n",
            "Loss: 1.518 | Acc: 44.248% (11469/25920)\n",
            "Loss: 1.518 | Acc: 44.273% (11504/25984)\n",
            "Loss: 1.517 | Acc: 44.314% (11543/26048)\n",
            "Loss: 1.517 | Acc: 44.294% (11566/26112)\n",
            "Loss: 1.517 | Acc: 44.304% (11597/26176)\n",
            "Loss: 1.517 | Acc: 44.314% (11628/26240)\n",
            "Loss: 1.516 | Acc: 44.332% (11661/26304)\n",
            "Loss: 1.516 | Acc: 44.326% (11688/26368)\n",
            "Loss: 1.517 | Acc: 44.314% (11713/26432)\n",
            "Loss: 1.517 | Acc: 44.327% (11745/26496)\n",
            "Loss: 1.517 | Acc: 44.337% (11776/26560)\n",
            "Loss: 1.517 | Acc: 44.321% (11800/26624)\n",
            "Loss: 1.517 | Acc: 44.320% (11828/26688)\n",
            "Loss: 1.517 | Acc: 44.341% (11862/26752)\n",
            "Loss: 1.517 | Acc: 44.362% (11896/26816)\n",
            "Loss: 1.517 | Acc: 44.382% (11930/26880)\n",
            "Loss: 1.516 | Acc: 44.411% (11966/26944)\n",
            "Loss: 1.516 | Acc: 44.402% (11992/27008)\n",
            "Loss: 1.517 | Acc: 44.404% (12021/27072)\n",
            "Loss: 1.517 | Acc: 44.402% (12049/27136)\n",
            "Loss: 1.517 | Acc: 44.397% (12076/27200)\n",
            "Loss: 1.517 | Acc: 44.399% (12105/27264)\n",
            "Loss: 1.516 | Acc: 44.401% (12134/27328)\n",
            "Loss: 1.516 | Acc: 44.400% (12162/27392)\n",
            "Loss: 1.516 | Acc: 44.395% (12189/27456)\n",
            "Loss: 1.516 | Acc: 44.411% (12222/27520)\n",
            "Loss: 1.516 | Acc: 44.424% (12254/27584)\n",
            "Loss: 1.516 | Acc: 44.423% (12282/27648)\n",
            "Loss: 1.516 | Acc: 44.446% (12317/27712)\n",
            "Loss: 1.516 | Acc: 44.456% (12348/27776)\n",
            "Loss: 1.516 | Acc: 44.443% (12373/27840)\n",
            "Loss: 1.516 | Acc: 44.460% (12406/27904)\n",
            "Loss: 1.516 | Acc: 44.462% (12435/27968)\n",
            "Loss: 1.516 | Acc: 44.449% (12460/28032)\n",
            "Loss: 1.516 | Acc: 44.465% (12493/28096)\n",
            "Loss: 1.516 | Acc: 44.460% (12520/28160)\n",
            "Loss: 1.516 | Acc: 44.483% (12555/28224)\n",
            "Loss: 1.516 | Acc: 44.482% (12583/28288)\n",
            "Loss: 1.516 | Acc: 44.487% (12613/28352)\n",
            "Loss: 1.516 | Acc: 44.468% (12636/28416)\n",
            "Loss: 1.516 | Acc: 44.484% (12669/28480)\n",
            "Loss: 1.516 | Acc: 44.496% (12701/28544)\n",
            "Loss: 1.515 | Acc: 44.509% (12733/28608)\n",
            "Loss: 1.515 | Acc: 44.503% (12760/28672)\n",
            "Loss: 1.515 | Acc: 44.495% (12786/28736)\n",
            "Loss: 1.515 | Acc: 44.510% (12819/28800)\n",
            "Loss: 1.515 | Acc: 44.516% (12849/28864)\n",
            "Loss: 1.515 | Acc: 44.528% (12881/28928)\n",
            "Loss: 1.515 | Acc: 44.530% (12910/28992)\n",
            "Loss: 1.515 | Acc: 44.531% (12939/29056)\n",
            "Loss: 1.515 | Acc: 44.523% (12965/29120)\n",
            "Loss: 1.514 | Acc: 44.531% (12996/29184)\n",
            "Loss: 1.515 | Acc: 44.506% (13017/29248)\n",
            "Loss: 1.515 | Acc: 44.514% (13048/29312)\n",
            "Loss: 1.514 | Acc: 44.513% (13076/29376)\n",
            "Loss: 1.514 | Acc: 44.531% (13110/29440)\n",
            "Loss: 1.514 | Acc: 44.536% (13140/29504)\n",
            "Loss: 1.514 | Acc: 44.538% (13169/29568)\n",
            "Loss: 1.513 | Acc: 44.563% (13205/29632)\n",
            "Loss: 1.513 | Acc: 44.558% (13232/29696)\n",
            "Loss: 1.513 | Acc: 44.560% (13261/29760)\n",
            "Loss: 1.513 | Acc: 44.561% (13290/29824)\n",
            "Loss: 1.513 | Acc: 44.566% (13320/29888)\n",
            "Loss: 1.513 | Acc: 44.561% (13347/29952)\n",
            "Loss: 1.514 | Acc: 44.536% (13368/30016)\n",
            "Loss: 1.514 | Acc: 44.521% (13392/30080)\n",
            "Loss: 1.514 | Acc: 44.506% (13416/30144)\n",
            "Loss: 1.514 | Acc: 44.511% (13446/30208)\n",
            "Loss: 1.515 | Acc: 44.493% (13469/30272)\n",
            "Loss: 1.514 | Acc: 44.495% (13498/30336)\n",
            "Loss: 1.514 | Acc: 44.503% (13529/30400)\n",
            "Loss: 1.514 | Acc: 44.531% (13566/30464)\n",
            "Loss: 1.514 | Acc: 44.539% (13597/30528)\n",
            "Loss: 1.514 | Acc: 44.531% (13623/30592)\n",
            "Loss: 1.514 | Acc: 44.517% (13647/30656)\n",
            "Loss: 1.514 | Acc: 44.515% (13675/30720)\n",
            "Loss: 1.514 | Acc: 44.517% (13704/30784)\n",
            "Loss: 1.514 | Acc: 44.538% (13739/30848)\n",
            "Loss: 1.514 | Acc: 44.530% (13765/30912)\n",
            "Loss: 1.514 | Acc: 44.531% (13794/30976)\n",
            "Loss: 1.513 | Acc: 44.536% (13824/31040)\n",
            "Loss: 1.513 | Acc: 44.551% (13857/31104)\n",
            "Loss: 1.513 | Acc: 44.542% (13883/31168)\n",
            "Loss: 1.513 | Acc: 44.541% (13911/31232)\n",
            "Loss: 1.513 | Acc: 44.533% (13937/31296)\n",
            "Loss: 1.513 | Acc: 44.534% (13966/31360)\n",
            "Loss: 1.514 | Acc: 44.520% (13990/31424)\n",
            "Loss: 1.514 | Acc: 44.534% (14023/31488)\n",
            "Loss: 1.513 | Acc: 44.549% (14056/31552)\n",
            "Loss: 1.513 | Acc: 44.569% (14091/31616)\n",
            "Loss: 1.512 | Acc: 44.602% (14130/31680)\n",
            "Loss: 1.512 | Acc: 44.604% (14159/31744)\n",
            "Loss: 1.512 | Acc: 44.599% (14186/31808)\n",
            "Loss: 1.512 | Acc: 44.591% (14212/31872)\n",
            "Loss: 1.512 | Acc: 44.586% (14239/31936)\n",
            "Loss: 1.512 | Acc: 44.587% (14268/32000)\n",
            "Loss: 1.513 | Acc: 44.573% (14292/32064)\n",
            "Loss: 1.513 | Acc: 44.565% (14318/32128)\n",
            "Loss: 1.513 | Acc: 44.567% (14347/32192)\n",
            "Loss: 1.513 | Acc: 44.547% (14369/32256)\n",
            "Loss: 1.513 | Acc: 44.551% (14399/32320)\n",
            "Loss: 1.512 | Acc: 44.578% (14436/32384)\n",
            "Loss: 1.512 | Acc: 44.579% (14465/32448)\n",
            "Loss: 1.513 | Acc: 44.584% (14495/32512)\n",
            "Loss: 1.513 | Acc: 44.591% (14526/32576)\n",
            "Loss: 1.513 | Acc: 44.571% (14548/32640)\n",
            "Loss: 1.514 | Acc: 44.569% (14576/32704)\n",
            "Loss: 1.514 | Acc: 44.565% (14603/32768)\n",
            "Loss: 1.514 | Acc: 44.557% (14629/32832)\n",
            "Loss: 1.514 | Acc: 44.559% (14658/32896)\n",
            "Loss: 1.513 | Acc: 44.575% (14692/32960)\n",
            "Loss: 1.514 | Acc: 44.568% (14718/33024)\n",
            "Loss: 1.514 | Acc: 44.578% (14750/33088)\n",
            "Loss: 1.514 | Acc: 44.589% (14782/33152)\n",
            "Loss: 1.514 | Acc: 44.584% (14809/33216)\n",
            "Loss: 1.513 | Acc: 44.606% (14845/33280)\n",
            "Loss: 1.514 | Acc: 44.581% (14865/33344)\n",
            "Loss: 1.513 | Acc: 44.600% (14900/33408)\n",
            "Loss: 1.513 | Acc: 44.607% (14931/33472)\n",
            "Loss: 1.513 | Acc: 44.615% (14962/33536)\n",
            "Loss: 1.513 | Acc: 44.619% (14992/33600)\n",
            "Loss: 1.513 | Acc: 44.603% (15015/33664)\n",
            "Loss: 1.513 | Acc: 44.601% (15043/33728)\n",
            "Loss: 1.512 | Acc: 44.626% (15080/33792)\n",
            "Loss: 1.512 | Acc: 44.627% (15109/33856)\n",
            "Loss: 1.513 | Acc: 44.620% (15135/33920)\n",
            "Loss: 1.513 | Acc: 44.595% (15155/33984)\n",
            "Loss: 1.513 | Acc: 44.602% (15186/34048)\n",
            "Loss: 1.512 | Acc: 44.606% (15216/34112)\n",
            "Loss: 1.512 | Acc: 44.610% (15246/34176)\n",
            "Loss: 1.512 | Acc: 44.632% (15282/34240)\n",
            "Loss: 1.511 | Acc: 44.627% (15309/34304)\n",
            "Loss: 1.512 | Acc: 44.605% (15330/34368)\n",
            "Loss: 1.511 | Acc: 44.607% (15359/34432)\n",
            "Loss: 1.511 | Acc: 44.611% (15389/34496)\n",
            "Loss: 1.511 | Acc: 44.621% (15421/34560)\n",
            "Loss: 1.511 | Acc: 44.628% (15452/34624)\n",
            "Loss: 1.511 | Acc: 44.632% (15482/34688)\n",
            "Loss: 1.511 | Acc: 44.642% (15514/34752)\n",
            "Loss: 1.511 | Acc: 44.646% (15544/34816)\n",
            "Loss: 1.511 | Acc: 44.653% (15575/34880)\n",
            "Loss: 1.511 | Acc: 44.651% (15603/34944)\n",
            "Loss: 1.510 | Acc: 44.667% (15637/35008)\n",
            "Loss: 1.510 | Acc: 44.671% (15667/35072)\n",
            "Loss: 1.510 | Acc: 44.678% (15698/35136)\n",
            "Loss: 1.510 | Acc: 44.676% (15726/35200)\n",
            "Loss: 1.510 | Acc: 44.660% (15749/35264)\n",
            "Loss: 1.510 | Acc: 44.664% (15779/35328)\n",
            "Loss: 1.510 | Acc: 44.660% (15806/35392)\n",
            "Loss: 1.510 | Acc: 44.661% (15835/35456)\n",
            "Loss: 1.510 | Acc: 44.659% (15863/35520)\n",
            "Loss: 1.510 | Acc: 44.658% (15891/35584)\n",
            "Loss: 1.510 | Acc: 44.659% (15920/35648)\n",
            "Loss: 1.510 | Acc: 44.666% (15951/35712)\n",
            "Loss: 1.510 | Acc: 44.658% (15977/35776)\n",
            "Loss: 1.510 | Acc: 44.662% (16007/35840)\n",
            "Loss: 1.511 | Acc: 44.650% (16031/35904)\n",
            "Loss: 1.510 | Acc: 44.659% (16063/35968)\n",
            "Loss: 1.510 | Acc: 44.663% (16093/36032)\n",
            "Loss: 1.510 | Acc: 44.656% (16119/36096)\n",
            "Loss: 1.510 | Acc: 44.663% (16150/36160)\n",
            "Loss: 1.510 | Acc: 44.691% (16189/36224)\n",
            "Loss: 1.509 | Acc: 44.715% (16226/36288)\n",
            "Loss: 1.509 | Acc: 44.724% (16258/36352)\n",
            "Loss: 1.508 | Acc: 44.714% (16283/36416)\n",
            "Loss: 1.508 | Acc: 44.740% (16321/36480)\n",
            "Loss: 1.508 | Acc: 44.727% (16345/36544)\n",
            "Loss: 1.508 | Acc: 44.714% (16369/36608)\n",
            "Loss: 1.508 | Acc: 44.702% (16393/36672)\n",
            "Loss: 1.509 | Acc: 44.711% (16425/36736)\n",
            "Loss: 1.508 | Acc: 44.723% (16458/36800)\n",
            "Loss: 1.508 | Acc: 44.737% (16492/36864)\n",
            "Loss: 1.508 | Acc: 44.741% (16522/36928)\n",
            "Loss: 1.509 | Acc: 44.742% (16551/36992)\n",
            "Loss: 1.508 | Acc: 44.746% (16581/37056)\n",
            "Loss: 1.508 | Acc: 44.744% (16609/37120)\n",
            "Loss: 1.509 | Acc: 44.742% (16637/37184)\n",
            "Loss: 1.509 | Acc: 44.735% (16663/37248)\n",
            "Loss: 1.509 | Acc: 44.750% (16697/37312)\n",
            "Loss: 1.508 | Acc: 44.769% (16733/37376)\n",
            "Loss: 1.508 | Acc: 44.778% (16765/37440)\n",
            "Loss: 1.508 | Acc: 44.806% (16804/37504)\n",
            "Loss: 1.508 | Acc: 44.807% (16833/37568)\n",
            "Loss: 1.508 | Acc: 44.816% (16865/37632)\n",
            "Loss: 1.507 | Acc: 44.814% (16893/37696)\n",
            "Loss: 1.507 | Acc: 44.817% (16923/37760)\n",
            "Loss: 1.507 | Acc: 44.818% (16952/37824)\n",
            "Loss: 1.507 | Acc: 44.843% (16990/37888)\n",
            "Loss: 1.506 | Acc: 44.851% (17022/37952)\n",
            "Loss: 1.506 | Acc: 44.873% (17059/38016)\n",
            "Loss: 1.505 | Acc: 44.892% (17095/38080)\n",
            "Loss: 1.506 | Acc: 44.875% (17117/38144)\n",
            "Loss: 1.505 | Acc: 44.878% (17147/38208)\n",
            "Loss: 1.505 | Acc: 44.894% (17182/38272)\n",
            "Loss: 1.505 | Acc: 44.890% (17209/38336)\n",
            "Loss: 1.505 | Acc: 44.885% (17236/38400)\n",
            "Loss: 1.505 | Acc: 44.873% (17260/38464)\n",
            "Loss: 1.505 | Acc: 44.908% (17302/38528)\n",
            "Loss: 1.505 | Acc: 44.913% (17333/38592)\n",
            "Loss: 1.504 | Acc: 44.930% (17368/38656)\n",
            "Loss: 1.504 | Acc: 44.941% (17401/38720)\n",
            "Loss: 1.504 | Acc: 44.954% (17435/38784)\n",
            "Loss: 1.504 | Acc: 44.947% (17461/38848)\n",
            "Loss: 1.504 | Acc: 44.940% (17487/38912)\n",
            "Loss: 1.504 | Acc: 44.935% (17514/38976)\n",
            "Loss: 1.504 | Acc: 44.931% (17541/39040)\n",
            "Loss: 1.504 | Acc: 44.944% (17575/39104)\n",
            "Loss: 1.503 | Acc: 44.947% (17605/39168)\n",
            "Loss: 1.504 | Acc: 44.940% (17631/39232)\n",
            "Loss: 1.504 | Acc: 44.936% (17658/39296)\n",
            "Loss: 1.503 | Acc: 44.947% (17691/39360)\n",
            "Loss: 1.503 | Acc: 44.955% (17723/39424)\n",
            "Loss: 1.503 | Acc: 44.950% (17750/39488)\n",
            "Loss: 1.503 | Acc: 44.964% (17784/39552)\n",
            "Loss: 1.503 | Acc: 44.949% (17807/39616)\n",
            "Loss: 1.503 | Acc: 44.952% (17837/39680)\n",
            "Loss: 1.503 | Acc: 44.963% (17870/39744)\n",
            "Loss: 1.503 | Acc: 44.966% (17900/39808)\n",
            "Loss: 1.503 | Acc: 44.981% (17935/39872)\n",
            "Loss: 1.503 | Acc: 44.992% (17968/39936)\n",
            "Loss: 1.502 | Acc: 45.000% (18000/40000)\n",
            "Loss: 1.502 | Acc: 45.018% (18036/40064)\n",
            "Loss: 1.502 | Acc: 44.994% (18055/40128)\n",
            "Loss: 1.503 | Acc: 44.984% (18080/40192)\n",
            "Loss: 1.503 | Acc: 45.000% (18115/40256)\n",
            "Loss: 1.503 | Acc: 45.000% (18144/40320)\n",
            "Loss: 1.502 | Acc: 44.998% (18172/40384)\n",
            "Loss: 1.502 | Acc: 44.996% (18200/40448)\n",
            "Loss: 1.503 | Acc: 44.997% (18229/40512)\n",
            "Loss: 1.503 | Acc: 44.982% (18252/40576)\n",
            "Loss: 1.503 | Acc: 44.988% (18283/40640)\n",
            "Loss: 1.503 | Acc: 44.983% (18310/40704)\n",
            "Loss: 1.503 | Acc: 44.996% (18344/40768)\n",
            "Loss: 1.502 | Acc: 44.992% (18371/40832)\n",
            "Loss: 1.502 | Acc: 44.990% (18399/40896)\n",
            "Loss: 1.502 | Acc: 45.000% (18432/40960)\n",
            "Loss: 1.502 | Acc: 45.003% (18462/41024)\n",
            "Loss: 1.502 | Acc: 45.006% (18492/41088)\n",
            "Loss: 1.502 | Acc: 45.006% (18521/41152)\n",
            "Loss: 1.501 | Acc: 45.019% (18555/41216)\n",
            "Loss: 1.501 | Acc: 45.022% (18585/41280)\n",
            "Loss: 1.501 | Acc: 45.039% (18621/41344)\n",
            "Loss: 1.501 | Acc: 45.049% (18654/41408)\n",
            "Loss: 1.500 | Acc: 45.057% (18686/41472)\n",
            "Loss: 1.501 | Acc: 45.050% (18712/41536)\n",
            "Loss: 1.501 | Acc: 45.048% (18740/41600)\n",
            "Loss: 1.501 | Acc: 45.053% (18771/41664)\n",
            "Loss: 1.500 | Acc: 45.058% (18802/41728)\n",
            "Loss: 1.500 | Acc: 45.073% (18837/41792)\n",
            "Loss: 1.500 | Acc: 45.074% (18866/41856)\n",
            "Loss: 1.499 | Acc: 45.081% (18898/41920)\n",
            "Loss: 1.500 | Acc: 45.096% (18933/41984)\n",
            "Loss: 1.499 | Acc: 45.115% (18970/42048)\n",
            "Loss: 1.499 | Acc: 45.134% (19007/42112)\n",
            "Loss: 1.499 | Acc: 45.149% (19042/42176)\n",
            "Loss: 1.498 | Acc: 45.168% (19079/42240)\n",
            "Loss: 1.498 | Acc: 45.161% (19105/42304)\n",
            "Loss: 1.498 | Acc: 45.171% (19138/42368)\n",
            "Loss: 1.498 | Acc: 45.185% (19173/42432)\n",
            "Loss: 1.497 | Acc: 45.185% (19202/42496)\n",
            "Loss: 1.498 | Acc: 45.186% (19231/42560)\n",
            "Loss: 1.498 | Acc: 45.183% (19259/42624)\n",
            "Loss: 1.497 | Acc: 45.191% (19291/42688)\n",
            "Loss: 1.497 | Acc: 45.186% (19318/42752)\n",
            "Loss: 1.497 | Acc: 45.182% (19345/42816)\n",
            "Loss: 1.498 | Acc: 45.177% (19372/42880)\n",
            "Loss: 1.498 | Acc: 45.175% (19400/42944)\n",
            "Loss: 1.497 | Acc: 45.175% (19429/43008)\n",
            "Loss: 1.497 | Acc: 45.185% (19462/43072)\n",
            "Loss: 1.497 | Acc: 45.210% (19502/43136)\n",
            "Loss: 1.497 | Acc: 45.199% (19526/43200)\n",
            "Loss: 1.497 | Acc: 45.211% (19560/43264)\n",
            "Loss: 1.497 | Acc: 45.213% (19590/43328)\n",
            "Loss: 1.496 | Acc: 45.213% (19619/43392)\n",
            "Loss: 1.496 | Acc: 45.218% (19650/43456)\n",
            "Loss: 1.496 | Acc: 45.209% (19675/43520)\n",
            "Loss: 1.496 | Acc: 45.212% (19705/43584)\n",
            "Loss: 1.496 | Acc: 45.209% (19733/43648)\n",
            "Loss: 1.496 | Acc: 45.203% (19759/43712)\n",
            "Loss: 1.496 | Acc: 45.223% (19797/43776)\n",
            "Loss: 1.496 | Acc: 45.219% (19824/43840)\n",
            "Loss: 1.496 | Acc: 45.226% (19856/43904)\n",
            "Loss: 1.496 | Acc: 45.226% (19885/43968)\n",
            "Loss: 1.496 | Acc: 45.233% (19917/44032)\n",
            "Loss: 1.495 | Acc: 45.244% (19951/44096)\n",
            "Loss: 1.495 | Acc: 45.236% (19976/44160)\n",
            "Loss: 1.495 | Acc: 45.256% (20014/44224)\n",
            "Loss: 1.495 | Acc: 45.265% (20047/44288)\n",
            "Loss: 1.495 | Acc: 45.252% (20070/44352)\n",
            "Loss: 1.495 | Acc: 45.279% (20111/44416)\n",
            "Loss: 1.495 | Acc: 45.274% (20138/44480)\n",
            "Loss: 1.495 | Acc: 45.281% (20170/44544)\n",
            "Loss: 1.495 | Acc: 45.283% (20200/44608)\n",
            "Loss: 1.495 | Acc: 45.279% (20227/44672)\n",
            "Loss: 1.494 | Acc: 45.279% (20256/44736)\n",
            "Loss: 1.495 | Acc: 45.283% (20287/44800)\n",
            "Loss: 1.495 | Acc: 45.272% (20311/44864)\n",
            "Loss: 1.495 | Acc: 45.270% (20339/44928)\n",
            "Loss: 1.495 | Acc: 45.266% (20366/44992)\n",
            "Loss: 1.495 | Acc: 45.261% (20393/45056)\n",
            "Loss: 1.495 | Acc: 45.255% (20419/45120)\n",
            "Loss: 1.495 | Acc: 45.253% (20447/45184)\n",
            "Loss: 1.495 | Acc: 45.237% (20469/45248)\n",
            "Loss: 1.495 | Acc: 45.260% (20508/45312)\n",
            "Loss: 1.495 | Acc: 45.264% (20539/45376)\n",
            "Loss: 1.496 | Acc: 45.255% (20564/45440)\n",
            "Loss: 1.495 | Acc: 45.260% (20595/45504)\n",
            "Loss: 1.495 | Acc: 45.260% (20624/45568)\n",
            "Loss: 1.495 | Acc: 45.271% (20658/45632)\n",
            "Loss: 1.495 | Acc: 45.269% (20686/45696)\n",
            "Loss: 1.495 | Acc: 45.284% (20722/45760)\n",
            "Loss: 1.495 | Acc: 45.293% (20755/45824)\n",
            "Loss: 1.495 | Acc: 45.304% (20789/45888)\n",
            "Loss: 1.495 | Acc: 45.293% (20813/45952)\n",
            "Loss: 1.495 | Acc: 45.295% (20843/46016)\n",
            "Loss: 1.495 | Acc: 45.297% (20873/46080)\n",
            "Loss: 1.495 | Acc: 45.306% (20906/46144)\n",
            "Loss: 1.495 | Acc: 45.300% (20932/46208)\n",
            "Loss: 1.495 | Acc: 45.304% (20963/46272)\n",
            "Loss: 1.495 | Acc: 45.302% (20991/46336)\n",
            "Loss: 1.495 | Acc: 45.302% (21020/46400)\n",
            "Loss: 1.495 | Acc: 45.312% (21054/46464)\n",
            "Loss: 1.495 | Acc: 45.319% (21086/46528)\n",
            "Loss: 1.495 | Acc: 45.321% (21116/46592)\n",
            "Loss: 1.495 | Acc: 45.325% (21147/46656)\n",
            "Loss: 1.495 | Acc: 45.310% (21169/46720)\n",
            "Loss: 1.495 | Acc: 45.323% (21204/46784)\n",
            "Loss: 1.495 | Acc: 45.323% (21233/46848)\n",
            "Loss: 1.495 | Acc: 45.321% (21261/46912)\n",
            "Loss: 1.495 | Acc: 45.325% (21292/46976)\n",
            "Loss: 1.495 | Acc: 45.334% (21325/47040)\n",
            "Loss: 1.495 | Acc: 45.329% (21352/47104)\n",
            "Loss: 1.495 | Acc: 45.338% (21385/47168)\n",
            "Loss: 1.495 | Acc: 45.340% (21415/47232)\n",
            "Loss: 1.495 | Acc: 45.346% (21447/47296)\n",
            "Loss: 1.495 | Acc: 45.332% (21469/47360)\n",
            "Loss: 1.495 | Acc: 45.338% (21501/47424)\n",
            "Loss: 1.495 | Acc: 45.342% (21532/47488)\n",
            "Loss: 1.495 | Acc: 45.348% (21564/47552)\n",
            "Loss: 1.495 | Acc: 45.340% (21589/47616)\n",
            "Loss: 1.494 | Acc: 45.361% (21628/47680)\n",
            "Loss: 1.495 | Acc: 45.365% (21659/47744)\n",
            "Loss: 1.495 | Acc: 45.363% (21687/47808)\n",
            "Loss: 1.495 | Acc: 45.367% (21718/47872)\n",
            "Loss: 1.495 | Acc: 45.360% (21744/47936)\n",
            "Loss: 1.495 | Acc: 45.367% (21776/48000)\n",
            "Loss: 1.495 | Acc: 45.362% (21803/48064)\n",
            "Loss: 1.495 | Acc: 45.364% (21833/48128)\n",
            "Loss: 1.495 | Acc: 45.371% (21865/48192)\n",
            "Loss: 1.495 | Acc: 45.358% (21888/48256)\n",
            "Loss: 1.495 | Acc: 45.375% (21925/48320)\n",
            "Loss: 1.494 | Acc: 45.385% (21959/48384)\n",
            "Loss: 1.494 | Acc: 45.389% (21990/48448)\n",
            "Loss: 1.494 | Acc: 45.397% (22023/48512)\n",
            "Loss: 1.494 | Acc: 45.403% (22055/48576)\n",
            "Loss: 1.494 | Acc: 45.403% (22084/48640)\n",
            "Loss: 1.494 | Acc: 45.423% (22123/48704)\n",
            "Loss: 1.494 | Acc: 45.421% (22151/48768)\n",
            "Loss: 1.494 | Acc: 45.433% (22186/48832)\n",
            "Loss: 1.493 | Acc: 45.443% (22220/48896)\n",
            "Loss: 1.493 | Acc: 45.455% (22255/48960)\n",
            "Loss: 1.493 | Acc: 45.457% (22274/49000)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 45.457142857142856\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.385 | Acc: 56.250% (36/64)\n",
            "Loss: 1.317 | Acc: 55.469% (71/128)\n",
            "Loss: 1.380 | Acc: 51.562% (99/192)\n",
            "Loss: 1.444 | Acc: 49.609% (127/256)\n",
            "Loss: 1.409 | Acc: 49.688% (159/320)\n",
            "Loss: 1.423 | Acc: 48.698% (187/384)\n",
            "Loss: 1.422 | Acc: 48.214% (216/448)\n",
            "Loss: 1.411 | Acc: 48.828% (250/512)\n",
            "Loss: 1.406 | Acc: 48.264% (278/576)\n",
            "Loss: 1.406 | Acc: 48.438% (310/640)\n",
            "Loss: 1.417 | Acc: 47.301% (333/704)\n",
            "Loss: 1.421 | Acc: 47.786% (367/768)\n",
            "Loss: 1.421 | Acc: 47.596% (396/832)\n",
            "Loss: 1.417 | Acc: 47.656% (427/896)\n",
            "Loss: 1.414 | Acc: 47.604% (457/960)\n",
            "Loss: 1.404 | Acc: 48.047% (492/1024)\n",
            "Loss: 1.414 | Acc: 47.794% (520/1088)\n",
            "Loss: 1.412 | Acc: 47.569% (548/1152)\n",
            "Loss: 1.405 | Acc: 47.697% (580/1216)\n",
            "Loss: 1.426 | Acc: 47.109% (603/1280)\n",
            "Loss: 1.425 | Acc: 46.726% (628/1344)\n",
            "Loss: 1.423 | Acc: 46.875% (660/1408)\n",
            "Loss: 1.414 | Acc: 46.807% (689/1472)\n",
            "Loss: 1.415 | Acc: 47.070% (723/1536)\n",
            "Loss: 1.414 | Acc: 47.375% (758/1600)\n",
            "Loss: 1.415 | Acc: 47.296% (787/1664)\n",
            "Loss: 1.413 | Acc: 47.454% (820/1728)\n",
            "Loss: 1.410 | Acc: 47.489% (851/1792)\n",
            "Loss: 1.410 | Acc: 47.522% (882/1856)\n",
            "Loss: 1.409 | Acc: 47.448% (911/1920)\n",
            "Loss: 1.412 | Acc: 47.732% (947/1984)\n",
            "Loss: 1.412 | Acc: 47.998% (983/2048)\n",
            "Loss: 1.406 | Acc: 48.011% (1014/2112)\n",
            "Loss: 1.407 | Acc: 47.932% (1043/2176)\n",
            "Loss: 1.406 | Acc: 47.946% (1074/2240)\n",
            "Loss: 1.405 | Acc: 48.090% (1108/2304)\n",
            "Loss: 1.404 | Acc: 48.226% (1142/2368)\n",
            "Loss: 1.401 | Acc: 48.438% (1178/2432)\n",
            "Loss: 1.402 | Acc: 48.317% (1206/2496)\n",
            "Loss: 1.409 | Acc: 48.203% (1234/2560)\n",
            "Loss: 1.413 | Acc: 48.095% (1262/2624)\n",
            "Loss: 1.411 | Acc: 48.140% (1294/2688)\n",
            "Loss: 1.409 | Acc: 48.219% (1327/2752)\n",
            "Loss: 1.408 | Acc: 48.295% (1360/2816)\n",
            "Loss: 1.408 | Acc: 48.299% (1391/2880)\n",
            "Loss: 1.406 | Acc: 48.438% (1426/2944)\n",
            "Loss: 1.406 | Acc: 48.371% (1455/3008)\n",
            "Loss: 1.406 | Acc: 48.372% (1486/3072)\n",
            "Loss: 1.405 | Acc: 48.119% (1509/3136)\n",
            "Loss: 1.407 | Acc: 48.125% (1540/3200)\n",
            "Loss: 1.407 | Acc: 48.162% (1572/3264)\n",
            "Loss: 1.409 | Acc: 48.137% (1602/3328)\n",
            "Loss: 1.408 | Acc: 48.113% (1632/3392)\n",
            "Loss: 1.409 | Acc: 48.003% (1659/3456)\n",
            "Loss: 1.411 | Acc: 47.869% (1685/3520)\n",
            "Loss: 1.407 | Acc: 48.075% (1723/3584)\n",
            "Loss: 1.410 | Acc: 48.054% (1753/3648)\n",
            "Loss: 1.408 | Acc: 48.168% (1788/3712)\n",
            "Loss: 1.410 | Acc: 48.120% (1817/3776)\n",
            "Loss: 1.409 | Acc: 48.125% (1848/3840)\n",
            "Loss: 1.409 | Acc: 48.028% (1875/3904)\n",
            "Loss: 1.406 | Acc: 48.160% (1911/3968)\n",
            "Loss: 1.407 | Acc: 48.165% (1942/4032)\n",
            "Loss: 1.406 | Acc: 48.145% (1972/4096)\n",
            "Loss: 1.407 | Acc: 48.149% (2003/4160)\n",
            "Loss: 1.405 | Acc: 48.224% (2037/4224)\n",
            "Loss: 1.405 | Acc: 48.298% (2071/4288)\n",
            "Loss: 1.406 | Acc: 48.231% (2099/4352)\n",
            "Loss: 1.404 | Acc: 48.370% (2136/4416)\n",
            "Loss: 1.402 | Acc: 48.527% (2174/4480)\n",
            "Loss: 1.401 | Acc: 48.526% (2205/4544)\n",
            "Loss: 1.404 | Acc: 48.481% (2234/4608)\n",
            "Loss: 1.402 | Acc: 48.673% (2274/4672)\n",
            "Loss: 1.400 | Acc: 48.733% (2308/4736)\n",
            "Loss: 1.402 | Acc: 48.729% (2339/4800)\n",
            "Loss: 1.400 | Acc: 48.828% (2375/4864)\n",
            "Loss: 1.400 | Acc: 48.884% (2409/4928)\n",
            "Loss: 1.401 | Acc: 48.798% (2436/4992)\n",
            "Loss: 1.399 | Acc: 48.952% (2475/5056)\n",
            "Loss: 1.400 | Acc: 48.945% (2506/5120)\n",
            "Loss: 1.399 | Acc: 49.055% (2543/5184)\n",
            "Loss: 1.398 | Acc: 49.009% (2572/5248)\n",
            "Loss: 1.397 | Acc: 48.946% (2600/5312)\n",
            "Loss: 1.398 | Acc: 48.940% (2631/5376)\n",
            "Loss: 1.399 | Acc: 48.897% (2660/5440)\n",
            "Loss: 1.399 | Acc: 48.892% (2691/5504)\n",
            "Loss: 1.403 | Acc: 48.779% (2716/5568)\n",
            "Loss: 1.404 | Acc: 48.757% (2746/5632)\n",
            "Loss: 1.404 | Acc: 48.824% (2781/5696)\n",
            "Loss: 1.402 | Acc: 48.889% (2816/5760)\n",
            "Loss: 1.401 | Acc: 48.987% (2853/5824)\n",
            "Loss: 1.403 | Acc: 48.896% (2879/5888)\n",
            "Loss: 1.405 | Acc: 48.824% (2906/5952)\n",
            "Loss: 1.405 | Acc: 48.770% (2934/6016)\n",
            "Loss: 1.405 | Acc: 48.766% (2965/6080)\n",
            "Loss: 1.404 | Acc: 48.747% (2995/6144)\n",
            "Loss: 1.405 | Acc: 48.679% (3022/6208)\n",
            "Loss: 1.407 | Acc: 48.597% (3048/6272)\n",
            "Loss: 1.406 | Acc: 48.627% (3081/6336)\n",
            "Loss: 1.405 | Acc: 48.578% (3109/6400)\n",
            "Loss: 1.408 | Acc: 48.422% (3130/6464)\n",
            "Loss: 1.407 | Acc: 48.483% (3165/6528)\n",
            "Loss: 1.407 | Acc: 48.559% (3201/6592)\n",
            "Loss: 1.406 | Acc: 48.573% (3233/6656)\n",
            "Loss: 1.406 | Acc: 48.542% (3262/6720)\n",
            "Loss: 1.406 | Acc: 48.555% (3294/6784)\n",
            "Loss: 1.403 | Acc: 48.730% (3337/6848)\n",
            "Loss: 1.407 | Acc: 48.568% (3357/6912)\n",
            "Loss: 1.410 | Acc: 48.524% (3385/6976)\n",
            "Loss: 1.411 | Acc: 48.480% (3413/7040)\n",
            "Loss: 1.410 | Acc: 48.494% (3445/7104)\n",
            "Loss: 1.408 | Acc: 48.549% (3480/7168)\n",
            "Loss: 1.408 | Acc: 48.534% (3510/7232)\n",
            "Loss: 1.405 | Acc: 48.602% (3546/7296)\n",
            "Loss: 1.403 | Acc: 48.601% (3577/7360)\n",
            "Loss: 1.404 | Acc: 48.572% (3606/7424)\n",
            "Loss: 1.403 | Acc: 48.598% (3639/7488)\n",
            "Loss: 1.403 | Acc: 48.557% (3667/7552)\n",
            "Loss: 1.403 | Acc: 48.556% (3698/7616)\n",
            "Loss: 1.403 | Acc: 48.568% (3730/7680)\n",
            "Loss: 1.402 | Acc: 48.592% (3763/7744)\n",
            "Loss: 1.401 | Acc: 48.642% (3798/7808)\n",
            "Loss: 1.401 | Acc: 48.653% (3830/7872)\n",
            "Loss: 1.402 | Acc: 48.601% (3857/7936)\n",
            "Loss: 1.401 | Acc: 48.650% (3892/8000)\n",
            "Loss: 1.400 | Acc: 48.661% (3924/8064)\n",
            "Loss: 1.401 | Acc: 48.671% (3956/8128)\n",
            "Loss: 1.400 | Acc: 48.669% (3987/8192)\n",
            "Loss: 1.401 | Acc: 48.619% (4014/8256)\n",
            "Loss: 1.404 | Acc: 48.450% (4031/8320)\n",
            "Loss: 1.404 | Acc: 48.438% (4061/8384)\n",
            "Loss: 1.404 | Acc: 48.438% (4092/8448)\n",
            "Loss: 1.405 | Acc: 48.449% (4124/8512)\n",
            "Loss: 1.406 | Acc: 48.449% (4155/8576)\n",
            "Loss: 1.407 | Acc: 48.426% (4184/8640)\n",
            "Loss: 1.407 | Acc: 48.415% (4214/8704)\n",
            "Loss: 1.407 | Acc: 48.392% (4243/8768)\n",
            "Loss: 1.407 | Acc: 48.438% (4278/8832)\n",
            "Loss: 1.407 | Acc: 48.438% (4309/8896)\n",
            "Loss: 1.407 | Acc: 48.471% (4343/8960)\n",
            "Loss: 1.408 | Acc: 48.438% (4371/9024)\n",
            "Loss: 1.408 | Acc: 48.393% (4398/9088)\n",
            "Loss: 1.409 | Acc: 48.350% (4425/9152)\n",
            "Loss: 1.406 | Acc: 48.470% (4467/9216)\n",
            "Loss: 1.405 | Acc: 48.524% (4503/9280)\n",
            "Loss: 1.406 | Acc: 48.502% (4532/9344)\n",
            "Loss: 1.407 | Acc: 48.491% (4562/9408)\n",
            "Loss: 1.408 | Acc: 48.459% (4590/9472)\n",
            "Loss: 1.407 | Acc: 48.479% (4623/9536)\n",
            "Loss: 1.407 | Acc: 48.531% (4659/9600)\n",
            "Loss: 1.406 | Acc: 48.593% (4696/9664)\n",
            "Loss: 1.407 | Acc: 48.499% (4718/9728)\n",
            "Loss: 1.408 | Acc: 48.448% (4744/9792)\n",
            "Loss: 1.408 | Acc: 48.427% (4773/9856)\n",
            "Loss: 1.408 | Acc: 48.448% (4806/9920)\n",
            "Loss: 1.408 | Acc: 48.417% (4834/9984)\n",
            "Loss: 1.408 | Acc: 48.420% (4842/10000)\n",
            "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 48.42\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 1.491 | Acc: 46.875% (30/64)\n",
            "Loss: 1.458 | Acc: 46.875% (60/128)\n",
            "Loss: 1.412 | Acc: 46.875% (90/192)\n",
            "Loss: 1.366 | Acc: 48.828% (125/256)\n",
            "Loss: 1.385 | Acc: 49.375% (158/320)\n",
            "Loss: 1.398 | Acc: 47.917% (184/384)\n",
            "Loss: 1.405 | Acc: 47.991% (215/448)\n",
            "Loss: 1.377 | Acc: 48.242% (247/512)\n",
            "Loss: 1.396 | Acc: 47.569% (274/576)\n",
            "Loss: 1.389 | Acc: 46.875% (300/640)\n",
            "Loss: 1.381 | Acc: 48.295% (340/704)\n",
            "Loss: 1.370 | Acc: 48.568% (373/768)\n",
            "Loss: 1.355 | Acc: 49.760% (414/832)\n",
            "Loss: 1.338 | Acc: 50.223% (450/896)\n",
            "Loss: 1.334 | Acc: 50.729% (487/960)\n",
            "Loss: 1.346 | Acc: 50.098% (513/1024)\n",
            "Loss: 1.345 | Acc: 50.368% (548/1088)\n",
            "Loss: 1.338 | Acc: 50.694% (584/1152)\n",
            "Loss: 1.351 | Acc: 51.069% (621/1216)\n",
            "Loss: 1.356 | Acc: 50.859% (651/1280)\n",
            "Loss: 1.351 | Acc: 51.339% (690/1344)\n",
            "Loss: 1.358 | Acc: 51.420% (724/1408)\n",
            "Loss: 1.362 | Acc: 51.155% (753/1472)\n",
            "Loss: 1.374 | Acc: 50.586% (777/1536)\n",
            "Loss: 1.381 | Acc: 50.188% (803/1600)\n",
            "Loss: 1.375 | Acc: 50.481% (840/1664)\n",
            "Loss: 1.382 | Acc: 50.231% (868/1728)\n",
            "Loss: 1.378 | Acc: 50.391% (903/1792)\n",
            "Loss: 1.379 | Acc: 50.377% (935/1856)\n",
            "Loss: 1.383 | Acc: 50.156% (963/1920)\n",
            "Loss: 1.384 | Acc: 49.950% (991/1984)\n",
            "Loss: 1.389 | Acc: 49.658% (1017/2048)\n",
            "Loss: 1.396 | Acc: 49.384% (1043/2112)\n",
            "Loss: 1.393 | Acc: 49.586% (1079/2176)\n",
            "Loss: 1.393 | Acc: 49.509% (1109/2240)\n",
            "Loss: 1.399 | Acc: 49.349% (1137/2304)\n",
            "Loss: 1.396 | Acc: 49.324% (1168/2368)\n",
            "Loss: 1.396 | Acc: 49.342% (1200/2432)\n",
            "Loss: 1.404 | Acc: 48.958% (1222/2496)\n",
            "Loss: 1.407 | Acc: 48.750% (1248/2560)\n",
            "Loss: 1.405 | Acc: 48.857% (1282/2624)\n",
            "Loss: 1.406 | Acc: 48.735% (1310/2688)\n",
            "Loss: 1.408 | Acc: 48.692% (1340/2752)\n",
            "Loss: 1.413 | Acc: 48.544% (1367/2816)\n",
            "Loss: 1.413 | Acc: 48.681% (1402/2880)\n",
            "Loss: 1.414 | Acc: 48.505% (1428/2944)\n",
            "Loss: 1.416 | Acc: 48.471% (1458/3008)\n",
            "Loss: 1.414 | Acc: 48.665% (1495/3072)\n",
            "Loss: 1.417 | Acc: 48.597% (1524/3136)\n",
            "Loss: 1.422 | Acc: 48.312% (1546/3200)\n",
            "Loss: 1.426 | Acc: 48.284% (1576/3264)\n",
            "Loss: 1.427 | Acc: 48.197% (1604/3328)\n",
            "Loss: 1.428 | Acc: 48.025% (1629/3392)\n",
            "Loss: 1.431 | Acc: 47.917% (1656/3456)\n",
            "Loss: 1.432 | Acc: 47.898% (1686/3520)\n",
            "Loss: 1.431 | Acc: 47.907% (1717/3584)\n",
            "Loss: 1.425 | Acc: 48.163% (1757/3648)\n",
            "Loss: 1.427 | Acc: 48.060% (1784/3712)\n",
            "Loss: 1.430 | Acc: 47.961% (1811/3776)\n",
            "Loss: 1.431 | Acc: 47.865% (1838/3840)\n",
            "Loss: 1.433 | Acc: 47.848% (1868/3904)\n",
            "Loss: 1.433 | Acc: 47.908% (1901/3968)\n",
            "Loss: 1.434 | Acc: 47.817% (1928/4032)\n",
            "Loss: 1.432 | Acc: 47.974% (1965/4096)\n",
            "Loss: 1.429 | Acc: 48.053% (1999/4160)\n",
            "Loss: 1.429 | Acc: 48.011% (2028/4224)\n",
            "Loss: 1.425 | Acc: 48.158% (2065/4288)\n",
            "Loss: 1.425 | Acc: 48.231% (2099/4352)\n",
            "Loss: 1.425 | Acc: 48.234% (2130/4416)\n",
            "Loss: 1.422 | Acc: 48.393% (2168/4480)\n",
            "Loss: 1.420 | Acc: 48.460% (2202/4544)\n",
            "Loss: 1.420 | Acc: 48.394% (2230/4608)\n",
            "Loss: 1.421 | Acc: 48.438% (2263/4672)\n",
            "Loss: 1.421 | Acc: 48.438% (2294/4736)\n",
            "Loss: 1.421 | Acc: 48.479% (2327/4800)\n",
            "Loss: 1.424 | Acc: 48.396% (2354/4864)\n",
            "Loss: 1.425 | Acc: 48.316% (2381/4928)\n",
            "Loss: 1.424 | Acc: 48.317% (2412/4992)\n",
            "Loss: 1.423 | Acc: 48.319% (2443/5056)\n",
            "Loss: 1.423 | Acc: 48.340% (2475/5120)\n",
            "Loss: 1.424 | Acc: 48.206% (2499/5184)\n",
            "Loss: 1.422 | Acc: 48.266% (2533/5248)\n",
            "Loss: 1.423 | Acc: 48.268% (2564/5312)\n",
            "Loss: 1.423 | Acc: 48.251% (2594/5376)\n",
            "Loss: 1.421 | Acc: 48.346% (2630/5440)\n",
            "Loss: 1.422 | Acc: 48.310% (2659/5504)\n",
            "Loss: 1.422 | Acc: 48.276% (2688/5568)\n",
            "Loss: 1.422 | Acc: 48.295% (2720/5632)\n",
            "Loss: 1.421 | Acc: 48.332% (2753/5696)\n",
            "Loss: 1.421 | Acc: 48.299% (2782/5760)\n",
            "Loss: 1.421 | Acc: 48.249% (2810/5824)\n",
            "Loss: 1.424 | Acc: 48.166% (2836/5888)\n",
            "Loss: 1.425 | Acc: 48.185% (2868/5952)\n",
            "Loss: 1.425 | Acc: 48.188% (2899/6016)\n",
            "Loss: 1.424 | Acc: 48.257% (2934/6080)\n",
            "Loss: 1.426 | Acc: 48.307% (2968/6144)\n",
            "Loss: 1.426 | Acc: 48.309% (2999/6208)\n",
            "Loss: 1.424 | Acc: 48.358% (3033/6272)\n",
            "Loss: 1.423 | Acc: 48.501% (3073/6336)\n",
            "Loss: 1.422 | Acc: 48.547% (3107/6400)\n",
            "Loss: 1.421 | Acc: 48.654% (3145/6464)\n",
            "Loss: 1.420 | Acc: 48.713% (3180/6528)\n",
            "Loss: 1.417 | Acc: 48.802% (3217/6592)\n",
            "Loss: 1.414 | Acc: 48.948% (3258/6656)\n",
            "Loss: 1.414 | Acc: 48.988% (3292/6720)\n",
            "Loss: 1.413 | Acc: 48.983% (3323/6784)\n",
            "Loss: 1.413 | Acc: 48.934% (3351/6848)\n",
            "Loss: 1.413 | Acc: 48.872% (3378/6912)\n",
            "Loss: 1.413 | Acc: 48.853% (3408/6976)\n",
            "Loss: 1.413 | Acc: 48.906% (3443/7040)\n",
            "Loss: 1.413 | Acc: 48.874% (3472/7104)\n",
            "Loss: 1.413 | Acc: 48.870% (3503/7168)\n",
            "Loss: 1.414 | Acc: 48.783% (3528/7232)\n",
            "Loss: 1.415 | Acc: 48.835% (3563/7296)\n",
            "Loss: 1.414 | Acc: 48.804% (3592/7360)\n",
            "Loss: 1.415 | Acc: 48.734% (3618/7424)\n",
            "Loss: 1.416 | Acc: 48.705% (3647/7488)\n",
            "Loss: 1.416 | Acc: 48.729% (3680/7552)\n",
            "Loss: 1.413 | Acc: 48.779% (3715/7616)\n",
            "Loss: 1.414 | Acc: 48.737% (3743/7680)\n",
            "Loss: 1.414 | Acc: 48.773% (3777/7744)\n",
            "Loss: 1.414 | Acc: 48.770% (3808/7808)\n",
            "Loss: 1.415 | Acc: 48.717% (3835/7872)\n",
            "Loss: 1.415 | Acc: 48.677% (3863/7936)\n",
            "Loss: 1.415 | Acc: 48.725% (3898/8000)\n",
            "Loss: 1.415 | Acc: 48.748% (3931/8064)\n",
            "Loss: 1.415 | Acc: 48.733% (3961/8128)\n",
            "Loss: 1.414 | Acc: 48.792% (3997/8192)\n",
            "Loss: 1.416 | Acc: 48.789% (4028/8256)\n",
            "Loss: 1.416 | Acc: 48.798% (4060/8320)\n",
            "Loss: 1.415 | Acc: 48.867% (4097/8384)\n",
            "Loss: 1.415 | Acc: 48.852% (4127/8448)\n",
            "Loss: 1.414 | Acc: 48.849% (4158/8512)\n",
            "Loss: 1.413 | Acc: 48.951% (4198/8576)\n",
            "Loss: 1.412 | Acc: 48.958% (4230/8640)\n",
            "Loss: 1.411 | Acc: 49.012% (4266/8704)\n",
            "Loss: 1.412 | Acc: 48.962% (4293/8768)\n",
            "Loss: 1.411 | Acc: 48.992% (4327/8832)\n",
            "Loss: 1.409 | Acc: 49.011% (4360/8896)\n",
            "Loss: 1.409 | Acc: 49.062% (4396/8960)\n",
            "Loss: 1.412 | Acc: 48.980% (4420/9024)\n",
            "Loss: 1.411 | Acc: 48.966% (4450/9088)\n",
            "Loss: 1.411 | Acc: 49.006% (4485/9152)\n",
            "Loss: 1.412 | Acc: 48.947% (4511/9216)\n",
            "Loss: 1.413 | Acc: 48.933% (4541/9280)\n",
            "Loss: 1.412 | Acc: 48.994% (4578/9344)\n",
            "Loss: 1.412 | Acc: 48.969% (4607/9408)\n",
            "Loss: 1.411 | Acc: 49.039% (4645/9472)\n",
            "Loss: 1.411 | Acc: 49.046% (4677/9536)\n",
            "Loss: 1.412 | Acc: 48.958% (4700/9600)\n",
            "Loss: 1.411 | Acc: 49.038% (4739/9664)\n",
            "Loss: 1.411 | Acc: 49.034% (4770/9728)\n",
            "Loss: 1.410 | Acc: 49.050% (4803/9792)\n",
            "Loss: 1.410 | Acc: 49.077% (4837/9856)\n",
            "Loss: 1.411 | Acc: 49.032% (4864/9920)\n",
            "Loss: 1.411 | Acc: 49.028% (4895/9984)\n",
            "Loss: 1.411 | Acc: 49.015% (4925/10048)\n",
            "Loss: 1.412 | Acc: 48.981% (4953/10112)\n",
            "Loss: 1.412 | Acc: 48.978% (4984/10176)\n",
            "Loss: 1.413 | Acc: 48.975% (5015/10240)\n",
            "Loss: 1.411 | Acc: 49.049% (5054/10304)\n",
            "Loss: 1.413 | Acc: 49.016% (5082/10368)\n",
            "Loss: 1.412 | Acc: 49.089% (5121/10432)\n",
            "Loss: 1.412 | Acc: 49.076% (5151/10496)\n",
            "Loss: 1.412 | Acc: 49.110% (5186/10560)\n",
            "Loss: 1.412 | Acc: 49.125% (5219/10624)\n",
            "Loss: 1.411 | Acc: 49.092% (5247/10688)\n",
            "Loss: 1.411 | Acc: 49.116% (5281/10752)\n",
            "Loss: 1.411 | Acc: 49.122% (5313/10816)\n",
            "Loss: 1.411 | Acc: 49.118% (5344/10880)\n",
            "Loss: 1.412 | Acc: 49.114% (5375/10944)\n",
            "Loss: 1.412 | Acc: 49.073% (5402/11008)\n",
            "Loss: 1.412 | Acc: 49.097% (5436/11072)\n",
            "Loss: 1.412 | Acc: 49.102% (5468/11136)\n",
            "Loss: 1.412 | Acc: 49.134% (5503/11200)\n",
            "Loss: 1.412 | Acc: 49.103% (5531/11264)\n",
            "Loss: 1.412 | Acc: 49.073% (5559/11328)\n",
            "Loss: 1.412 | Acc: 49.087% (5592/11392)\n",
            "Loss: 1.413 | Acc: 49.022% (5616/11456)\n",
            "Loss: 1.411 | Acc: 49.045% (5650/11520)\n",
            "Loss: 1.411 | Acc: 49.068% (5684/11584)\n",
            "Loss: 1.412 | Acc: 48.978% (5705/11648)\n",
            "Loss: 1.412 | Acc: 48.924% (5730/11712)\n",
            "Loss: 1.413 | Acc: 48.947% (5764/11776)\n",
            "Loss: 1.413 | Acc: 48.927% (5793/11840)\n",
            "Loss: 1.413 | Acc: 48.933% (5825/11904)\n",
            "Loss: 1.412 | Acc: 48.930% (5856/11968)\n",
            "Loss: 1.412 | Acc: 48.911% (5885/12032)\n",
            "Loss: 1.412 | Acc: 48.975% (5924/12096)\n",
            "Loss: 1.412 | Acc: 48.956% (5953/12160)\n",
            "Loss: 1.413 | Acc: 48.920% (5980/12224)\n",
            "Loss: 1.413 | Acc: 48.926% (6012/12288)\n",
            "Loss: 1.414 | Acc: 48.915% (6042/12352)\n",
            "Loss: 1.415 | Acc: 48.905% (6072/12416)\n",
            "Loss: 1.415 | Acc: 48.886% (6101/12480)\n",
            "Loss: 1.415 | Acc: 48.868% (6130/12544)\n",
            "Loss: 1.415 | Acc: 48.850% (6159/12608)\n",
            "Loss: 1.414 | Acc: 48.856% (6191/12672)\n",
            "Loss: 1.415 | Acc: 48.877% (6225/12736)\n",
            "Loss: 1.415 | Acc: 48.875% (6256/12800)\n",
            "Loss: 1.414 | Acc: 48.896% (6290/12864)\n",
            "Loss: 1.414 | Acc: 48.902% (6322/12928)\n",
            "Loss: 1.413 | Acc: 48.946% (6359/12992)\n",
            "Loss: 1.414 | Acc: 48.951% (6391/13056)\n",
            "Loss: 1.414 | Acc: 48.941% (6421/13120)\n",
            "Loss: 1.415 | Acc: 48.923% (6450/13184)\n",
            "Loss: 1.415 | Acc: 48.898% (6478/13248)\n",
            "Loss: 1.415 | Acc: 48.903% (6510/13312)\n",
            "Loss: 1.415 | Acc: 48.886% (6539/13376)\n",
            "Loss: 1.415 | Acc: 48.921% (6575/13440)\n",
            "Loss: 1.415 | Acc: 48.911% (6605/13504)\n",
            "Loss: 1.416 | Acc: 48.894% (6634/13568)\n",
            "Loss: 1.416 | Acc: 48.892% (6665/13632)\n",
            "Loss: 1.415 | Acc: 48.919% (6700/13696)\n",
            "Loss: 1.415 | Acc: 48.946% (6735/13760)\n",
            "Loss: 1.414 | Acc: 48.987% (6772/13824)\n",
            "Loss: 1.413 | Acc: 48.999% (6805/13888)\n",
            "Loss: 1.414 | Acc: 48.989% (6835/13952)\n",
            "Loss: 1.414 | Acc: 48.951% (6861/14016)\n",
            "Loss: 1.415 | Acc: 48.928% (6889/14080)\n",
            "Loss: 1.415 | Acc: 48.939% (6922/14144)\n",
            "Loss: 1.415 | Acc: 48.909% (6949/14208)\n",
            "Loss: 1.415 | Acc: 48.914% (6981/14272)\n",
            "Loss: 1.414 | Acc: 48.940% (7016/14336)\n",
            "Loss: 1.414 | Acc: 48.931% (7046/14400)\n",
            "Loss: 1.414 | Acc: 48.921% (7076/14464)\n",
            "Loss: 1.414 | Acc: 48.906% (7105/14528)\n",
            "Loss: 1.414 | Acc: 48.897% (7135/14592)\n",
            "Loss: 1.414 | Acc: 48.901% (7167/14656)\n",
            "Loss: 1.413 | Acc: 48.940% (7204/14720)\n",
            "Loss: 1.414 | Acc: 48.931% (7234/14784)\n",
            "Loss: 1.414 | Acc: 48.902% (7261/14848)\n",
            "Loss: 1.414 | Acc: 48.880% (7289/14912)\n",
            "Loss: 1.414 | Acc: 48.872% (7319/14976)\n",
            "Loss: 1.414 | Acc: 48.883% (7352/15040)\n",
            "Loss: 1.414 | Acc: 48.874% (7382/15104)\n",
            "Loss: 1.416 | Acc: 48.826% (7406/15168)\n",
            "Loss: 1.416 | Acc: 48.825% (7437/15232)\n",
            "Loss: 1.415 | Acc: 48.849% (7472/15296)\n",
            "Loss: 1.415 | Acc: 48.822% (7499/15360)\n",
            "Loss: 1.414 | Acc: 48.859% (7536/15424)\n",
            "Loss: 1.413 | Acc: 48.909% (7575/15488)\n",
            "Loss: 1.413 | Acc: 48.920% (7608/15552)\n",
            "Loss: 1.413 | Acc: 48.918% (7639/15616)\n",
            "Loss: 1.413 | Acc: 48.916% (7670/15680)\n",
            "Loss: 1.413 | Acc: 48.901% (7699/15744)\n",
            "Loss: 1.413 | Acc: 48.912% (7732/15808)\n",
            "Loss: 1.413 | Acc: 48.916% (7764/15872)\n",
            "Loss: 1.413 | Acc: 48.870% (7788/15936)\n",
            "Loss: 1.412 | Acc: 48.894% (7823/16000)\n",
            "Loss: 1.412 | Acc: 48.879% (7852/16064)\n",
            "Loss: 1.412 | Acc: 48.909% (7888/16128)\n",
            "Loss: 1.411 | Acc: 48.950% (7926/16192)\n",
            "Loss: 1.412 | Acc: 48.960% (7959/16256)\n",
            "Loss: 1.412 | Acc: 48.958% (7990/16320)\n",
            "Loss: 1.411 | Acc: 48.993% (8027/16384)\n",
            "Loss: 1.412 | Acc: 48.954% (8052/16448)\n",
            "Loss: 1.413 | Acc: 48.940% (8081/16512)\n",
            "Loss: 1.414 | Acc: 48.902% (8106/16576)\n",
            "Loss: 1.414 | Acc: 48.882% (8134/16640)\n",
            "Loss: 1.414 | Acc: 48.904% (8169/16704)\n",
            "Loss: 1.413 | Acc: 48.921% (8203/16768)\n",
            "Loss: 1.413 | Acc: 48.942% (8238/16832)\n",
            "Loss: 1.412 | Acc: 48.976% (8275/16896)\n",
            "Loss: 1.411 | Acc: 49.033% (8316/16960)\n",
            "Loss: 1.411 | Acc: 49.001% (8342/17024)\n",
            "Loss: 1.412 | Acc: 49.017% (8376/17088)\n",
            "Loss: 1.412 | Acc: 49.032% (8410/17152)\n",
            "Loss: 1.412 | Acc: 49.053% (8445/17216)\n",
            "Loss: 1.411 | Acc: 49.091% (8483/17280)\n",
            "Loss: 1.410 | Acc: 49.112% (8518/17344)\n",
            "Loss: 1.410 | Acc: 49.133% (8553/17408)\n",
            "Loss: 1.409 | Acc: 49.176% (8592/17472)\n",
            "Loss: 1.410 | Acc: 49.162% (8621/17536)\n",
            "Loss: 1.410 | Acc: 49.170% (8654/17600)\n",
            "Loss: 1.410 | Acc: 49.179% (8687/17664)\n",
            "Loss: 1.410 | Acc: 49.182% (8719/17728)\n",
            "Loss: 1.410 | Acc: 49.163% (8747/17792)\n",
            "Loss: 1.409 | Acc: 49.188% (8783/17856)\n",
            "Loss: 1.409 | Acc: 49.208% (8818/17920)\n",
            "Loss: 1.409 | Acc: 49.188% (8846/17984)\n",
            "Loss: 1.409 | Acc: 49.180% (8876/18048)\n",
            "Loss: 1.409 | Acc: 49.177% (8907/18112)\n",
            "Loss: 1.409 | Acc: 49.175% (8938/18176)\n",
            "Loss: 1.409 | Acc: 49.161% (8967/18240)\n",
            "Loss: 1.409 | Acc: 49.191% (9004/18304)\n",
            "Loss: 1.409 | Acc: 49.205% (9038/18368)\n",
            "Loss: 1.408 | Acc: 49.224% (9073/18432)\n",
            "Loss: 1.409 | Acc: 49.211% (9102/18496)\n",
            "Loss: 1.408 | Acc: 49.219% (9135/18560)\n",
            "Loss: 1.408 | Acc: 49.227% (9168/18624)\n",
            "Loss: 1.408 | Acc: 49.229% (9200/18688)\n",
            "Loss: 1.408 | Acc: 49.259% (9237/18752)\n",
            "Loss: 1.408 | Acc: 49.261% (9269/18816)\n",
            "Loss: 1.408 | Acc: 49.248% (9298/18880)\n",
            "Loss: 1.408 | Acc: 49.245% (9329/18944)\n",
            "Loss: 1.407 | Acc: 49.253% (9362/19008)\n",
            "Loss: 1.407 | Acc: 49.234% (9390/19072)\n",
            "Loss: 1.407 | Acc: 49.268% (9428/19136)\n",
            "Loss: 1.408 | Acc: 49.271% (9460/19200)\n",
            "Loss: 1.408 | Acc: 49.263% (9490/19264)\n",
            "Loss: 1.407 | Acc: 49.250% (9519/19328)\n",
            "Loss: 1.407 | Acc: 49.263% (9553/19392)\n",
            "Loss: 1.407 | Acc: 49.265% (9585/19456)\n",
            "Loss: 1.407 | Acc: 49.288% (9621/19520)\n",
            "Loss: 1.406 | Acc: 49.311% (9657/19584)\n",
            "Loss: 1.406 | Acc: 49.282% (9683/19648)\n",
            "Loss: 1.406 | Acc: 49.310% (9720/19712)\n",
            "Loss: 1.405 | Acc: 49.353% (9760/19776)\n",
            "Loss: 1.405 | Acc: 49.330% (9787/19840)\n",
            "Loss: 1.405 | Acc: 49.327% (9818/19904)\n",
            "Loss: 1.405 | Acc: 49.284% (9841/19968)\n",
            "Loss: 1.405 | Acc: 49.306% (9877/20032)\n",
            "Loss: 1.405 | Acc: 49.318% (9911/20096)\n",
            "Loss: 1.404 | Acc: 49.335% (9946/20160)\n",
            "Loss: 1.404 | Acc: 49.352% (9981/20224)\n",
            "Loss: 1.404 | Acc: 49.310% (10004/20288)\n",
            "Loss: 1.404 | Acc: 49.302% (10034/20352)\n",
            "Loss: 1.405 | Acc: 49.260% (10057/20416)\n",
            "Loss: 1.404 | Acc: 49.258% (10088/20480)\n",
            "Loss: 1.405 | Acc: 49.216% (10111/20544)\n",
            "Loss: 1.406 | Acc: 49.180% (10135/20608)\n",
            "Loss: 1.405 | Acc: 49.192% (10169/20672)\n",
            "Loss: 1.405 | Acc: 49.190% (10200/20736)\n",
            "Loss: 1.405 | Acc: 49.212% (10236/20800)\n",
            "Loss: 1.405 | Acc: 49.209% (10267/20864)\n",
            "Loss: 1.405 | Acc: 49.207% (10298/20928)\n",
            "Loss: 1.405 | Acc: 49.200% (10328/20992)\n",
            "Loss: 1.406 | Acc: 49.178% (10355/21056)\n",
            "Loss: 1.406 | Acc: 49.167% (10384/21120)\n",
            "Loss: 1.405 | Acc: 49.179% (10418/21184)\n",
            "Loss: 1.405 | Acc: 49.176% (10449/21248)\n",
            "Loss: 1.405 | Acc: 49.169% (10479/21312)\n",
            "Loss: 1.405 | Acc: 49.181% (10513/21376)\n",
            "Loss: 1.406 | Acc: 49.179% (10544/21440)\n",
            "Loss: 1.405 | Acc: 49.177% (10575/21504)\n",
            "Loss: 1.406 | Acc: 49.152% (10601/21568)\n",
            "Loss: 1.405 | Acc: 49.168% (10636/21632)\n",
            "Loss: 1.405 | Acc: 49.157% (10665/21696)\n",
            "Loss: 1.405 | Acc: 49.187% (10703/21760)\n",
            "Loss: 1.405 | Acc: 49.166% (10730/21824)\n",
            "Loss: 1.405 | Acc: 49.182% (10765/21888)\n",
            "Loss: 1.405 | Acc: 49.212% (10803/21952)\n",
            "Loss: 1.404 | Acc: 49.214% (10835/22016)\n",
            "Loss: 1.403 | Acc: 49.226% (10869/22080)\n",
            "Loss: 1.403 | Acc: 49.214% (10898/22144)\n",
            "Loss: 1.403 | Acc: 49.244% (10936/22208)\n",
            "Loss: 1.402 | Acc: 49.259% (10971/22272)\n",
            "Loss: 1.403 | Acc: 49.261% (11003/22336)\n",
            "Loss: 1.402 | Acc: 49.277% (11038/22400)\n",
            "Loss: 1.402 | Acc: 49.279% (11070/22464)\n",
            "Loss: 1.402 | Acc: 49.268% (11099/22528)\n",
            "Loss: 1.402 | Acc: 49.314% (11141/22592)\n",
            "Loss: 1.402 | Acc: 49.298% (11169/22656)\n",
            "Loss: 1.401 | Acc: 49.305% (11202/22720)\n",
            "Loss: 1.401 | Acc: 49.311% (11235/22784)\n",
            "Loss: 1.401 | Acc: 49.287% (11261/22848)\n",
            "Loss: 1.402 | Acc: 49.284% (11292/22912)\n",
            "Loss: 1.401 | Acc: 49.321% (11332/22976)\n",
            "Loss: 1.400 | Acc: 49.345% (11369/23040)\n",
            "Loss: 1.399 | Acc: 49.377% (11408/23104)\n",
            "Loss: 1.399 | Acc: 49.383% (11441/23168)\n",
            "Loss: 1.399 | Acc: 49.380% (11472/23232)\n",
            "Loss: 1.399 | Acc: 49.378% (11503/23296)\n",
            "Loss: 1.400 | Acc: 49.358% (11530/23360)\n",
            "Loss: 1.399 | Acc: 49.355% (11561/23424)\n",
            "Loss: 1.400 | Acc: 49.357% (11593/23488)\n",
            "Loss: 1.400 | Acc: 49.350% (11623/23552)\n",
            "Loss: 1.400 | Acc: 49.306% (11644/23616)\n",
            "Loss: 1.400 | Acc: 49.312% (11677/23680)\n",
            "Loss: 1.400 | Acc: 49.339% (11715/23744)\n",
            "Loss: 1.400 | Acc: 49.332% (11745/23808)\n",
            "Loss: 1.399 | Acc: 49.367% (11785/23872)\n",
            "Loss: 1.398 | Acc: 49.378% (11819/23936)\n",
            "Loss: 1.398 | Acc: 49.388% (11853/24000)\n",
            "Loss: 1.398 | Acc: 49.368% (11880/24064)\n",
            "Loss: 1.398 | Acc: 49.353% (11908/24128)\n",
            "Loss: 1.398 | Acc: 49.359% (11941/24192)\n",
            "Loss: 1.398 | Acc: 49.390% (11980/24256)\n",
            "Loss: 1.398 | Acc: 49.387% (12011/24320)\n",
            "Loss: 1.398 | Acc: 49.418% (12050/24384)\n",
            "Loss: 1.399 | Acc: 49.415% (12081/24448)\n",
            "Loss: 1.398 | Acc: 49.413% (12112/24512)\n",
            "Loss: 1.398 | Acc: 49.426% (12147/24576)\n",
            "Loss: 1.398 | Acc: 49.420% (12177/24640)\n",
            "Loss: 1.398 | Acc: 49.381% (12199/24704)\n",
            "Loss: 1.398 | Acc: 49.402% (12236/24768)\n",
            "Loss: 1.398 | Acc: 49.412% (12270/24832)\n",
            "Loss: 1.397 | Acc: 49.414% (12302/24896)\n",
            "Loss: 1.397 | Acc: 49.407% (12332/24960)\n",
            "Loss: 1.397 | Acc: 49.385% (12358/25024)\n",
            "Loss: 1.397 | Acc: 49.394% (12392/25088)\n",
            "Loss: 1.396 | Acc: 49.416% (12429/25152)\n",
            "Loss: 1.396 | Acc: 49.425% (12463/25216)\n",
            "Loss: 1.396 | Acc: 49.438% (12498/25280)\n",
            "Loss: 1.396 | Acc: 49.448% (12532/25344)\n",
            "Loss: 1.396 | Acc: 49.429% (12559/25408)\n",
            "Loss: 1.397 | Acc: 49.419% (12588/25472)\n",
            "Loss: 1.397 | Acc: 49.417% (12619/25536)\n",
            "Loss: 1.397 | Acc: 49.398% (12646/25600)\n",
            "Loss: 1.397 | Acc: 49.412% (12681/25664)\n",
            "Loss: 1.397 | Acc: 49.398% (12709/25728)\n",
            "Loss: 1.398 | Acc: 49.372% (12734/25792)\n",
            "Loss: 1.397 | Acc: 49.381% (12768/25856)\n",
            "Loss: 1.397 | Acc: 49.379% (12799/25920)\n",
            "Loss: 1.397 | Acc: 49.373% (12829/25984)\n",
            "Loss: 1.397 | Acc: 49.359% (12857/26048)\n",
            "Loss: 1.397 | Acc: 49.357% (12888/26112)\n",
            "Loss: 1.397 | Acc: 49.377% (12925/26176)\n",
            "Loss: 1.397 | Acc: 49.390% (12960/26240)\n",
            "Loss: 1.396 | Acc: 49.434% (13003/26304)\n",
            "Loss: 1.396 | Acc: 49.439% (13036/26368)\n",
            "Loss: 1.396 | Acc: 49.467% (13075/26432)\n",
            "Loss: 1.395 | Acc: 49.494% (13114/26496)\n",
            "Loss: 1.395 | Acc: 49.477% (13141/26560)\n",
            "Loss: 1.396 | Acc: 49.444% (13164/26624)\n",
            "Loss: 1.396 | Acc: 49.475% (13204/26688)\n",
            "Loss: 1.395 | Acc: 49.507% (13244/26752)\n",
            "Loss: 1.395 | Acc: 49.511% (13277/26816)\n",
            "Loss: 1.395 | Acc: 49.494% (13304/26880)\n",
            "Loss: 1.395 | Acc: 49.480% (13332/26944)\n",
            "Loss: 1.395 | Acc: 49.493% (13367/27008)\n",
            "Loss: 1.395 | Acc: 49.490% (13398/27072)\n",
            "Loss: 1.395 | Acc: 49.484% (13428/27136)\n",
            "Loss: 1.395 | Acc: 49.507% (13466/27200)\n",
            "Loss: 1.395 | Acc: 49.498% (13495/27264)\n",
            "Loss: 1.395 | Acc: 49.513% (13531/27328)\n",
            "Loss: 1.395 | Acc: 49.511% (13562/27392)\n",
            "Loss: 1.396 | Acc: 49.501% (13591/27456)\n",
            "Loss: 1.396 | Acc: 49.513% (13626/27520)\n",
            "Loss: 1.396 | Acc: 49.525% (13661/27584)\n",
            "Loss: 1.396 | Acc: 49.533% (13695/27648)\n",
            "Loss: 1.395 | Acc: 49.556% (13733/27712)\n",
            "Loss: 1.395 | Acc: 49.546% (13762/27776)\n",
            "Loss: 1.396 | Acc: 49.515% (13785/27840)\n",
            "Loss: 1.395 | Acc: 49.516% (13817/27904)\n",
            "Loss: 1.395 | Acc: 49.521% (13850/27968)\n",
            "Loss: 1.395 | Acc: 49.526% (13883/28032)\n",
            "Loss: 1.395 | Acc: 49.516% (13912/28096)\n",
            "Loss: 1.395 | Acc: 49.517% (13944/28160)\n",
            "Loss: 1.395 | Acc: 49.518% (13976/28224)\n",
            "Loss: 1.394 | Acc: 49.530% (14011/28288)\n",
            "Loss: 1.395 | Acc: 49.513% (14038/28352)\n",
            "Loss: 1.394 | Acc: 49.525% (14073/28416)\n",
            "Loss: 1.394 | Acc: 49.544% (14110/28480)\n",
            "Loss: 1.394 | Acc: 49.562% (14147/28544)\n",
            "Loss: 1.393 | Acc: 49.577% (14183/28608)\n",
            "Loss: 1.393 | Acc: 49.581% (14216/28672)\n",
            "Loss: 1.393 | Acc: 49.586% (14249/28736)\n",
            "Loss: 1.393 | Acc: 49.590% (14282/28800)\n",
            "Loss: 1.393 | Acc: 49.581% (14311/28864)\n",
            "Loss: 1.393 | Acc: 49.575% (14341/28928)\n",
            "Loss: 1.394 | Acc: 49.555% (14367/28992)\n",
            "Loss: 1.393 | Acc: 49.573% (14404/29056)\n",
            "Loss: 1.393 | Acc: 49.571% (14435/29120)\n",
            "Loss: 1.393 | Acc: 49.565% (14465/29184)\n",
            "Loss: 1.393 | Acc: 49.576% (14500/29248)\n",
            "Loss: 1.393 | Acc: 49.577% (14532/29312)\n",
            "Loss: 1.393 | Acc: 49.571% (14562/29376)\n",
            "Loss: 1.393 | Acc: 49.586% (14598/29440)\n",
            "Loss: 1.393 | Acc: 49.600% (14634/29504)\n",
            "Loss: 1.393 | Acc: 49.604% (14667/29568)\n",
            "Loss: 1.393 | Acc: 49.595% (14696/29632)\n",
            "Loss: 1.393 | Acc: 49.599% (14729/29696)\n",
            "Loss: 1.393 | Acc: 49.593% (14759/29760)\n",
            "Loss: 1.393 | Acc: 49.591% (14790/29824)\n",
            "Loss: 1.393 | Acc: 49.599% (14824/29888)\n",
            "Loss: 1.393 | Acc: 49.583% (14851/29952)\n",
            "Loss: 1.392 | Acc: 49.590% (14885/30016)\n",
            "Loss: 1.392 | Acc: 49.604% (14921/30080)\n",
            "Loss: 1.392 | Acc: 49.615% (14956/30144)\n",
            "Loss: 1.392 | Acc: 49.613% (14987/30208)\n",
            "Loss: 1.392 | Acc: 49.607% (15017/30272)\n",
            "Loss: 1.392 | Acc: 49.604% (15048/30336)\n",
            "Loss: 1.392 | Acc: 49.599% (15078/30400)\n",
            "Loss: 1.392 | Acc: 49.626% (15118/30464)\n",
            "Loss: 1.392 | Acc: 49.617% (15147/30528)\n",
            "Loss: 1.392 | Acc: 49.640% (15186/30592)\n",
            "Loss: 1.392 | Acc: 49.644% (15219/30656)\n",
            "Loss: 1.392 | Acc: 49.645% (15251/30720)\n",
            "Loss: 1.392 | Acc: 49.646% (15283/30784)\n",
            "Loss: 1.392 | Acc: 49.637% (15312/30848)\n",
            "Loss: 1.393 | Acc: 49.625% (15340/30912)\n",
            "Loss: 1.393 | Acc: 49.622% (15371/30976)\n",
            "Loss: 1.393 | Acc: 49.620% (15402/31040)\n",
            "Loss: 1.393 | Acc: 49.633% (15438/31104)\n",
            "Loss: 1.392 | Acc: 49.657% (15477/31168)\n",
            "Loss: 1.393 | Acc: 49.635% (15502/31232)\n",
            "Loss: 1.393 | Acc: 49.610% (15526/31296)\n",
            "Loss: 1.393 | Acc: 49.608% (15557/31360)\n",
            "Loss: 1.393 | Acc: 49.589% (15583/31424)\n",
            "Loss: 1.393 | Acc: 49.616% (15623/31488)\n",
            "Loss: 1.392 | Acc: 49.629% (15659/31552)\n",
            "Loss: 1.393 | Acc: 49.605% (15683/31616)\n",
            "Loss: 1.393 | Acc: 49.596% (15712/31680)\n",
            "Loss: 1.393 | Acc: 49.609% (15748/31744)\n",
            "Loss: 1.392 | Acc: 49.626% (15785/31808)\n",
            "Loss: 1.392 | Acc: 49.645% (15823/31872)\n",
            "Loss: 1.392 | Acc: 49.649% (15856/31936)\n",
            "Loss: 1.392 | Acc: 49.656% (15890/32000)\n",
            "Loss: 1.392 | Acc: 49.654% (15921/32064)\n",
            "Loss: 1.392 | Acc: 49.667% (15957/32128)\n",
            "Loss: 1.392 | Acc: 49.652% (15984/32192)\n",
            "Loss: 1.392 | Acc: 49.662% (16019/32256)\n",
            "Loss: 1.392 | Acc: 49.653% (16048/32320)\n",
            "Loss: 1.392 | Acc: 49.648% (16078/32384)\n",
            "Loss: 1.392 | Acc: 49.646% (16109/32448)\n",
            "Loss: 1.393 | Acc: 49.625% (16134/32512)\n",
            "Loss: 1.393 | Acc: 49.610% (16161/32576)\n",
            "Loss: 1.393 | Acc: 49.629% (16199/32640)\n",
            "Loss: 1.393 | Acc: 49.612% (16225/32704)\n",
            "Loss: 1.393 | Acc: 49.615% (16258/32768)\n",
            "Loss: 1.393 | Acc: 49.613% (16289/32832)\n",
            "Loss: 1.393 | Acc: 49.611% (16320/32896)\n",
            "Loss: 1.393 | Acc: 49.606% (16350/32960)\n",
            "Loss: 1.393 | Acc: 49.591% (16377/33024)\n",
            "Loss: 1.394 | Acc: 49.577% (16404/33088)\n",
            "Loss: 1.394 | Acc: 49.575% (16435/33152)\n",
            "Loss: 1.394 | Acc: 49.582% (16469/33216)\n",
            "Loss: 1.393 | Acc: 49.582% (16501/33280)\n",
            "Loss: 1.394 | Acc: 49.571% (16529/33344)\n",
            "Loss: 1.394 | Acc: 49.557% (16556/33408)\n",
            "Loss: 1.394 | Acc: 49.567% (16591/33472)\n",
            "Loss: 1.394 | Acc: 49.568% (16623/33536)\n",
            "Loss: 1.394 | Acc: 49.551% (16649/33600)\n",
            "Loss: 1.394 | Acc: 49.522% (16671/33664)\n",
            "Loss: 1.394 | Acc: 49.526% (16704/33728)\n",
            "Loss: 1.394 | Acc: 49.524% (16735/33792)\n",
            "Loss: 1.394 | Acc: 49.519% (16765/33856)\n",
            "Loss: 1.394 | Acc: 49.517% (16796/33920)\n",
            "Loss: 1.394 | Acc: 49.512% (16826/33984)\n",
            "Loss: 1.394 | Acc: 49.501% (16854/34048)\n",
            "Loss: 1.394 | Acc: 49.493% (16883/34112)\n",
            "Loss: 1.394 | Acc: 49.488% (16913/34176)\n",
            "Loss: 1.394 | Acc: 49.489% (16945/34240)\n",
            "Loss: 1.394 | Acc: 49.484% (16975/34304)\n",
            "Loss: 1.394 | Acc: 49.497% (17011/34368)\n",
            "Loss: 1.394 | Acc: 49.506% (17046/34432)\n",
            "Loss: 1.394 | Acc: 49.507% (17078/34496)\n",
            "Loss: 1.394 | Acc: 49.517% (17113/34560)\n",
            "Loss: 1.394 | Acc: 49.518% (17145/34624)\n",
            "Loss: 1.394 | Acc: 49.527% (17180/34688)\n",
            "Loss: 1.393 | Acc: 49.545% (17218/34752)\n",
            "Loss: 1.394 | Acc: 49.535% (17246/34816)\n",
            "Loss: 1.394 | Acc: 49.521% (17273/34880)\n",
            "Loss: 1.394 | Acc: 49.516% (17303/34944)\n",
            "Loss: 1.394 | Acc: 49.526% (17338/35008)\n",
            "Loss: 1.394 | Acc: 49.521% (17368/35072)\n",
            "Loss: 1.395 | Acc: 49.496% (17391/35136)\n",
            "Loss: 1.395 | Acc: 49.469% (17413/35200)\n",
            "Loss: 1.395 | Acc: 49.458% (17441/35264)\n",
            "Loss: 1.395 | Acc: 49.454% (17471/35328)\n",
            "Loss: 1.395 | Acc: 49.449% (17501/35392)\n",
            "Loss: 1.395 | Acc: 49.456% (17535/35456)\n",
            "Loss: 1.395 | Acc: 49.454% (17566/35520)\n",
            "Loss: 1.395 | Acc: 49.435% (17591/35584)\n",
            "Loss: 1.395 | Acc: 49.439% (17624/35648)\n",
            "Loss: 1.394 | Acc: 49.440% (17656/35712)\n",
            "Loss: 1.395 | Acc: 49.416% (17679/35776)\n",
            "Loss: 1.395 | Acc: 49.408% (17708/35840)\n",
            "Loss: 1.395 | Acc: 49.401% (17737/35904)\n",
            "Loss: 1.395 | Acc: 49.411% (17772/35968)\n",
            "Loss: 1.395 | Acc: 49.423% (17808/36032)\n",
            "Loss: 1.395 | Acc: 49.438% (17845/36096)\n",
            "Loss: 1.395 | Acc: 49.425% (17872/36160)\n",
            "Loss: 1.394 | Acc: 49.434% (17907/36224)\n",
            "Loss: 1.394 | Acc: 49.446% (17943/36288)\n",
            "Loss: 1.394 | Acc: 49.453% (17977/36352)\n",
            "Loss: 1.394 | Acc: 49.448% (18007/36416)\n",
            "Loss: 1.395 | Acc: 49.438% (18035/36480)\n",
            "Loss: 1.395 | Acc: 49.420% (18060/36544)\n",
            "Loss: 1.396 | Acc: 49.407% (18087/36608)\n",
            "Loss: 1.395 | Acc: 49.406% (18118/36672)\n",
            "Loss: 1.396 | Acc: 49.398% (18147/36736)\n",
            "Loss: 1.395 | Acc: 49.418% (18186/36800)\n",
            "Loss: 1.395 | Acc: 49.417% (18217/36864)\n",
            "Loss: 1.396 | Acc: 49.412% (18247/36928)\n",
            "Loss: 1.396 | Acc: 49.408% (18277/36992)\n",
            "Loss: 1.396 | Acc: 49.404% (18307/37056)\n",
            "Loss: 1.396 | Acc: 49.388% (18333/37120)\n",
            "Loss: 1.396 | Acc: 49.381% (18362/37184)\n",
            "Loss: 1.396 | Acc: 49.396% (18399/37248)\n",
            "Loss: 1.396 | Acc: 49.389% (18428/37312)\n",
            "Loss: 1.396 | Acc: 49.395% (18462/37376)\n",
            "Loss: 1.396 | Acc: 49.418% (18502/37440)\n",
            "Loss: 1.395 | Acc: 49.437% (18541/37504)\n",
            "Loss: 1.395 | Acc: 49.433% (18571/37568)\n",
            "Loss: 1.395 | Acc: 49.439% (18605/37632)\n",
            "Loss: 1.395 | Acc: 49.443% (18638/37696)\n",
            "Loss: 1.395 | Acc: 49.476% (18682/37760)\n",
            "Loss: 1.395 | Acc: 49.469% (18711/37824)\n",
            "Loss: 1.395 | Acc: 49.464% (18741/37888)\n",
            "Loss: 1.395 | Acc: 49.460% (18771/37952)\n",
            "Loss: 1.395 | Acc: 49.434% (18793/38016)\n",
            "Loss: 1.395 | Acc: 49.443% (18828/38080)\n",
            "Loss: 1.395 | Acc: 49.457% (18865/38144)\n",
            "Loss: 1.395 | Acc: 49.443% (18891/38208)\n",
            "Loss: 1.395 | Acc: 49.436% (18920/38272)\n",
            "Loss: 1.395 | Acc: 49.431% (18950/38336)\n",
            "Loss: 1.395 | Acc: 49.430% (18981/38400)\n",
            "Loss: 1.395 | Acc: 49.428% (19012/38464)\n",
            "Loss: 1.394 | Acc: 49.437% (19047/38528)\n",
            "Loss: 1.394 | Acc: 49.440% (19080/38592)\n",
            "Loss: 1.394 | Acc: 49.436% (19110/38656)\n",
            "Loss: 1.394 | Acc: 49.411% (19132/38720)\n",
            "Loss: 1.394 | Acc: 49.417% (19166/38784)\n",
            "Loss: 1.394 | Acc: 49.439% (19206/38848)\n",
            "Loss: 1.394 | Acc: 49.435% (19236/38912)\n",
            "Loss: 1.394 | Acc: 49.430% (19266/38976)\n",
            "Loss: 1.394 | Acc: 49.416% (19292/39040)\n",
            "Loss: 1.394 | Acc: 49.435% (19331/39104)\n",
            "Loss: 1.394 | Acc: 49.433% (19362/39168)\n",
            "Loss: 1.394 | Acc: 49.437% (19395/39232)\n",
            "Loss: 1.394 | Acc: 49.425% (19422/39296)\n",
            "Loss: 1.394 | Acc: 49.421% (19452/39360)\n",
            "Loss: 1.394 | Acc: 49.429% (19487/39424)\n",
            "Loss: 1.394 | Acc: 49.428% (19518/39488)\n",
            "Loss: 1.395 | Acc: 49.421% (19547/39552)\n",
            "Loss: 1.395 | Acc: 49.427% (19581/39616)\n",
            "Loss: 1.394 | Acc: 49.446% (19620/39680)\n",
            "Loss: 1.394 | Acc: 49.454% (19655/39744)\n",
            "Loss: 1.394 | Acc: 49.442% (19682/39808)\n",
            "Loss: 1.394 | Acc: 49.441% (19713/39872)\n",
            "Loss: 1.394 | Acc: 49.444% (19746/39936)\n",
            "Loss: 1.394 | Acc: 49.430% (19772/40000)\n",
            "Loss: 1.394 | Acc: 49.438% (19807/40064)\n",
            "Loss: 1.394 | Acc: 49.439% (19839/40128)\n",
            "Loss: 1.394 | Acc: 49.440% (19871/40192)\n",
            "Loss: 1.394 | Acc: 49.456% (19909/40256)\n",
            "Loss: 1.393 | Acc: 49.457% (19941/40320)\n",
            "Loss: 1.393 | Acc: 49.458% (19973/40384)\n",
            "Loss: 1.393 | Acc: 49.481% (20014/40448)\n",
            "Loss: 1.393 | Acc: 49.474% (20043/40512)\n",
            "Loss: 1.393 | Acc: 49.480% (20077/40576)\n",
            "Loss: 1.393 | Acc: 49.476% (20107/40640)\n",
            "Loss: 1.393 | Acc: 49.484% (20142/40704)\n",
            "Loss: 1.393 | Acc: 49.487% (20175/40768)\n",
            "Loss: 1.393 | Acc: 49.476% (20202/40832)\n",
            "Loss: 1.393 | Acc: 49.467% (20230/40896)\n",
            "Loss: 1.393 | Acc: 49.443% (20252/40960)\n",
            "Loss: 1.394 | Acc: 49.442% (20283/41024)\n",
            "Loss: 1.394 | Acc: 49.426% (20308/41088)\n",
            "Loss: 1.394 | Acc: 49.417% (20336/41152)\n",
            "Loss: 1.394 | Acc: 49.423% (20370/41216)\n",
            "Loss: 1.394 | Acc: 49.411% (20397/41280)\n",
            "Loss: 1.394 | Acc: 49.429% (20436/41344)\n",
            "Loss: 1.394 | Acc: 49.430% (20468/41408)\n",
            "Loss: 1.394 | Acc: 49.424% (20497/41472)\n",
            "Loss: 1.394 | Acc: 49.413% (20524/41536)\n",
            "Loss: 1.394 | Acc: 49.418% (20558/41600)\n",
            "Loss: 1.394 | Acc: 49.426% (20593/41664)\n",
            "Loss: 1.394 | Acc: 49.425% (20624/41728)\n",
            "Loss: 1.393 | Acc: 49.438% (20661/41792)\n",
            "Loss: 1.394 | Acc: 49.441% (20694/41856)\n",
            "Loss: 1.394 | Acc: 49.437% (20724/41920)\n",
            "Loss: 1.393 | Acc: 49.450% (20761/41984)\n",
            "Loss: 1.393 | Acc: 49.453% (20794/42048)\n",
            "Loss: 1.393 | Acc: 49.454% (20826/42112)\n",
            "Loss: 1.393 | Acc: 49.459% (20860/42176)\n",
            "Loss: 1.393 | Acc: 49.479% (20900/42240)\n",
            "Loss: 1.393 | Acc: 49.468% (20927/42304)\n",
            "Loss: 1.393 | Acc: 49.467% (20958/42368)\n",
            "Loss: 1.393 | Acc: 49.456% (20985/42432)\n",
            "Loss: 1.393 | Acc: 49.438% (21009/42496)\n",
            "Loss: 1.393 | Acc: 49.438% (21041/42560)\n",
            "Loss: 1.393 | Acc: 49.458% (21081/42624)\n",
            "Loss: 1.393 | Acc: 49.454% (21111/42688)\n",
            "Loss: 1.393 | Acc: 49.443% (21138/42752)\n",
            "Loss: 1.394 | Acc: 49.437% (21167/42816)\n",
            "Loss: 1.394 | Acc: 49.415% (21189/42880)\n",
            "Loss: 1.394 | Acc: 49.418% (21222/42944)\n",
            "Loss: 1.394 | Acc: 49.407% (21249/43008)\n",
            "Loss: 1.394 | Acc: 49.420% (21286/43072)\n",
            "Loss: 1.394 | Acc: 49.420% (21318/43136)\n",
            "Loss: 1.394 | Acc: 49.421% (21350/43200)\n",
            "Loss: 1.394 | Acc: 49.418% (21380/43264)\n",
            "Loss: 1.394 | Acc: 49.416% (21411/43328)\n",
            "Loss: 1.394 | Acc: 49.417% (21443/43392)\n",
            "Loss: 1.394 | Acc: 49.397% (21466/43456)\n",
            "Loss: 1.394 | Acc: 49.393% (21496/43520)\n",
            "Loss: 1.394 | Acc: 49.392% (21527/43584)\n",
            "Loss: 1.394 | Acc: 49.391% (21558/43648)\n",
            "Loss: 1.394 | Acc: 49.387% (21588/43712)\n",
            "Loss: 1.394 | Acc: 49.383% (21618/43776)\n",
            "Loss: 1.394 | Acc: 49.405% (21659/43840)\n",
            "Loss: 1.394 | Acc: 49.412% (21694/43904)\n",
            "Loss: 1.394 | Acc: 49.402% (21721/43968)\n",
            "Loss: 1.394 | Acc: 49.398% (21751/44032)\n",
            "Loss: 1.394 | Acc: 49.392% (21780/44096)\n",
            "Loss: 1.394 | Acc: 49.407% (21818/44160)\n",
            "Loss: 1.394 | Acc: 49.423% (21857/44224)\n",
            "Loss: 1.394 | Acc: 49.433% (21893/44288)\n",
            "Loss: 1.393 | Acc: 49.439% (21927/44352)\n",
            "Loss: 1.394 | Acc: 49.426% (21953/44416)\n",
            "Loss: 1.394 | Acc: 49.420% (21982/44480)\n",
            "Loss: 1.393 | Acc: 49.428% (22017/44544)\n",
            "Loss: 1.393 | Acc: 49.435% (22052/44608)\n",
            "Loss: 1.393 | Acc: 49.434% (22083/44672)\n",
            "Loss: 1.393 | Acc: 49.441% (22118/44736)\n",
            "Loss: 1.393 | Acc: 49.446% (22152/44800)\n",
            "Loss: 1.393 | Acc: 49.452% (22186/44864)\n",
            "Loss: 1.393 | Acc: 49.448% (22216/44928)\n",
            "Loss: 1.393 | Acc: 49.455% (22251/44992)\n",
            "Loss: 1.393 | Acc: 49.461% (22285/45056)\n",
            "Loss: 1.393 | Acc: 49.441% (22308/45120)\n",
            "Loss: 1.393 | Acc: 49.458% (22347/45184)\n",
            "Loss: 1.393 | Acc: 49.465% (22382/45248)\n",
            "Loss: 1.394 | Acc: 49.457% (22410/45312)\n",
            "Loss: 1.394 | Acc: 49.449% (22438/45376)\n",
            "Loss: 1.393 | Acc: 49.470% (22479/45440)\n",
            "Loss: 1.393 | Acc: 49.470% (22511/45504)\n",
            "Loss: 1.393 | Acc: 49.471% (22543/45568)\n",
            "Loss: 1.393 | Acc: 49.481% (22579/45632)\n",
            "Loss: 1.393 | Acc: 49.477% (22609/45696)\n",
            "Loss: 1.393 | Acc: 49.471% (22638/45760)\n",
            "Loss: 1.394 | Acc: 49.476% (22672/45824)\n",
            "Loss: 1.393 | Acc: 49.490% (22710/45888)\n",
            "Loss: 1.393 | Acc: 49.489% (22741/45952)\n",
            "Loss: 1.393 | Acc: 49.502% (22779/46016)\n",
            "Loss: 1.393 | Acc: 49.505% (22812/46080)\n",
            "Loss: 1.393 | Acc: 49.515% (22848/46144)\n",
            "Loss: 1.393 | Acc: 49.520% (22882/46208)\n",
            "Loss: 1.393 | Acc: 49.520% (22914/46272)\n",
            "Loss: 1.393 | Acc: 49.512% (22942/46336)\n",
            "Loss: 1.393 | Acc: 49.511% (22973/46400)\n",
            "Loss: 1.393 | Acc: 49.518% (23008/46464)\n",
            "Loss: 1.393 | Acc: 49.525% (23043/46528)\n",
            "Loss: 1.392 | Acc: 49.526% (23075/46592)\n",
            "Loss: 1.393 | Acc: 49.524% (23106/46656)\n",
            "Loss: 1.393 | Acc: 49.527% (23139/46720)\n",
            "Loss: 1.393 | Acc: 49.538% (23176/46784)\n",
            "Loss: 1.393 | Acc: 49.533% (23205/46848)\n",
            "Loss: 1.393 | Acc: 49.533% (23237/46912)\n",
            "Loss: 1.393 | Acc: 49.542% (23273/46976)\n",
            "Loss: 1.393 | Acc: 49.554% (23310/47040)\n",
            "Loss: 1.392 | Acc: 49.552% (23341/47104)\n",
            "Loss: 1.393 | Acc: 49.542% (23368/47168)\n",
            "Loss: 1.393 | Acc: 49.538% (23398/47232)\n",
            "Loss: 1.392 | Acc: 49.554% (23437/47296)\n",
            "Loss: 1.392 | Acc: 49.563% (23473/47360)\n",
            "Loss: 1.392 | Acc: 49.553% (23500/47424)\n",
            "Loss: 1.392 | Acc: 49.545% (23528/47488)\n",
            "Loss: 1.392 | Acc: 49.550% (23562/47552)\n",
            "Loss: 1.392 | Acc: 49.546% (23592/47616)\n",
            "Loss: 1.392 | Acc: 49.551% (23626/47680)\n",
            "Loss: 1.392 | Acc: 49.556% (23660/47744)\n",
            "Loss: 1.392 | Acc: 49.557% (23692/47808)\n",
            "Loss: 1.392 | Acc: 49.551% (23721/47872)\n",
            "Loss: 1.392 | Acc: 49.560% (23757/47936)\n",
            "Loss: 1.392 | Acc: 49.556% (23787/48000)\n",
            "Loss: 1.392 | Acc: 49.561% (23821/48064)\n",
            "Loss: 1.391 | Acc: 49.574% (23859/48128)\n",
            "Loss: 1.392 | Acc: 49.568% (23888/48192)\n",
            "Loss: 1.392 | Acc: 49.569% (23920/48256)\n",
            "Loss: 1.392 | Acc: 49.574% (23954/48320)\n",
            "Loss: 1.392 | Acc: 49.574% (23986/48384)\n",
            "Loss: 1.391 | Acc: 49.583% (24022/48448)\n",
            "Loss: 1.391 | Acc: 49.582% (24053/48512)\n",
            "Loss: 1.391 | Acc: 49.566% (24077/48576)\n",
            "Loss: 1.391 | Acc: 49.562% (24107/48640)\n",
            "Loss: 1.391 | Acc: 49.571% (24143/48704)\n",
            "Loss: 1.391 | Acc: 49.582% (24180/48768)\n",
            "Loss: 1.390 | Acc: 49.586% (24214/48832)\n",
            "Loss: 1.391 | Acc: 49.593% (24249/48896)\n",
            "Loss: 1.390 | Acc: 49.602% (24285/48960)\n",
            "Loss: 1.391 | Acc: 49.598% (24303/49000)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 49.59795918367347\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.176 | Acc: 56.250% (36/64)\n",
            "Loss: 1.197 | Acc: 53.125% (68/128)\n",
            "Loss: 1.308 | Acc: 49.479% (95/192)\n",
            "Loss: 1.362 | Acc: 49.609% (127/256)\n",
            "Loss: 1.306 | Acc: 51.250% (164/320)\n",
            "Loss: 1.353 | Acc: 50.260% (193/384)\n",
            "Loss: 1.378 | Acc: 47.768% (214/448)\n",
            "Loss: 1.375 | Acc: 47.852% (245/512)\n",
            "Loss: 1.357 | Acc: 48.958% (282/576)\n",
            "Loss: 1.352 | Acc: 49.531% (317/640)\n",
            "Loss: 1.372 | Acc: 49.148% (346/704)\n",
            "Loss: 1.380 | Acc: 49.089% (377/768)\n",
            "Loss: 1.378 | Acc: 49.159% (409/832)\n",
            "Loss: 1.374 | Acc: 49.665% (445/896)\n",
            "Loss: 1.358 | Acc: 49.792% (478/960)\n",
            "Loss: 1.352 | Acc: 50.293% (515/1024)\n",
            "Loss: 1.355 | Acc: 49.908% (543/1088)\n",
            "Loss: 1.351 | Acc: 49.653% (572/1152)\n",
            "Loss: 1.347 | Acc: 49.836% (606/1216)\n",
            "Loss: 1.364 | Acc: 49.297% (631/1280)\n",
            "Loss: 1.366 | Acc: 48.810% (656/1344)\n",
            "Loss: 1.370 | Acc: 48.935% (689/1408)\n",
            "Loss: 1.361 | Acc: 49.117% (723/1472)\n",
            "Loss: 1.360 | Acc: 49.414% (759/1536)\n",
            "Loss: 1.367 | Acc: 49.375% (790/1600)\n",
            "Loss: 1.372 | Acc: 49.459% (823/1664)\n",
            "Loss: 1.373 | Acc: 49.479% (855/1728)\n",
            "Loss: 1.371 | Acc: 49.721% (891/1792)\n",
            "Loss: 1.369 | Acc: 49.946% (927/1856)\n",
            "Loss: 1.376 | Acc: 49.740% (955/1920)\n",
            "Loss: 1.375 | Acc: 49.748% (987/1984)\n",
            "Loss: 1.375 | Acc: 50.000% (1024/2048)\n",
            "Loss: 1.366 | Acc: 50.331% (1063/2112)\n",
            "Loss: 1.367 | Acc: 50.276% (1094/2176)\n",
            "Loss: 1.365 | Acc: 50.357% (1128/2240)\n",
            "Loss: 1.362 | Acc: 50.260% (1158/2304)\n",
            "Loss: 1.363 | Acc: 50.253% (1190/2368)\n",
            "Loss: 1.362 | Acc: 50.329% (1224/2432)\n",
            "Loss: 1.365 | Acc: 50.361% (1257/2496)\n",
            "Loss: 1.369 | Acc: 50.195% (1285/2560)\n",
            "Loss: 1.372 | Acc: 50.038% (1313/2624)\n",
            "Loss: 1.370 | Acc: 50.186% (1349/2688)\n",
            "Loss: 1.369 | Acc: 50.145% (1380/2752)\n",
            "Loss: 1.374 | Acc: 49.964% (1407/2816)\n",
            "Loss: 1.375 | Acc: 50.069% (1442/2880)\n",
            "Loss: 1.370 | Acc: 50.272% (1480/2944)\n",
            "Loss: 1.369 | Acc: 50.432% (1517/3008)\n",
            "Loss: 1.368 | Acc: 50.586% (1554/3072)\n",
            "Loss: 1.368 | Acc: 50.415% (1581/3136)\n",
            "Loss: 1.367 | Acc: 50.406% (1613/3200)\n",
            "Loss: 1.367 | Acc: 50.337% (1643/3264)\n",
            "Loss: 1.367 | Acc: 50.391% (1677/3328)\n",
            "Loss: 1.367 | Acc: 50.413% (1710/3392)\n",
            "Loss: 1.366 | Acc: 50.463% (1744/3456)\n",
            "Loss: 1.366 | Acc: 50.398% (1774/3520)\n",
            "Loss: 1.362 | Acc: 50.642% (1815/3584)\n",
            "Loss: 1.363 | Acc: 50.685% (1849/3648)\n",
            "Loss: 1.361 | Acc: 50.889% (1889/3712)\n",
            "Loss: 1.362 | Acc: 50.900% (1922/3776)\n",
            "Loss: 1.364 | Acc: 50.911% (1955/3840)\n",
            "Loss: 1.365 | Acc: 50.922% (1988/3904)\n",
            "Loss: 1.365 | Acc: 50.932% (2021/3968)\n",
            "Loss: 1.367 | Acc: 50.967% (2055/4032)\n",
            "Loss: 1.369 | Acc: 50.928% (2086/4096)\n",
            "Loss: 1.369 | Acc: 51.010% (2122/4160)\n",
            "Loss: 1.368 | Acc: 51.160% (2161/4224)\n",
            "Loss: 1.368 | Acc: 51.143% (2193/4288)\n",
            "Loss: 1.370 | Acc: 51.103% (2224/4352)\n",
            "Loss: 1.366 | Acc: 51.223% (2262/4416)\n",
            "Loss: 1.365 | Acc: 51.250% (2296/4480)\n",
            "Loss: 1.364 | Acc: 51.298% (2331/4544)\n",
            "Loss: 1.367 | Acc: 51.128% (2356/4608)\n",
            "Loss: 1.365 | Acc: 51.177% (2391/4672)\n",
            "Loss: 1.365 | Acc: 51.161% (2423/4736)\n",
            "Loss: 1.370 | Acc: 51.167% (2456/4800)\n",
            "Loss: 1.369 | Acc: 51.254% (2493/4864)\n",
            "Loss: 1.369 | Acc: 51.278% (2527/4928)\n",
            "Loss: 1.371 | Acc: 51.182% (2555/4992)\n",
            "Loss: 1.371 | Acc: 51.206% (2589/5056)\n",
            "Loss: 1.373 | Acc: 51.074% (2615/5120)\n",
            "Loss: 1.374 | Acc: 51.061% (2647/5184)\n",
            "Loss: 1.376 | Acc: 50.991% (2676/5248)\n",
            "Loss: 1.375 | Acc: 50.998% (2709/5312)\n",
            "Loss: 1.377 | Acc: 50.967% (2740/5376)\n",
            "Loss: 1.378 | Acc: 50.993% (2774/5440)\n",
            "Loss: 1.380 | Acc: 50.981% (2806/5504)\n",
            "Loss: 1.384 | Acc: 50.916% (2835/5568)\n",
            "Loss: 1.388 | Acc: 50.888% (2866/5632)\n",
            "Loss: 1.388 | Acc: 50.860% (2897/5696)\n",
            "Loss: 1.385 | Acc: 50.972% (2936/5760)\n",
            "Loss: 1.384 | Acc: 50.996% (2970/5824)\n",
            "Loss: 1.387 | Acc: 50.900% (2997/5888)\n",
            "Loss: 1.388 | Acc: 50.823% (3025/5952)\n",
            "Loss: 1.386 | Acc: 50.831% (3058/6016)\n",
            "Loss: 1.385 | Acc: 50.806% (3089/6080)\n",
            "Loss: 1.385 | Acc: 50.846% (3124/6144)\n",
            "Loss: 1.385 | Acc: 50.822% (3155/6208)\n",
            "Loss: 1.387 | Acc: 50.797% (3186/6272)\n",
            "Loss: 1.386 | Acc: 50.868% (3223/6336)\n",
            "Loss: 1.384 | Acc: 50.859% (3255/6400)\n",
            "Loss: 1.385 | Acc: 50.804% (3284/6464)\n",
            "Loss: 1.383 | Acc: 50.843% (3319/6528)\n",
            "Loss: 1.382 | Acc: 50.865% (3353/6592)\n",
            "Loss: 1.380 | Acc: 50.962% (3392/6656)\n",
            "Loss: 1.380 | Acc: 50.938% (3423/6720)\n",
            "Loss: 1.379 | Acc: 50.988% (3459/6784)\n",
            "Loss: 1.376 | Acc: 51.037% (3495/6848)\n",
            "Loss: 1.378 | Acc: 50.955% (3522/6912)\n",
            "Loss: 1.380 | Acc: 50.903% (3551/6976)\n",
            "Loss: 1.383 | Acc: 50.824% (3578/7040)\n",
            "Loss: 1.382 | Acc: 50.845% (3612/7104)\n",
            "Loss: 1.382 | Acc: 50.879% (3647/7168)\n",
            "Loss: 1.384 | Acc: 50.761% (3671/7232)\n",
            "Loss: 1.381 | Acc: 50.795% (3706/7296)\n",
            "Loss: 1.379 | Acc: 50.829% (3741/7360)\n",
            "Loss: 1.381 | Acc: 50.768% (3769/7424)\n",
            "Loss: 1.379 | Acc: 50.841% (3807/7488)\n",
            "Loss: 1.378 | Acc: 50.834% (3839/7552)\n",
            "Loss: 1.379 | Acc: 50.801% (3869/7616)\n",
            "Loss: 1.378 | Acc: 50.807% (3902/7680)\n",
            "Loss: 1.376 | Acc: 50.839% (3937/7744)\n",
            "Loss: 1.377 | Acc: 50.820% (3968/7808)\n",
            "Loss: 1.377 | Acc: 50.838% (4002/7872)\n",
            "Loss: 1.376 | Acc: 50.806% (4032/7936)\n",
            "Loss: 1.377 | Acc: 50.825% (4066/8000)\n",
            "Loss: 1.377 | Acc: 50.856% (4101/8064)\n",
            "Loss: 1.377 | Acc: 50.861% (4134/8128)\n",
            "Loss: 1.377 | Acc: 50.854% (4166/8192)\n",
            "Loss: 1.379 | Acc: 50.739% (4189/8256)\n",
            "Loss: 1.382 | Acc: 50.589% (4209/8320)\n",
            "Loss: 1.384 | Acc: 50.489% (4233/8384)\n",
            "Loss: 1.384 | Acc: 50.521% (4268/8448)\n",
            "Loss: 1.385 | Acc: 50.517% (4300/8512)\n",
            "Loss: 1.385 | Acc: 50.525% (4333/8576)\n",
            "Loss: 1.386 | Acc: 50.475% (4361/8640)\n",
            "Loss: 1.387 | Acc: 50.448% (4391/8704)\n",
            "Loss: 1.387 | Acc: 50.433% (4422/8768)\n",
            "Loss: 1.388 | Acc: 50.396% (4451/8832)\n",
            "Loss: 1.386 | Acc: 50.461% (4489/8896)\n",
            "Loss: 1.386 | Acc: 50.446% (4520/8960)\n",
            "Loss: 1.385 | Acc: 50.454% (4553/9024)\n",
            "Loss: 1.386 | Acc: 50.429% (4583/9088)\n",
            "Loss: 1.387 | Acc: 50.393% (4612/9152)\n",
            "Loss: 1.384 | Acc: 50.532% (4657/9216)\n",
            "Loss: 1.383 | Acc: 50.571% (4693/9280)\n",
            "Loss: 1.383 | Acc: 50.589% (4727/9344)\n",
            "Loss: 1.384 | Acc: 50.585% (4759/9408)\n",
            "Loss: 1.384 | Acc: 50.581% (4791/9472)\n",
            "Loss: 1.383 | Acc: 50.640% (4829/9536)\n",
            "Loss: 1.383 | Acc: 50.677% (4865/9600)\n",
            "Loss: 1.382 | Acc: 50.704% (4900/9664)\n",
            "Loss: 1.381 | Acc: 50.689% (4931/9728)\n",
            "Loss: 1.382 | Acc: 50.643% (4959/9792)\n",
            "Loss: 1.383 | Acc: 50.609% (4988/9856)\n",
            "Loss: 1.384 | Acc: 50.575% (5017/9920)\n",
            "Loss: 1.385 | Acc: 50.541% (5046/9984)\n",
            "Loss: 1.384 | Acc: 50.550% (5055/10000)\n",
            "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 50.55\n",
            "\n",
            "Final train set accuracy is 49.59795918367347\n",
            "Final test set accuracy is 50.55\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "num_trans_layers = 4\n",
        "num_heads = 4\n",
        "image_k = 32\n",
        "patch_k = 4\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "model = ViT(hidden_dims,input_dims,output_dims,num_trans_layers,num_heads,image_k,patch_k)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9,0.999))\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             \n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "test_accs=[]\n",
        "for epoch in range(3):\n",
        "    tr_acc = train(model, optimizer, loader_train)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))  \n",
        "    \n",
        "    test_acc = evaluate(model, loader_test)\n",
        "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
        "              .format(epoch, test_acc))  \n",
        "    \n",
        "    tr_accs.append(tr_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    \n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec757796",
      "metadata": {
        "id": "ec757796"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "comp411",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "c0ced58895ecd070854e3ee569999fa0019a08973460a0d7e9f07c38b9686bb7"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "238cf8e173a548c0ab6cf47f75735fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece3db6e8fb843e6b3d8055fcb50b4d1",
            "placeholder": "​",
            "style": "IPY_MODEL_afa737f1c7a54d36bcf2feed2b6f1b95",
            "value": "100%"
          }
        },
        "4ea1a7c745c14811af373576f212cd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_618c338ee0804002a4e5c2c0dd0158b7",
            "placeholder": "​",
            "style": "IPY_MODEL_62807e8b2fae424aa17bb71c90d971fb",
            "value": " 170498071/170498071 [00:06&lt;00:00, 29189395.79it/s]"
          }
        },
        "618c338ee0804002a4e5c2c0dd0158b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62807e8b2fae424aa17bb71c90d971fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "885bd009bd5f49f18cd6c3917eafe8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa737f1c7a54d36bcf2feed2b6f1b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd8104dc914c400989fb1b9b7897b479": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d342e26f331b45cd9c390a7bdd1c1e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_238cf8e173a548c0ab6cf47f75735fe3",
              "IPY_MODEL_d86e82a2c0e349faa81b2d4db15b77b7",
              "IPY_MODEL_4ea1a7c745c14811af373576f212cd8a"
            ],
            "layout": "IPY_MODEL_885bd009bd5f49f18cd6c3917eafe8d7"
          }
        },
        "d86e82a2c0e349faa81b2d4db15b77b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8104dc914c400989fb1b9b7897b479",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3caba48189a48cca56663445598b014",
            "value": 170498071
          }
        },
        "e3caba48189a48cca56663445598b014": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ece3db6e8fb843e6b3d8055fcb50b4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
